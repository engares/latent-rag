### Directory tree for: .

./
├── AUTORAG.code-workspace
├── README.md
├── config/
│   ├── config.yaml
│   └── prompts/
│       └── system_prompt.txt
├── data/
│   ├── SQUAD/
│   │   ├── chunk_index.parquet
│   │   ├── chunk_index_infer.parquet
│   │   ├── sbert_cache/
│   │   │   ├── sbert_8b49db0d09_all-MiniLM-L6-v2.pt
│   │   │   └── sbert_8ebee4d368_all-MiniLM-L6-v2.pt
│   │   ├── squad_contrastive_embeddings.pt
│   │   ├── squad_dae_embeddings.pt
│   │   └── squad_vae_embeddings.pt
│   ├── SQUAD2/
│   │   ├── squad_contrastive_embeddings.pt
│   │   ├── squad_dae_embeddings.pt
│   │   └── squad_vae_embeddings.pt
│   ├── data_processing.py
│   ├── index/
│   │   ├── faiss_chunks.faiss
│   │   └── faiss_chunks.faiss.meta.json
│   └── torch_datasets.py
├── evaluation/
│   ├── autoencoder_metrics.py
│   ├── benchmark.py
│   ├── embedding_visualization.py
│   ├── generation_metrics.py
│   └── retrieval_metrics.py
├── fig/
│   ├── cae_tsne_2d_1200s_10k_perp30.png
│   ├── cae_tsne_2d_1200s_10k_perp30_negatives_distribution.png
│   ├── dae_tsne_2d_1200s_10k_perp30.png
│   ├── dae_tsne_2d_1200s_10k_perp30_negatives_distribution.png
│   └── old/
│       ├── pca_3d.png
│       ├── tsne.png
│       ├── tsne_pairs.png
│       ├── tsne_rank.png
│       ├── tsne_squad.png
│       ├── tsne_summary.png
│       └── viz.png
├── generation/
│   └── generator.py
├── logs/
│   └── run.log
├── main.py
├── models/
│   ├── base_autoencoder.py
│   ├── checkpoints/
│   │   ├── contrastive_ae.pth
│   │   ├── dae_text.pth
│   │   ├── vae_text.pth
│   │   └── vae_text.pth:Zone.Identifier
│   ├── contrastive_autoencoder.py
│   ├── denoising_autoencoder.py
│   └── variational_autoencoder.py
├── requeriments.txt
├── retrieval/
│   ├── FAISSEmbeddingRetriever.py
│   ├── base.py
│   ├── bm25.py
│   ├── bruteforce.py
│   ├── dpr.py
│   ├── embedder.py
│   └── retriever.py
├── save_snapshot.sh*
├── snapshot.txt
├── style_guide.md
├── test/
│   ├── test_alignment_squad_embeddings.py
│   ├── test_chunk_utils.py
│   ├── test_data_processing.py
│   ├── test_evaluation.py
│   ├── test_loss_functions.py
│   ├── test_models.py
│   ├── test_retrieval.py
│   ├── test_train_scripts.py
│   └── test_visualization.py
├── tests.ipynb
├── training/
│   ├── loss_functions.py
│   ├── train_cae.py
│   ├── train_dae.py
│   └── train_vae.py
└── utils/
    ├── chunk_utils.py
    ├── data_utils.py
    ├── load_config.py
    ├── training_utils.py
    └── visualization_exp.py

18 directories, 75 files


### File contents

README.md
"""
# rag\_autoencoder\_tfm

A repository for training and evaluating retrieval-augmented generation (RAG) pipelines enhanced by various autoencoder compression methods. Supported variants include:

* Variational Autoencoder (VAE)
* Denoising Autoencoder (DAE)
* Contrastive Autoencoder (CAE)

This framework covers from data preparation, model training, retrieval to optional generation with LLMs, and comprehensive evaluation metrics.

---

## Table of Contents

1. [Features](#features)
2. [Prerequisites](#prerequisites)
3. [Setup](#setup)
4. [Configuration](#configuration)
5. [Data Preparation](#data-preparation)
6. [Training](#training)

   * [VAE](#vae)
   * [DAE](#dae)
   * [CAE](#cae)
7. [Pipeline Execution](#pipeline-execution)
8. [Evaluation](#evaluation)
10. [Project Structure](#project-structure)
11. [Testing](#testing)

---

## Features

* Encode text embeddings using SBERT and compress with VAE/DAE/CAE.
* Retrieve top‑k relevant documents using cosine, Euclidean or Mahalanobis similarity.
* Generate answers via RAG using OpenAI API with custom system prompts.
* Evaluate retrieval (Recall\@k, MRR, nDCG) and generation (BLEU, ROUGE-L, METEOR) with bootstrap CIs.
* Configurable via YAML; extensible for other datasets or API providers.

## Prerequisites

* Python ≥ 3.10
* GPU recommended for large-scale embedding and training. (Trained ona 4060 8GB VRAM)
* OpenAI API key (set in `.env` as `OPENAI_API_KEY`).

Install dependencies:

```bash
pip install -r requirements.txt
```

## Setup

1. Clone the repository:

```bash

git clone https://github.com/engares/latent-rag.git
cd latent-rag
```

2. Create `.env` with your OpenAI key:
```ini
OPENAI_API_KEY=your_api_key_here
````

3. Adjust paths and hyperparameters in **`config/config.yaml`** as needed.

## Configuration

**`config/config.yaml`** contains:

* Project metadata (name, version)
* Directory paths (`data_dir`, `checkpoints_dir`, `logs_dir`)
* Embedding model settings
* Autoencoder parameters and checkpoints
* Training hyperparameters (batch size, epochs, LR)
* Retrieval & generation options
* Evaluation metrics
* Logging level and file

System prompt for generation is located in **`config/prompts/system_prompt.txt`**.

## Data Preparation

Data tensors for SQuAD are generated automatically when running training or pipeline. To prepare manually:

```bash
python -c "from utils.data_utils import ensure_squad_data; ensure_squad_data(output_dir='./data/SQUAD_DELETE')"
```

This creates:

* `data/SQUAD/squad_vae_embeddings.pt`
* `data/SQUAD/squad_dae_embeddings.pt`
* `data/SQUAD/squad_contrastive_embeddings.pt`

## Training

### VAE

```bash
python training/train_vae.py \
  --dataset squad \
  --epochs 50 \
  --batch_size 128 \
  --lr 1e-3 \
  --save_path models/checkpoints/vae_text.pth
```

### DAE

```bash
python training/train_dae.py \
  --dataset squad \
  --epochs 50 \
  --batch_size 128 \
  --lr 1e-3 \
  --save_path models/checkpoints/dae_text.pth
```

### CAE

```bash
python training/train_cae.py \
  --dataset squad \
  --epochs 50 \
  --batch_size 128 \
  --lr 1e-3 \
  --save_path models/checkpoints/contrastive_ae.pth
```

All training scripts support early stopping, checkpointing and configurable device.

## Pipeline Execution

Run the end-to-end RAG pipeline:

```bash
python main.py --config config/config.yaml --ae_type vae
```

Replace `--ae_type` with `dae`, `contrastive`, `all` or `none`. The pipeline will:

1. Encode corpus and queries
2. Retrieve top‑k documents
3. Optionally generate answers via GPT-4o-mini (if `--generate` is specified)
4. Evaluate retrieval and generation metrics

To enable the generation step, include the `--generate` flag:

```bash
python main.py --config config/config.yaml --ae_type vae --generate
```

## Evaluation

* Retrieval metrics: per-query and aggregated Recall\@k, MRR, nDCG.
* Generation metrics: BLEU, ROUGE-L, METEOR with 95% bootstrap CIs.
* Visualise embeddings via **`evaluation/autoencoder_metrics.py`** (t-SNE plots).

Commands:

```python
from evaluation.retrieval_metrics import evaluate_retrieval
from evaluation.generation_metrics import evaluate_generation_bootstrap
```


Perfecto. Aquí tienes un nuevo apartado **[9. Embedding Visualisation](#embedding-visualisation)** para añadir en tu `README.md`, totalmente integrado con el estilo del proyecto y basado en los módulos que ya has implementado:


### Experimental Embedding Visualisation

This module provides intuitive **visual diagnostics** of how well autoencoder-compressed embeddings preserve semantic similarity. It allows direct comparison between **original SBERT embeddings** and their **compressed representations** (via VAE, DAE or CAE).

Supported plots include:

* Low-dimensional **t-SNE or PCA projections** (2D or 3D) of (query, document) pairs.
* **Colour-coded scatter** plots showing cosine distances.
* **Histogram + CDF** of pairwise distances before and after compression.
* **Positive vs. Negative distance distributions**, to visualise separation margins.

Each visualisation is saved to disk automatically.

### Example

```bash
python -m utils.visualization_exp \
  --sbert-cache data/SQUAD/sbert_cache/sbert_2254a38d6b_all-MiniLM-L6-v2.pt \
  --checkpoint  models/checkpoints/contrastive_ae.pth \
  --projection  tsne \
  --components  2 \
  --sample-size 1200 \
  --k-near 10
```

This generates:

* `fig/cae_tsne_2d_1200s_10k_perp30.png`:
  Low-dimensional visualisation + recall and distance histograms.
* `fig/cae_tsne_2d_1200s_10k_perp30_negatives_distribution.png`:
  Side-by-side histogram comparing cosine distances:

  * $q \to d^+$ (positives)
  * $q \to d^-$ (negatives)


<br>

## Project Structure

```text
src/
├── config/           # YAML and prompts
├── data/             # Embedding tensors and loaders
├── evaluation/       # Metrics and visualisations
├── generation/       # RAG generator
├── models/           # AE implementations
├── retrieval/        # Embeddings & retriever
├── training/         # Training scripts and loss functions
├── utils/            # Helpers (config, data, logging)
├── test/             # Unit tests (pytest)
├── main.py           # CLI orchestration
├── requirements.txt
└── style_guide.md
```

## Testing

Run all tests via pytest:

```bash
PYTHONPATH=. pytest -q
```

Coverage threshold: 80% (unit tests for data processing, models, retrieval, evaluation, training scripts).


"""

config/config.yaml
"""
project:
  name: rag_autoencoder_tfm
  version: "0.1"

paths:
  data_dir: "./data/SQUAD"
  checkpoints_dir: "./models/checkpoints"
  logs_dir: "./logs"

embedding_model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  max_length: 256

models:

# VARIATIONAL AUTOENCODER

  vae:
    input_dim: 384 # S- BERT embedding dimension
    latent_dim: 64 
    hidden_dim: 512
    dataset_file: "squad_vae_embeddings.pt"          
    checkpoint: "vae_text.pth"

# DENOISING AUTOENCODER
  dae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_file: "squad_dae_embeddings.pt"
    checkpoint: "dae_text.pth"

# CONTRASTIVE AUTOENCODER
  contrastive:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_file: "squad_cae_embeddings.pt"
    checkpoint: "coe_text.pth"

data:
  dataset: "squad"            #  "uda"  |  "squad"
  version: "v1"          #  v1, v2  ONly for squad
  max_samples: 2000        #  optional, blankk to use all samples
  include_unanswerable: false   #  Only Squad v2 has this option

training:
  batch_size: 128
  epochs: 50
  learning_rate: 1e-3
  seed: 42
  device: "cuda"  # "cpu"
  deterministic: false      # (true = modo debug)

retrieval:
  backend: faiss          # 'faiss' | 'bruteforce'
  index_type: flatip        # flatip | hnsw | ivfpq
  index_path: ./data/index/faiss_chunks.faiss
  use_gpu: true
  top_k: 10
  max_chunks_per_doc: 3   # (como se definió en el roadmap de chunking)

generation:
  provider: "openai"       # Just OpenAI by now
  model: "gpt-4o-mini"
  temperature: 0.3
  max_tokens: 256
  system_prompt_path: "./config/prompts/system_prompt.txt"

chunking:
  enabled: false
  mode: semantic        # "sliding" | "semantic"
  max_tokens: 128
  stride: 64
  min_tokens: 48        # solo para 'semantic'
  tokenizer_name: sentence-transformers/all-MiniLM-L6-v2
  index_out: "./data/SQUAD/chunk_index_infer.parquet"
  store_chunk_text: true

evaluation:
  retrieval_metrics: ["Recall@10", "MRR@10", "nDCG@10"] # You can change the k top  simply by changing the @k e.j Recall@10, "MRR@20"
  generation_metrics: ["ROUGE-L", "BLEU"]

logging:
  level: "INFO"
  log_to_file: true
  log_file: "./logs/run.log"

"""

config/prompts/system_prompt.txt
"""
Here is the user query and relevant text chunks. Step 1: Summarize user question in simpler words. Step 2: Decide which retrieved text chunks directly apply. Step 3: Combine those chunks into an outline. Step 4: Draft a single, coherent answer. Show all steps, then provide a final refined answer.
"""

data/data_processing.py
"""
#/data_processing.py

import random
import json
import re
from typing import List, Dict
from datasets import load_dataset

# ---------------------------------------------------------
# UDA Dataset Preprocessing for Autoencoder Training
# ---------------------------------------------------------
# Supports: Denoising AE (with artificial noise), VAE, Contrastive AE
# ---------------------------------------------------------

def clean_text(text: str) -> str:
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def add_noise(text: str, removal_prob=0.1, swap_prob=0.05) -> str:
    words = text.split()
    # Remove tokens
    words = [w for w in words if random.random() > removal_prob]
    # Swap nearby tokens
    for i in range(len(words)-1):
        if random.random() < swap_prob:
            words[i], words[i+1] = words[i+1], words[i]
    return " ".join(words)

def build_dae_dataset(samples: List[str]) -> List[Dict[str, str]]:
    dataset = []
    for original in samples:
        noisy = add_noise(original)
        dataset.append({"input": noisy, "target": original})
    return dataset

def build_contrastive_pairs(dataset, max_negatives=1) -> List[Dict]:
    pairs = []
    for example in dataset:
        q = example["query"]
        pos = example["positive_passages"][0]["text"]
        negs = [n["text"] for n in example["negative_passages"][:max_negatives]]
        for neg in negs:
            pairs.append({"query": q, "positive": pos, "negative": neg})
    return pairs

"""

data/torch_datasets.py
"""
# /data/torch_datasets.py
import torch
from torch.utils.data import Dataset
from typing import Dict, List, Tuple


# ---------- UTILIDADES COMUNES ------------------------------------------------
def _load_pt(path: str) -> Dict[str, torch.Tensor]:
    """
    Carga un fichero .pt con tensores y asegura dtype = float32 en CPU.
    El fichero se espera como un dict { name: Tensor }.
    """
    data = torch.load(path, map_location="cpu")
    return {k: v.float() for k, v in data.items()}


# ---------- DATASETS ---------------------------------------------------------


class EmbeddingVAEDataset(Dataset):
    """
    Carga el fichero .pt generado por `ensure_uda_data`.
    Estructura esperada:
        {"input": <tensor [N×D]>, "target": <tensor [N×D]>}
    """
    def __init__(self, path: str):
        data = torch.load(path, map_location="cpu")
        self.input  = data["input"].float()
        self.target = data["target"].float()
        assert self.input.shape == self.target.shape, "input/target tamaño desigual"

    def __len__(self):
        return self.input.size(0)

    def __getitem__(self, idx):
        return {
            "input":  self.input[idx],
            "target": self.target[idx],
        }


class EmbeddingDAEDataset(Dataset):
    """
    Carga 'uda_dae_embeddings.pt' producido por `ensure_uda_data`.

    Estructura:
        {
            "input":  Tensor [N × D]  (embeddings con ruido)
            "target": Tensor [N × D]  (embeddings limpios)
        }
    """
    def __init__(self, path: str):
        d = torch.load(path, map_location="cpu")
        self.x  = d["input" ].float()
        self.y  = d["target"].float()
        assert self.x.shape == self.y.shape, "Input / target mismatch"

    def __len__(self):          return self.x.size(0)
    def __getitem__(self, idx): return {"x": self.x[idx], "y": self.y[idx]}

    

class EmbeddingTripletDataset(Dataset):
    """
    Carga 'uda_contrastive_embeddings.pt' generado por `ensure_uda_data`.

    Estructura esperada:
        {
            "query":     Tensor [N × D],
            "positive":  Tensor [N × D],
            "negative":  Tensor [N × D]
        }
    """
    def __init__(self, path: str):
        data = torch.load(path, map_location="cpu")
        self.q  = data["query"].float()
        self.p  = data["positive"].float()
        self.n  = data["negative"].float()
        assert self.q.shape == self.p.shape == self.n.shape, "Dimensiones incompatibles"

    def __len__(self) -> int:          return self.q.size(0)

    def __getitem__(self, idx):        # devuelvo tensores individuales
        return {"q": self.q[idx],
                "p": self.p[idx],
                "n": self.n[idx]}


# ---------- PRUEBA RÁPIDA -----------------------------------------------------
if __name__ == "__main__":
    dae_ds = EmbeddingDAEDataset("./data/squad_dae_embeddings.pt")
    vae_ds = EmbeddingVAEDataset("./data/squad_vae_embeddings.pt")
    con_ds = EmbeddingTripletDataset("./data/squad_contrastive_embeddings.pt")

    print("DAE sample ⇒", {k: v.shape for k, v in dae_ds[0].items()})
    print("Contrastive sample ⇒", {k: v.shape for k, v in con_ds[0].items()})
    print("VAE sample ⇒", {k: v.shape for k, v in vae_ds[0].items()})

"""

evaluation/autoencoder_metrics.py
"""
import torch



def evaluate_reconstruction_loss(x: torch.Tensor, x_reconstructed: torch.Tensor, reduction: str = "mean") -> float:
    """Calcula el error de reconstrucción (MSE)."""
    loss_fn = torch.nn.MSELoss(reduction=reduction)
    return loss_fn(x_reconstructed, x).item()
"""

evaluation/benchmark.py
"""
# /evalaution/benchmark.py
from typing import Dict, Sequence
from retrieval.bm25 import BM25Retriever
from retrieval.dpr  import DPRRetriever
from retrieval.embedder import EmbeddingCompressor
from utils.load_config import load_config
from evaluation.retrieval_metrics import evaluate_retrieval, paired_bootstrap_test

Retrievers = {
    "bm25":  BM25Retriever(),
    "dpr":   DPRRetriever(),
    "sbert": EmbeddingCompressor(),          # no AE
    "vae":   EmbeddingCompressor(ae_type="vae"),
    "dae":   EmbeddingCompressor(ae_type="dae"),
    "cae":   EmbeddingCompressor(ae_type="contrastive"),
}

def run_benchmark(queries: Sequence[str],
                  corpus:  Sequence[str],
                  gold:    Sequence[Sequence[str]],
                  cfg_path="./config/config.yaml") -> None:
    cfg = load_config(cfg_path)
    metrics = cfg["evaluation"]["retrieval_metrics"]
    results: Dict[str, Dict[str, float]] = {}

    for name, retr in Retrievers.items():
        retr.build_index(corpus)
        retrieved = [retr.retrieve(q, k=cfg["retrieval"]["top_k"]) for q in queries]
        summary   = evaluate_retrieval(retrieved, gold, metrics)   # media + sd
        results[name] = {m: v["mean"] for m, v in summary.items()}
        print(f"{name.upper():<6}", summary)

    # Significancia (ejemplos)
    _print_sig(results, queries, gold, metrics[0])

def _print_sig(res, queries, gold, metric):
    from itertools import combinations
    for a, b in combinations(res.keys(), 2):
        pa, pb = res[a][metric], res[b][metric]
        # bootstrap paired significance (aproveche evaluate_generation_bootstrap si lo desea)
        print(f"{a} vs {b}: Δ={pa-pb:+.4f}")

"""

evaluation/embedding_visualization.py
"""
from __future__ import annotations

from collections import defaultdict
from itertools import cycle
from typing import Dict, Iterable, List, Mapping, MutableSequence, Optional, Sequence

import matplotlib.cm as cm
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from matplotlib.colors import Colormap
from matplotlib.ticker import PercentFormatter
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 – required for 3-D scatter
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
Tensor = torch.Tensor  

import matplotlib.cm as cm
from matplotlib.colors import Normalize


# ---------------------------------------------------------------------------
# Helper utilities
# ---------------------------------------------------------------------------



def _to_numpy(x: Tensor) -> "np.ndarray":  # type: ignore[name-defined]
    """Detach a tensor and move to CPU as *float32* NumPy array."""
    return x.detach().cpu().float().numpy()


def _rank_positive(q: Tensor, d: Tensor) -> Tensor:
    """Return **1-based** rank of each positive document using cosine similarity."""
    sim = F.cosine_similarity(q.unsqueeze(1), d.unsqueeze(0), dim=-1)
    return sim.argsort(dim=1, descending=True).argsort(dim=1).diagonal() + 1


def _project(
    x: Tensor,
    *,
    method: str,
    n_components: int,
    perplexity: float,
    seed: int,
) -> Tensor:
    """Return a low-dimensional projection via t-SNE or PCA."""
    if method == "tsne":
        tsne = TSNE(
            n_components=n_components,
            perplexity=perplexity,
            metric="cosine",
            init="pca",
            max_iter=1_000,
            random_state=seed,
        )
        return torch.from_numpy(tsne.fit_transform(_to_numpy(x))).float()
    if method == "pca":
        pca = PCA(n_components=n_components, random_state=seed)
        return torch.from_numpy(pca.fit_transform(_to_numpy(x))).float()
    raise ValueError("method must be 'tsne' or 'pca'")





# ---------------------------------------------------------------------------
# Plotting primitives
# ---------------------------------------------------------------------------


def _link(ax, p: Tensor, q: Tensor, colour: str):
    if p.numel() == 2:            # 2-D
        ax.plot([p[0], q[0]], [p[1], q[1]],
                color=colour, linewidth=0.8, alpha=0.8)
    else:                         # 3-D
        ax.plot([p[0], q[0]], [p[1], q[1]], [p[2], q[2]],
                color=colour, linewidth=0.8, alpha=0.8)


def _scatter_pairs(
    ax,
    q_emb: Tensor,
    d_emb: Tensor,
    dist: Tensor,
    *,
    cmap_name: str = "viridis_r",
    alpha: float = 0.3,
    s: int = 30,
    max_links: int = 10,
):
    """
    Dibuja las parejas (query-doc) coloreadas según la distancia del coseno.

    Params
    ------
    q_emb, d_emb : (N, 2|3)  Embeddings proyectados de queries y docs.
    dist         : (N,)      Distancias del coseno ||q - d|| en el espacio original.
    max_links    : int       Número máximo de enlaces (los más disimilares).
    """
    dim = q_emb.size(1)
    norm = Normalize(vmin=float(dist.min()), vmax=float(dist.max()))
    cmap = cm.get_cmap(cmap_name)

    # Matriz RGBA con alpha constante
    colors = cmap(norm(_to_numpy(dist)))
    colors[:, -1] = alpha

    # Dispersión de queries
    ax.scatter(
        q_emb[:, 0], q_emb[:, 1],
        *(q_emb[:, 2].T,) if dim == 3 else (),
        c=colors, marker="o", s=s, label="Query"
    )
    # Dispersión de docs
    ax.scatter(
        d_emb[:, 0], d_emb[:, 1],
        *(d_emb[:, 2].T,) if dim == 3 else (),
        c=colors, marker="^", s=s, label="Doc"
    )

    # Índices de las distancias más altas
    top_idx = dist.topk(min(max_links, len(dist))).indices

    # Enlaces coloreados (solo top-k)
    for i in top_idx:
        _link(ax, q_emb[i], d_emb[i], colour=colors[i])

    ax.set_xticks([]); ax.set_yticks([])
    if dim == 3:
        ax.set_zticks([])
    ax.legend(frameon=False, fontsize=7, loc="upper right")


def _hist_cdf(ax: "plt.Axes", d1: Tensor, d2: Tensor, *, bins: int) -> None:
    """Histogram + CDF of two 1-D distance distributions."""
    cmap = cm.get_cmap("viridis")  # Use the viridis colormap
    color1 = cmap(0.3)  # A color from the lower range of viridis
    color2 = cmap(0.7)  # A color from the higher range of viridis

    # Histogram
    ax.hist(d1.numpy(), bins=bins, alpha=0.5, label="Original dist.", color=color1)
    ax.hist(d2.numpy(), bins=bins, alpha=0.5, label="Compressed dist.", color=color2)
    ax.set_xlabel("Pair cosine distance ")
    ax.set_ylabel("Frequency")
    ax.legend(frameon=False, fontsize=7)

    # CDF
    ax2 = ax.twinx()
    for data, lbl, color in ((d1, "Original CDF", color1), (d2, "Compressed CDF", color2)):
        sorted_vals = torch.sort(data).values
        cdf = torch.arange(1, len(data) + 1) / len(data)
        ax2.plot(sorted_vals, cdf, label=lbl, color=color)
    ax2.yaxis.set_major_formatter(PercentFormatter(1.0))
    ax2.set_ylabel("CDF")
    ax2.legend(frameon=False, fontsize=7, loc="lower right")


def visualize_compressed_vs_original(
    q_orig: Tensor,
    d_orig: Tensor,
    q_comp: Tensor,
    d_comp: Tensor,
    *,
    projection: str = "tsne",
    n_components: int = 2,
    sample_size: int = 1_000,
    k_near: int = 5,
    perplexity: float = 30.0,
    bins: int = 30,
    random_state: int = 42,
    save_path: Optional[str] = None,
    save_negatives_path: Optional[str] = None,
) -> Dict[str, float]:
    """Visualise original vs. compressed embeddings and save two figures:
      1) scatter + hist/CDF
      2) dist positives vs. negatives."""

    torch.manual_seed(random_state)
    N = len(q_orig)
    if sample_size < N:
        idx = torch.randperm(N)[:sample_size]
        q_o, d_o, q_c, d_c = q_orig[idx], d_orig[idx], q_comp[idx], d_comp[idx]
    else:
        q_o, d_o, q_c, d_c = q_orig, d_orig, q_comp, d_comp


    # Recall
    rank_orig = _rank_positive(q_o, d_o)
    recall_orig = (rank_orig <= k_near).float().mean()
    rank_comp = _rank_positive(q_c, d_c)
    recall_comp = (rank_comp <= k_near).float().mean()

    # Projections (only for figure 1)
    orig_proj = _project(torch.cat([q_o, d_o]), method=projection,
                         n_components=n_components, perplexity=perplexity, seed=random_state)
    comp_proj = _project(torch.cat([q_c, d_c]), method=projection,
                         n_components=n_components, perplexity=perplexity, seed=random_state)
    q_proj_o, d_proj_o = orig_proj[: len(q_o)], orig_proj[len(q_o):]
    q_proj_c, d_proj_c = comp_proj[: len(q_o)], comp_proj[len(q_o):]

    # Cosine distances
    dist_o = 1 - F.cosine_similarity(q_o, d_o, dim=1)
    dist_c = 1 - F.cosine_similarity(q_c, d_c, dim=1)

    # ========== FIGURA 1: scatter + hist/CDF ==========
    fig = plt.figure(figsize=(14, 10 if n_components==2 else 12))
    gs = gridspec.GridSpec(2, 3, width_ratios=[1,1,0.05], height_ratios=[3,2])

    ax1 = fig.add_subplot(gs[0,0], projection="3d" if n_components==3 else None)
    ax2 = fig.add_subplot(gs[0,1], projection="3d" if n_components==3 else None)
    cbar_ax = fig.add_subplot(gs[0,2])
    ax_hist = fig.add_subplot(gs[1,:2])

    _scatter_pairs(ax1, q_proj_o, d_proj_o, dist_o)
    _scatter_pairs(ax2, q_proj_c, d_proj_c, dist_c)
    ax1.set_title(f"Original {projection.upper()} – Recall@{k_near}: {recall_orig:.1%}", fontsize=10)
    ax2.set_title(f"Compressed {projection.upper()} – Recall@{k_near}: {recall_comp:.1%}", fontsize=10)
    _hist_cdf(ax_hist, dist_o, dist_c, bins=bins)
    ax_hist.set_title(f"Pair distance distribution ({projection.upper()} {n_components}-D)")

    sm = cm.ScalarMappable(norm=Normalize(vmin=float(min(dist_o.min(), dist_c.min())),
                                         vmax=float(max(dist_o.max(), dist_c.max()))),
                           cmap="viridis_r")
    sm.set_array([])
    fig.colorbar(sm, cax=cbar_ax).set_label("Cosine distance", rotation=270, labelpad=15)

    fig.tight_layout()
    if save_path:
        fig.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

    plot_positive_vs_negative_distances(
        q_o, d_o, q_c, d_c,
        bins=bins,
        save_negatives_path=save_negatives_path
    )

    return {
        "recall_original": float(recall_orig),
        "recall_compressed": float(recall_comp),
    }


def plot_positive_vs_negative_distances(
    q_o: Tensor,
    d_o: Tensor,
    q_c: Tensor,
    d_c: Tensor,
    *,
    bins: int,
    save_negatives_path: Optional[str] = None
) -> None:
    """Plot positive vs. negative distances for original and compressed embeddings."""
    # Muestreo de negativos (una permutación aleatoria)
    perm = torch.randperm(len(d_o))
    neg_idx = torch.where(perm == torch.arange(len(d_o)), (perm + 1) % len(d_o), perm)
    d_neg_o = d_o[neg_idx]
    d_neg_c = d_c[neg_idx]

    # Calcular distancias del coseno para positivos y negativos
    dist_o = 1 - F.cosine_similarity(q_o, d_o, dim=1)
    dist_c = 1 - F.cosine_similarity(q_c, d_c, dim=1)
    dist_neg_o = 1 - F.cosine_similarity(q_o, d_neg_o, dim=1)
    dist_neg_c = 1 - F.cosine_similarity(q_c, d_neg_c, dim=1)

    fig, (axp, axn) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Distribución originales
    axp.hist(dist_o.numpy(), bins=bins, alpha=0.6, label="Positives", color="yellowgreen")
    axp.hist(dist_neg_o.numpy(), bins=bins, alpha=0.6, label="Negatives", color="firebrick")
    axp.set_title("Original: q–d⁺ vs q–d⁻")
    axp.set_xlabel("Cosine distance")
    axp.set_ylabel("Frequency")
    axp.legend(frameon=False)

    # Distribución comprimidas
    axn.hist(dist_c.numpy(), bins=bins, alpha=0.6, label="Positives", color="yellowgreen")
    axn.hist(dist_neg_c.numpy(), bins=bins, alpha=0.6, label="Negatives", color="firebrick")
    axn.set_title("Compressed: q–d⁺ vs q–d⁻")
    axn.set_xlabel("Cosine distance")
    axn.legend(frameon=False)

    fig.tight_layout()
    if save_negatives_path:
        fig.savefig(save_negatives_path, dpi=300, bbox_inches="tight")
    plt.show()

"""

evaluation/generation_metrics.py
"""
from __future__ import annotations

"""Métricas de generación con *bootstrap* y prueba de significancia emparejada.

Uso principal:
--------------
>>> mean_ci = evaluate_generation_bootstrap(refs, cands, metrics=["BLEU", "ROUGE-L"])
>>> pval = paired_bootstrap_test(refs, sys_a, sys_b, metric="BLEU")
"""

from collections.abc import Callable
from typing import List, Dict, Tuple
import numpy as np
from sacrebleu.metrics import BLEU as _BLEUMetric
from rouge_score import rouge_scorer
import random

###############################################################################
#  MÉTRICAS BÁSICAS                                                           #
###############################################################################

_bleu = _BLEUMetric()
_scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)


def compute_bleu(candidates: List[str], references: List[str]) -> float:
    """BLEU corpus‐level sacreBLEU (0‑100). Handles both flat and nested reference lists."""
    # Flatten references if it's a list of lists (shouldn't be for single-ref BLEU)
    if references and isinstance(references[0], list):
        # If already nested, flatten one level
        references = [ref for sublist in references for ref in (sublist if isinstance(sublist, list) else [sublist])]
    return _bleu.corpus_score(candidates, [references]).score


def compute_rouge_l(candidates: List[str], references: List[str]) -> float:
    """Promedio de ROUGE‑L (F1) ×100."""
    def to_str(x):
        if isinstance(x, list):
            return " ".join(map(str, x))
        return str(x)
    scores = [
        _scorer.score(to_str(ref), to_str(cand))["rougeL"].fmeasure * 100
        for ref, cand in zip(references, candidates)
    ]
    return float(np.mean(scores))


_metric_fn: Dict[str, Callable[[List[str], List[str]], float]] = {
    "BLEU": compute_bleu,
    "ROUGE-L": compute_rouge_l,
}

###############################################################################
#  BOOTSTRAP                                                                  #
###############################################################################

def _bootstrap_ci(
    func: Callable[[List[str], List[str]], float],
    refs: List[str],
    cands: List[str],
    n_samples: int = 2000,
    alpha: float = 0.05,
    seed: int | None = None,
) -> Tuple[float, float, float]:
    """Devuelve media, límite inferior y superior del IC al (1‑alpha)."""
    if seed is not None:
        random.seed(seed)
    N = len(refs)
    stats = []
    for _ in range(n_samples):
        idx = [random.randint(0, N - 1) for _ in range(N)]
        stats.append(func([cands[i] for i in idx], [refs[i] for i in idx]))
    stats_np = np.array(stats)
    mean = stats_np.mean()
    lower = np.percentile(stats_np, 100 * alpha / 2)
    upper = np.percentile(stats_np, 100 * (1 - alpha / 2))
    return mean, lower, upper


def evaluate_generation_bootstrap(
    references: List[str],
    candidates: List[str],
    metrics: List[str] | None = None,
    n_samples: int = 2000,
    alpha: float = 0.05,
    seed: int | None = None,
) -> Dict[str, Dict[str, float]]:
    """Calcula métricas + IC al 95 % mediante bootstrap.

    Retorna: {metric: {"mean": m, "ci_lower": l, "ci_upper": u}}
    """
    if metrics is None:
        metrics = ["BLEU", "ROUGE-L"]

    assert len(references) == len(candidates) >= 100, ( 
        "Se requieren al menos 100 pares ref‑cand para un IC mínimo; se recomienda ≥1000."
    )

    results: Dict[str, Dict[str, float]] = {}
    for m in metrics:
        if m not in _metric_fn:
            raise ValueError(f"Non soported metric '{m}'")
        mean, lo, hi = _bootstrap_ci(_metric_fn[m], references, candidates, n_samples, alpha, seed)
        results[m] = {"mean": mean, "ci_lower": lo, "ci_upper": hi}
    return results

###############################################################################
#  PAIRED BOOTSTRAP SIGNIFICANCE TEST                                         #
###############################################################################

def paired_bootstrap_test(
    references: List[str],
    sys_a: List[str],
    sys_b: List[str],
    metric: str = "BLEU",
    n_samples: int = 10000,
    seed: int | None = None,
) -> Dict[str, float]:
    """Prueba de significancia emparejada (bootstrap) entre dos sistemas.

    Devuelve un dict: {"diff_mean": d, "ci_lower": lo, "ci_upper": hi, "p_value": p}
    p‑value ≈ proporción de muestras con diferencia ≤0 (o ≥0, según el signo).
    """
    assert len(references) == len(sys_a) == len(sys_b)
    if seed is not None:
        random.seed(seed)

    if metric not in _metric_fn:
        raise ValueError(f"Métrica '{metric}' no soportada.")
    fn = _metric_fn[metric]

    N = len(references)
    diffs = []
    for _ in range(n_samples):
        idx = [random.randint(0, N - 1) for _ in range(N)]
        a_score = fn([sys_a[i] for i in idx], [references[i] for i in idx])
        b_score = fn([sys_b[i] for i in idx], [references[i] for i in idx])
        diffs.append(a_score - b_score)

    diffs_np = np.array(diffs)
    diff_mean = diffs_np.mean()
    ci_lower = np.percentile(diffs_np, 2.5)
    ci_upper = np.percentile(diffs_np, 97.5)
    # p‑value: H0: diff <= 0  (si diff_mean>0) ó diff >=0 (si diff_mean<0)
    if diff_mean >= 0:
        p_val = (diffs_np <= 0).mean()
    else:
        p_val = (diffs_np >= 0).mean()

    return {
        "diff_mean": diff_mean,
        "ci_lower": ci_lower,
        "ci_upper": ci_upper,
        "p_value": p_val,
    }

"""

evaluation/retrieval_metrics.py
"""
# evaluation/retrieval_metrics.py

from __future__ import annotations

import numpy as np
from typing import List, Sequence, Dict, Tuple, Union
              
ID = Union[int, str]

###############################################################################
#  Métricas elementales (1 consulta)
###############################################################################

def recall_at_k(retrieved: Sequence[ID], relevant: Sequence[ID], k: int) -> float:
    if not relevant:
        return 0.0
    return len(set(retrieved[:k]) & set(relevant)) / len(relevant)

def mrr(retrieved: Sequence[ID], relevant: Sequence[ID]) -> float:
    for i, d in enumerate(retrieved, 1):
        if d in relevant:
            return 1.0 / i
    return 0.0

def ndcg_at_k(retrieved: Sequence[ID], relevant: Sequence[ID], k: int) -> float:
    dcg = sum(
        1.0 / np.log2(i + 2) if d in relevant else 0.0
        for i, d in enumerate(retrieved[:k])
    )
    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), k)))
    return dcg / idcg if idcg else 0.0

###############################################################################
#  Vectorización sobre lotes
###############################################################################

def _parse_metric(m: str) -> Tuple[str, int | None]:
    return (m.split("@")[0], int(m.split("@")[1])) if "@" in m else (m, None)

def _score_single(
    retrieved: Sequence[ID],
    relevant: Sequence[ID],
    name: str,
    k: int | None,
) -> float:
    name = name.lower()
    if name == "recall" and k is not None:
        return recall_at_k(retrieved, relevant, k)
    if name == "mrr":
        return mrr(retrieved[: (k or len(retrieved))], relevant)
    if name == "ndcg" and k is not None:
        return ndcg_at_k(retrieved, relevant, k)
    raise ValueError(f"Metric '{name}' not found.")

def evaluate_retrieval(retrieved_batch: List[Sequence[ID]] | Sequence[ID],
    relevant_batch: List[Sequence[ID]] | Sequence[ID],
    metrics: List[str] | None = None,
    *,
    return_per_query: bool = False,
) -> Dict[str, Dict[str, float]] | Tuple[Dict[str, Dict[str, float]],
                                         List[Dict[str, float]]]:
    
    # ── Normalizar entrada a lote ──────────────────────────────────────────
    single = isinstance(retrieved_batch[0], (str, int))  # type: ignore[index]
    if single:
        retrieved_batch = [retrieved_batch]              # type: ignore[assignment]
        relevant_batch  = [relevant_batch]               # type: ignore[assignment]

    assert len(retrieved_batch) == len(relevant_batch), \
        "retrieved_batch and relevant_batch must have the same length."

    if not metrics:
        raise ValueError("No metrics specified.")

    Q = len(retrieved_batch)
    per_query: List[Dict[str, float]] = [{} for _ in range(Q)]
    summary: Dict[str, Dict[str, float]] = {}

    for m in metrics:
        name, k = _parse_metric(m)
        vals = [
            _score_single(r, rel, name, k)
            for r, rel in zip(retrieved_batch, relevant_batch)
        ]
        summary[m] = {
            "mean": float(np.mean(vals)),
            "std":  float(np.std(vals, ddof=1)) if Q > 1 else 0.0,
        }
        for d, v in zip(per_query, vals):
            d[m] = v

    if return_per_query:
        return summary, per_query
    if single:
        return {k: v["mean"] for k, v in summary.items()}      # compat.
    return summary

"""

generation/generator.py
"""
"""RAG Generator Module.

Implements a retrieval-augmented generation (RAG) pipeline using LLMs and retrieved documents.
"""
from __future__ import annotations
import os
import textwrap
import logging
from typing import List, Dict, Any
from dataclasses import dataclass, field
from openai import OpenAI, AsyncOpenAI

def _load_prompt(path: str) -> str:
    """Load a text prompt from a file.

    Args:
        path: Path to the prompt file.

    Returns:
        The content of the file as a string.

    Raises:
        FileNotFoundError: If the file does not exist.
    """
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        logging.getLogger(__name__).warning("Prompt not found: %s", path)
        return ""

@dataclass
class LLMSettings:
    """Configuration for the language model."""
    model: str = "gpt-4o-mini"
    temperature: float = 0.3
    top_p: float = 1.0
    max_tokens: int = 512
    system_prompt_path: str = "./config/prompts/system_prompt.txt"
    system_prompt: str = field(init=False)

    def __post_init__(self):
        self.system_prompt = _load_prompt(self.system_prompt_path)

@dataclass
class GeneratorConfig:
    """Configuration for the RAG generator."""
    llm: LLMSettings = field(default_factory=LLMSettings)
    max_context_tokens: int = 4096
    provider: str = "openai"
    extras: Dict[str, Any] = field(default_factory=dict)

class RAGGenerator:
    """Generator based on LLMs and retrieved documents."""

    def __init__(self, config: Dict[str, Any], **overrides):
        """Initialize the RAG generator.

        Args:
            config: Configuration dictionary loaded from YAML or other sources.
            overrides: Additional configuration overrides.
        """
        generator_config = {**config.get("generation", {}), **overrides}

        llm_config = generator_config.pop("llm", {})
        self.config = GeneratorConfig(
            llm=LLMSettings(**llm_config),
            **{k: v for k, v in generator_config.items() if k in {"max_context_tokens", "provider"}},
            extras={k: v for k, v in generator_config.items() if k not in {"max_context_tokens", "provider"}}
        )

        self.logger = logging.getLogger(self.__class__.__name__)
        self._initialize_openai()

    def generate(self, query: str, retrieved_docs: List[str]) -> str:
        """Generate a response using the LLM.

        Args:
            query: User query string.
            retrieved_docs: List of retrieved documents to use as context.

        Returns:
            Generated response string.
        """
        prompt = self._build_prompt(query, retrieved_docs)
        self.logger.debug("Prompt (%d chars) constructed.", len(prompt))

        response = self.client.chat.completions.create(
            model=self.config.llm.model,
            temperature=self.config.llm.temperature,
            top_p=self.config.llm.top_p,
            max_tokens=self.config.llm.max_tokens,
            messages=[
                {"role": "system", "content": self.config.llm.system_prompt},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()

    async def generate_async(self, query: str, retrieved_docs: List[str]) -> str:
        """Asynchronously generate a response using the LLM.

        Args:
            query: User query string.
            retrieved_docs: List of retrieved documents to use as context.

        Returns:
            Generated response string.
        """
        prompt = self._build_prompt(query, retrieved_docs)
        self.logger.debug("Prompt async (%d chars).", len(prompt))

        client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = await client.chat.completions.create(
            model=self.config.llm.model,
            temperature=self.config.llm.temperature,
            top_p=self.config.llm.top_p,
            max_tokens=self.config.llm.max_tokens,
            messages=[
                {"role": "system", "content": self.config.llm.system_prompt},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()

    def _initialize_openai(self) -> None:
        """Initialize the OpenAI client."""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise EnvironmentError(
                "Environment variable OPENAI_API_KEY is not defined. Add it to your .env or environment."
            )
        self.client = OpenAI(api_key=api_key)
        self.logger.info("OpenAI API key loaded successfully.")

    def _build_prompt(self, query: str, docs: List[str]) -> str:
        """Construct the prompt for the LLM.

        Args:
            query: User query string.
            docs: List of retrieved documents to use as context.

        Returns:
            Constructed prompt string.
        """
        context = self._truncate_docs(docs)
        joined = "\n\n".join(f"Doc {i+1}: {d}" for i, d in enumerate(context))
        return textwrap.dedent(
            f"""\
            Use only the following information to respond.\n\n{joined}\n\n
            Question: {query}\n\nAnswer:"""
        )

    def _truncate_docs(self, docs: List[str]) -> List[str]:
        """Truncate documents to fit within the token limit.

        Args:
            docs: List of documents to truncate.

        Returns:
            List of truncated documents.
        """
        max_chars = self.config.max_context_tokens * 4  # heuristic ≈ tokens * 4
        output, accumulated = [], 0
        for doc in docs:
            if accumulated + len(doc) > max_chars:
                break
            output.append(doc)
            accumulated += len(doc)
        return output

"""

main.py
"""
"""Main pipeline for RAG-AE experiments.

This script orchestrates the retrieval-augmented generation (RAG) pipeline, including encoding, retrieval, optional generation, and evaluation.
"""
from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple

import torch
from dotenv import load_dotenv
from utils.data_utils import load_evaluation_data

# Third‑party
from rich import print as rprint
from sentence_transformers import SentenceTransformer  # lazy‑loaded by embedder

# First‑party (repository) -----------------------------------------------------
from utils.load_config import init_logger, load_config
from utils.training_utils import resolve_device, set_seed
from retrieval.embedder import EmbeddingCompressor
from evaluation.retrieval_metrics import evaluate_retrieval
from evaluation.generation_metrics import (
    evaluate_generation_bootstrap as eval_generation,
)
from generation.generator import RAGGenerator
from models.variational_autoencoder import VariationalAutoencoder
from models.denoising_autoencoder import DenoisingAutoencoder
from models.contrastive_autoencoder import ContrastiveAutoencoder

from retrieval.retriever import build_retriever          
from retrieval.FAISSEmbeddingRetriever import FAISSEmbeddingRetriever
import time
from typing import Any

def _bool(x: Any) -> bool:
    """Convert a value to boolean, treating None as False.

    Args:
        x: Input value.

    Returns:
        Boolean representation of the input.
    """
    return bool(x) if x is not None else False

def _print_run_card(cfg: Dict[str, Any], ae_type: str, *, generate: bool) -> None:
    """Print a summary of the experiment configuration.

    Args:
        cfg: Configuration dictionary.
        ae_type: Type of autoencoder used.
        generate: Whether the generation step is enabled.
    """
    retr = cfg.get("retrieval", {})
    ch = cfg.get("chunking", {})
    data = cfg.get("data", {})
    embm = cfg.get("embedding_model", {})
    gen = cfg.get("generation", {})

    use_chunking = _bool(ch.get("enabled"))
    top_k = int(retr.get("top_k", 10))
    cand_k = int(retr.get("candidate_k", top_k * 3 if use_chunking else top_k))

    lines = [
        "Experiment Configuration",
        f"  Dataset: {data.get('dataset', '?')} / split=validation / max_samples={data.get('max_samples')}",
        f"  Embedding: {embm.get('name', '?')} (max_length={embm.get('max_length', '?')})",
        f"  Autoencoder: {ae_type}",
        f"  Retrieval: backend={retr.get('backend', 'faiss')} index_type={retr.get('index_type', 'hnsw')} "
        f"use_gpu={_bool(retr.get('use_gpu'))} top_k={top_k} candidate_k={cand_k} max_chunks_per_doc={retr.get('max_chunks_per_doc', 2)}",
        (
            f"  Chunking: enabled={use_chunking} mode={ch.get('mode', 'sliding')} "
            f"max_tokens={ch.get('max_tokens', 128)} stride={ch.get('stride', 64)} "
            + (f"min_tokens={ch.get('min_tokens', 48)} " if ch.get('mode', 'sliding') != 'sliding' else "")
            + f"tokenizer={ch.get('tokenizer_name', embm.get('name', '?'))}"
        ) if use_chunking else "  Chunking: disabled",
        f"  Evaluation: {', '.join(cfg.get('evaluation', {}).get('retrieval_metrics', ['Recall@10', 'MRR@10', 'nDCG@10']))}",
        (
            f"  Generation: provider={gen.get('provider')} model={gen.get('model')} "
            f"temperature={gen.get('temperature', 0.3)} max_tokens={gen.get('max_tokens', 256)}"
        ) if generate else "  Generation: disabled",
    ]
    from rich import print as rprint
    rprint("\n" + "\n".join(lines) + "\n")

# ---------------------------------------------------------------------------
# Helper factories
# ---------------------------------------------------------------------------

def _resolve_ckpt_path(checkpoint: str | None, cfg_paths: Dict[str, Any]) -> Path:
    """Returns the absolute path of the checkpoint.
    If relative, it is concatenated with paths.checkpoints_dir.
    """
    if not checkpoint:
        return Path()  # invalid
    p = Path(checkpoint)
    if p.is_absolute():
        return p
    base = Path(cfg_paths.get("checkpoints_dir", "./models/checkpoints"))
    return (base / p).resolve()

def _load_autoencoder(
    cfg_models: Dict[str, Dict[str, Any]],
    ae_type: str,
    device: str,
    cfg_paths: Dict[str, Any] | None = None,
) -> Optional[torch.nn.Module]:
    """Instantiate and load the requested autoencoder."""
    if ae_type == "none":
        return None

    if ae_type not in cfg_models:
        raise ValueError(f"[CONFIG] Auto‑encoder '{ae_type}' not found in 'models'.")

    mcfg = cfg_models[ae_type]
    input_dim  = mcfg.get("input_dim", 384)
    latent_dim = mcfg.get("latent_dim", 64)
    hidden_dim = mcfg.get("hidden_dim", 512)

    # --- Factory by ae_type (without requiring 'class' in YAML)
    if ae_type == "vae":
        model: torch.nn.Module = VariationalAutoencoder(input_dim, latent_dim, hidden_dim)
    elif ae_type == "dae":
        model = DenoisingAutoencoder(input_dim, latent_dim, hidden_dim)
    elif ae_type == "contrastive":
        model = ContrastiveAutoencoder(input_dim, latent_dim, hidden_dim)
    else:
        raise RuntimeError("Unrecognized AE type.")

    # --- Resolve checkpoint relative to paths.checkpoints_dir if needed
    ckpt = _resolve_ckpt_path(mcfg.get("checkpoint"), cfg_paths or {})
    if ckpt and ckpt.exists():
        model.load_state_dict(torch.load(str(ckpt), map_location=device))
    else:
        raise FileNotFoundError(
            f"Checkpoint for '{ae_type}' not found: {ckpt} "
            f"(check 'paths.checkpoints_dir' and 'models.{ae_type}.checkpoint')"
        )

    return model.to(device).eval()



# ---------------------------------------------------------------------------
# Pipeline steps
# ---------------------------------------------------------------------------

def _encode_corpus(
    compressor: EmbeddingCompressor,
    texts: Sequence[str],
    compress: bool = True,
) -> torch.Tensor:
    """Return document embeddings as `[N × D]` float32 CPU tensor."""

    return compressor.encode_text(list(texts), compress=compress)


def _evaluate_retrieval(
    retrieved: Sequence[Sequence[str]],
    relevant: Sequence[Sequence[str]] | Sequence[str],
    metrics: List[str],
) -> Dict[str, Dict[str, float]]:
    """Wrapper around `evaluate_retrieval` with sensible defaults."""

    return evaluate_retrieval(retrieved, relevant, metrics=metrics)


# ---------------------------------------------------------------------------
# Core runner
# ---------------------------------------------------------------------------

# --------------------------------------------------------------------------- #
#  PIPELINE RUNNER                                                            #
# --------------------------------------------------------------------------- #
class PipelineRunner:
    """Orchestrates the RAG pipeline: encode → retrieve → (optional) generate → evaluate."""

    def __init__(self, cfg: Dict[str, Any], ae_type: str, logger):
        """Initialize the pipeline runner.

        Args:
            cfg: Configuration dictionary.
            ae_type: Type of autoencoder used.
            logger: Logger instance.
        """
        self.cfg = cfg
        self.ae_type = ae_type
        self.logger = logger

        self.device = resolve_device(cfg.get("training", {}).get("device"))
        self.logger.main.info("Device resolved → %s", self.device)

        # Compressor (SBERT ± AE)
        ae_model = _load_autoencoder(cfg["models"], ae_type, self.device, cfg.get("paths", {}))
        self.compressor = EmbeddingCompressor(
            base_model_name=cfg["embedding_model"]["name"],
            autoencoder=ae_model,
            device=self.device,
        )
        self.logger.main.info("Compressor ready (AE = %s)", ae_type)

        # Retrieval configuration
        self.retr_cfg = cfg.get("retrieval", {})
        self.retriever = None  # Set in _build_retriever

        # Generator
        self.generator = RAGGenerator(cfg)

    # ------------------------------------------------------------------ #
    def _build_retriever(
        self,
        doc_embeddings: torch.Tensor,
        corpus: Sequence[str],
        doc_ids: Sequence[int],
    ):
        """Initialise retrieval backend (FAISS / BruteForce)."""
        t0 = time.perf_counter()
        self.retriever = build_retriever(
            embeddings=doc_embeddings,
            texts=corpus,
            doc_ids=doc_ids,
            cfg=self.retr_cfg,
        )
        self.logger.main.info(
            "Retriever backend '%s' initialised in %.2f s",
            self.retr_cfg.get("backend", "faiss"),
            time.perf_counter() - t0,
        )

    # ------------------------------------------------------------------ #
    def process(
        self,
        queries: Sequence[str],
        corpus: Sequence[str],
        relevant_docs: Optional[Sequence[Sequence[str]]] = None,
        generate: bool = False,
    ) -> None:
        """Run the pipeline: encode → retrieve → (optional) generate → evaluate.

        Args:
            queries: List of query strings.
            corpus: List of document strings.
            relevant_docs: Ground-truth relevant documents for evaluation.
            generate: Whether to run the generation step.
        """
        self.logger.main.info(
            "Running pipeline: |queries|=%d |corpus|=%d", len(queries), len(corpus)
        )

        # Immutable copy for doc-level evaluation
        orig_docs = list(corpus)
        context2docid: Dict[str, int] = {t: i for i, t in enumerate(orig_docs)}

        # Optional chunking for inference
        ch_cfg = self.cfg.get("chunking", {})
        use_chunking = bool(ch_cfg.get("enabled", False))
        if use_chunking:
            from utils.data_utils import prepare_inference_chunks
            chunks, chunk_index = prepare_inference_chunks(
                orig_docs,
                mode=ch_cfg.get("mode", "sliding"),
                max_tokens=ch_cfg.get("max_tokens", 128),
                stride=ch_cfg.get("stride", 64),
                min_tokens=ch_cfg.get("min_tokens", 48),
                tokenizer_name=ch_cfg.get("tokenizer_name", self.cfg["embedding_model"]["name"]),
                index_out=ch_cfg.get("index_out"),
                store_chunk_text=ch_cfg.get("store_chunk_text", True),
            )
            corpus = chunks
            corpus_doc_ids: List[int] = chunk_index["doc_id"].astype(int).tolist()  # chunk → doc
            self.logger.main.info("Chunking enabled: |docs|=%d → |chunks|=%d", len(orig_docs), len(corpus))
            self.logger.main.debug("Chunking configuration: %s", ch_cfg)
        else:
            corpus_doc_ids = list(range(len(corpus)))

        # Encode corpus once
        doc_embeddings = _encode_corpus(self.compressor, corpus, compress=True)

        # Build or load retrieval index
        self._build_retriever(doc_embeddings, corpus, corpus_doc_ids)

        # Encode queries
        query_embeddings = _encode_corpus(self.compressor, queries, compress=True)

        # Retrieve with doc-level MaxSim aggregation
        top_k = int(self.retr_cfg.get("top_k", 10))
        cand_k = int(self.retr_cfg.get("candidate_k", top_k * 3 if use_chunking else top_k))

        all_retrieved_docids: List[List[int]] = []
        answers: List[str] = []

        for idx, (q, q_emb) in enumerate(zip(queries, query_embeddings)):
            texts_k, scores_k, docids_k = self.retriever.retrieve(q_emb, top_k=cand_k)

            # Aggregation: doc-level MaxSim
            agg: Dict[int, float] = {}
            for did, sc in zip(docids_k, scores_k):
                prev = agg.get(did)
                if (prev is None) or (sc > prev):
                    agg[did] = sc

            # Re-rank docs by max score (desc) and truncate to top_k unique
            ranked_docids = sorted(agg, key=agg.get, reverse=True)[:top_k]
            all_retrieved_docids.append(ranked_docids)

            # Optional: context for LLM prioritizing chunks of top docs
            if generate:
                per_doc_cap = int(self.retr_cfg.get("max_chunks_per_doc", 2))
                used: Dict[int, int] = {d: 0 for d in ranked_docids}
                selected_chunks: List[str] = []
                for t, d in zip(texts_k, docids_k):
                    if d in used and used[d] < per_doc_cap:
                        selected_chunks.append(t)
                        used[d] += 1
                    if len(selected_chunks) >= max(1, per_doc_cap * len(ranked_docids)):
                        break
                ctx_for_llm = selected_chunks if selected_chunks else texts_k[:top_k]
                ans = self.generator.generate(q, ctx_for_llm)
                answers.append(ans)
                self.logger.main.debug("[%d] Q: %s | A: %s", idx, q, (ans[:60] + "…") if ans else "")

        # Evaluation (doc-level, using doc_ids)
        if relevant_docs:
            relevant_doc_ids: List[List[int]] = []
            missing = 0
            for rel_list in relevant_docs:
                ids = []
                for ctx in rel_list:
                    did = context2docid.get(ctx)
                    if did is not None:
                        ids.append(did)
                    else:
                        missing += 1
                relevant_doc_ids.append(ids)
            if missing:
                self.logger.main.warning("Relevant items missing from mapping: %d", missing)

            retrieved_as_str = [[str(did) for did in row] for row in all_retrieved_docids]
            relevant_as_str = [[str(did) for did in row] for row in relevant_doc_ids]

            eval_cfg = self.cfg.get("evaluation", {})
            ret_metrics = _evaluate_retrieval(
                retrieved_as_str,
                relevant_as_str,
                metrics=eval_cfg.get("retrieval_metrics", ["Recall@5"]),
            )
            rprint("\n[Retrieval evaluation]\n")
            for k, v in ret_metrics.items():
                rprint(f"{k}: {v['mean']:.4f} ± {v['std']:.4f}")

        # Optional generation metrics (requires many samples)
        if generate and relevant_docs and len(queries) >= 100:
            gen_metrics = eval_generation(
                references=[r[0] for r in relevant_docs],
                candidates=answers,
                metrics=self.cfg.get("evaluation", {}).get("generation_metrics", ["ROUGE-L"]),
            )
            rprint("\n[Generation evaluation]\n")
            for m, d in gen_metrics.items():
                rprint(f"{m}: {d['mean']:.2f} (CI 95%: {d['ci_lower']:.2f}–{d['ci_upper']:.2f})")


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def _parse_args() -> argparse.Namespace:  # noqa: D401
    """Return command‑line arguments."""

    pre_parser = argparse.ArgumentParser(add_help=False)
    pre_parser.add_argument("--config", default="./config/config.yaml")
    known, _ = pre_parser.parse_known_args(sys.argv[1:])

    cfg = load_config(known.config)
    valid_ae = list(cfg.get("models", {}).keys()) + ["none", "all"]

    parser = argparse.ArgumentParser(description="Run RAG‑AE experimental pipeline")
    parser.add_argument("--config", default="./config/config.yaml", help="Path to YAML config")
    parser.add_argument("--ae_type", default="vae", choices=valid_ae, help="Select auto‑encoder variant")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")

    parser.add_argument("--dataset", choices=["squad", "uda"], default="squad",
                    help="Dataset for evaluation (SQuAD or UDA)")
    parser.add_argument("--max_samples", type=int, default=2000,
                        help="Maximum number of queries to use")
    parser.add_argument("--benchmark", action="store_true",
                        help="Compare against BM25, DPR, SBERT, AE...")
    parser.add_argument("--generate", action="store_true", help="Run generation step (RAG)")

    return parser.parse_args()


# ---------------------------------------------------------------------------
# Entry‑point
# ---------------------------------------------------------------------------

def main() -> None:  # noqa: D401 – standard script
    args = _parse_args()

    cfg = load_config(args.config)
    log = init_logger(cfg.get("logging", {}))
    set_seed(args.seed, cfg.get("training", {}).get("deterministic", False), logger=log.train)
    load_dotenv()

    ae_variants = (
        [args.ae_type]
        if args.ae_type != "all"
        else [k for k in cfg.get("models", {}).keys() if k in {"vae", "dae", "contrastive"}]
    )

    # --------------------------------------------------------------------- Toy corpus (replace with real dataset) --
    queries, corpus, relevant = load_evaluation_data(args.dataset, max_samples=args.max_samples)

    # --------------------------------------------------------------------- Run each variant
    for ae in ae_variants:
        rprint(f"\n[bold cyan]==== PIPELINE ({ae.upper()}) ====\n[/]")
        _print_run_card(cfg, ae, generate=args.generate)
        runner = PipelineRunner(cfg, ae, log)
        runner.process(queries, corpus, relevant_docs=relevant, generate=args.generate)


if __name__ == "__main__":
    main()

"""

models/base_autoencoder.py
"""
import torch
import torch.nn as nn
from abc import ABC, abstractmethod

class BaseAutoencoder(nn.Module, ABC):
    def __init__(self, input_dim: int, latent_dim: int):
        super(BaseAutoencoder, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim

    @abstractmethod
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        pass

    @abstractmethod
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        pass

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encode(x)
        return self.decode(z)

"""

models/checkpoints/vae_text.pth:Zone.Identifier
"""

"""

models/contrastive_autoencoder.py
"""
# /models/contrastive_autoencoder.py
import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder

class ContrastiveAutoencoder(BaseAutoencoder):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super(ContrastiveAutoencoder, self).__init__(input_dim, latent_dim)

        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # Useful if input vectors are normalized
        )

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encoder(x)
        return torch.nn.functional.normalize(z, p=2, dim=-1)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)
      
    def forward(self, x: torch.Tensor):
        z = self.encode(x)
        x_recon = self.decode(z)
        return x_recon, z

"""

models/denoising_autoencoder.py
"""
# /models/denoising_autoencoder.py

import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder
import torch.nn.functional as F

class DenoisingAutoencoder(BaseAutoencoder):
    """Feed‑forward Denoising Autoencoder.

    The dataset must supply *noisy* inputs; the model learns to reconstruct the
    clean version. Use `dae_loss` (MSE) during training.
    """

    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super().__init__(input_dim, latent_dim)

        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # assume inputs ∈ [0,1]; change if different scale
        )

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        return self.encoder(x)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encode(x)
        return self.decode(z)

"""

models/variational_autoencoder.py
"""
# / models/variational_autoencoder.py
import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder

class VariationalAutoencoder(BaseAutoencoder):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super(VariationalAutoencoder, self).__init__(input_dim, latent_dim)
        
        # Encoder: proyecciones a la media y desviación estándar
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.mu_layer = nn.Linear(hidden_dim, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # Asumimos entrada normalizada (0-1)
        )

    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        h = self.encoder(x)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        return mu, logvar

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = mu if not self.training else self.reparameterize(mu, logvar)
        x_reconstructed = self.decode(z)
        return x_reconstructed, mu, logvar

"""

requeriments.txt
"""
# Core ML & Data Processing
torch>=2.0.0
transformers>=4.38.0
sentence-transformers>=2.2.2
scikit-learn>=1.3.0
numpy>=1.24.0
pandas>=2.0.0

datasets>=2.19.0
scipy>=1.10.0
openai>=1.14.0
pyserini>=0.21
faiss-cpu
faiss-gpu

# Visualization & Analysis
matplotlib>=3.7.0
seaborn>=0.12.0

# Evaluation Metrics (Efficient and Torch-Compatible)
rouge-score >=0.1.2
sacrebleu>=2.5.1

# Configuration & Utilities
pyyaml>=6.0
python-dotenv>=1.0.0

# Optional CLI Enhancements
rich>=13.0.0

pytest 
"""

retrieval/FAISSEmbeddingRetriever.py
"""
# retrieval/FAISSEmbeddingRetriever.py
from __future__ import annotations

from pathlib import Path
from typing import Sequence, Tuple, List, Optional, Dict, Any
import json

import numpy as np
import faiss
import torch


class FAISSEmbeddingRetriever:
    """
    Indexador y recuperador FAISS para embeddings densos.

    Características:
      - Métrica INNER_PRODUCT (IP) con normalización L2: IP ≈ coseno.
      - Persistencia opcional de índice + metadatos (textos, doc_ids y fingerprint).
      - Reconstrucción automática si el índice en disco no es compatible
        (dimensión, modelo de embedding, AE, chunking, etc.).
      - Comprobación de sanidad tras indexar (self-search de un vector).
    """

    # ------------------------------- Init ---------------------------------- #
    def __init__(
        self,
        embedding_dim: int,
        index_path: Optional[str | Path] = None,
        index_type: str = "hnsw",   # "flatip" | "hnsw" | "ivfpq"
        use_gpu: bool = False,
        *,
        hnsw_M: int = 32,
        ef_construction: int = 200,
        ef_search: int = 64,
    ):
        self.d = int(embedding_dim)
        self.index_type = index_type
        self.path = Path(index_path) if index_path else None
        self.use_gpu = use_gpu

        self.hnsw_M = int(hnsw_M)
        self.ef_construction = int(ef_construction)
        self.ef_search = int(ef_search)

        # Memoria de metadatos
        self._texts: List[str] = []
        self._doc_ids: List[int] = []
        self.meta_fp: Dict[str, Any] = {}

        # Índice (CPU por defecto)
        self.index = self._build_index(self.d, self.index_type)

        # Cargar índice si existe (lo validaremos en build())
        if self.path and self.path.exists():
            try:
                cpu_index = faiss.read_index(str(self.path), faiss.IO_FLAG_MMAP)
                # No cambiamos aún: la compatibilidad se decide en build()
                self.index = cpu_index
                self._load_metadata()
            except Exception:
                # Archivo corrupto o incompatible → empezamos limpio
                self.index = self._build_index(self.d, self.index_type)
                self._texts, self._doc_ids, self.meta_fp = [], [], {}

        # Mover a GPU si procede
        self.gpu_enabled = False
        if self.use_gpu and hasattr(faiss, "StandardGpuResources"):
            try:
                if faiss.get_num_gpus() > 0:
                    res = faiss.StandardGpuResources()
                    self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
                    self.gpu_enabled = True
                else:
                    print("[WARN] FAISS GPU no disponible. Uso CPU.")
            except Exception:
                print("[WARN] FAISS GPU no disponible. Uso CPU.")

        # Ajuste de parámetros HNSW
        self._maybe_set_hnsw_params(self.index)

    # ------------------------------ Helpers -------------------------------- #
    def _build_index(self, d: int, kind: str) -> faiss.Index:
        if kind == "flatip":                      # Exacto, IP
            return faiss.IndexFlatIP(d)
        if kind == "hnsw":                        # Aproximado, IP
            idx = faiss.IndexHNSWFlat(d, self.hnsw_M, faiss.METRIC_INNER_PRODUCT)
            idx.hnsw.efConstruction = self.ef_construction
            idx.hnsw.efSearch = self.ef_search
            return idx
        if kind == "ivfpq":                       # Cuantización (no recomend. para N pequeño)
            quant = faiss.IndexFlatIP(d)
            return faiss.IndexIVFPQ(quant, d, 4096, 16, 8)
        raise ValueError(f"Index type not supported: {kind}")

    def _maybe_set_hnsw_params(self, index: faiss.Index) -> None:
        # Ajusta efSearch si es HNSW
        if isinstance(index, faiss.IndexHNSW):
            index.hnsw.efSearch = self.ef_search
        # Si está en GPU y es HNSW, FAISS gestiona internamente los equivalentes.

    def _meta_path(self) -> Path:
        assert self.path is not None
        return self.path.with_suffix(self.path.suffix + ".meta.json")

    def _save_metadata(self) -> None:
        if not self.path:
            return
        meta = {
            "texts": self._texts,
            "doc_ids": self._doc_ids,
            "fingerprint": self.meta_fp,
        }
        self._meta_path().parent.mkdir(parents=True, exist_ok=True)
        with self._meta_path().open("w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False)

    def _load_metadata(self) -> None:
        if not self.path:
            return
        mp = self._meta_path()
        if not mp.exists():
            self._texts, self._doc_ids, self.meta_fp = [], [], {}
            return
        with mp.open("r", encoding="utf-8") as f:
            meta = json.load(f)
        self._texts = list(meta.get("texts", []))
        self._doc_ids = list(meta.get("doc_ids", []))
        self.meta_fp = dict(meta.get("fingerprint", {}))

    @staticmethod
    def _normalize_l2_inplace(x: np.ndarray) -> None:
        # IP ≈ coseno si normalizamos L2
        faiss.normalize_L2(x)

    @staticmethod
    def _fingerprint(
        *,
        d: int,
        embedding_model: Optional[str],
        ae_type: Optional[str],
        latent_dim: Optional[int],
        chunking_cfg: Optional[Dict[str, Any]],
        metric: str = "ip",
        normalize_l2: bool = True,
        version: int = 1,
    ) -> Dict[str, Any]:
        ch = chunking_cfg or {}
        return {
            "d": int(d),
            "embedding_model": embedding_model,
            "ae_type": ae_type,
            "latent_dim": int(latent_dim) if latent_dim is not None else None,
            "chunking": {
                "enabled": bool(ch.get("enabled", False)),
                "mode": ch.get("mode", "sliding"),
                "max_tokens": int(ch.get("max_tokens", 128)) if ch.get("max_tokens") is not None else None,
                "stride": int(ch.get("stride", 64)) if ch.get("stride") is not None else None,
                "min_tokens": int(ch.get("min_tokens", 48)) if ch.get("min_tokens") is not None else None,
            },
            "metric": metric,
            "normalize_l2": bool(normalize_l2),
            "version": int(version),
        }

    def _compatible(self, current_fp: Dict[str, Any]) -> bool:
        m = self.meta_fp or {}
        keys = ["d", "embedding_model", "ae_type", "latent_dim", "metric", "normalize_l2", "version"]
        if any(m.get(k) != current_fp.get(k) for k in keys):
            return False
        mch = (m.get("chunking") or {})
        cch = (current_fp.get("chunking") or {})
        for k in ["enabled", "mode", "max_tokens", "stride", "min_tokens"]:
            if mch.get(k) != cch.get(k):
                return False
        return True

    # ------------------------------- Build --------------------------------- #
    def build(
        self,
        embeddings: torch.Tensor,           # [N, D] (CPU/GPU), float32 preferido
        texts: Sequence[str],
        doc_ids: Sequence[int] | None = None,
        train: bool = True,
        *,
        embedding_model_name: Optional[str] = None,
        ae_type: Optional[str] = None,
        latent_dim: Optional[int] = None,
        chunking_cfg: Optional[Dict[str, Any]] = None,
    ) -> None:
        assert len(embeddings) == len(texts), "len mismatch (embeddings vs texts)"
        if doc_ids is not None:
            assert len(texts) == len(doc_ids), "len mismatch (texts vs doc_ids)"

        # 1) Preparar X (CPU, float32, C-contiguous) + normalización L2
        x = embeddings.detach().cpu().numpy().astype("float32", copy=False)
        self._normalize_l2_inplace(x)

        # 2) Fingerprint actual
        cur_fp = self._fingerprint(
            d=int(x.shape[1]),
            embedding_model=embedding_model_name,
            ae_type=ae_type,
            latent_dim=latent_dim,
            chunking_cfg=chunking_cfg,
            metric="ip",
            normalize_l2=True,
            version=1,
        )

        # 3) Validar índice en disco; si incompatible → reconstruir limpio
        rebuild = False
        if hasattr(self.index, "d") and int(getattr(self.index, "d")) != cur_fp["d"]:
            rebuild = True
        if (self.path and self.path.exists()) and (not self._compatible(cur_fp)):
            rebuild = True

        if rebuild:
            # Volver a CPU si estamos en GPU
            if getattr(self, "gpu_enabled", False) and hasattr(faiss, "index_gpu_to_cpu"):
                try:
                    self.index = faiss.index_gpu_to_cpu(self.index)
                except Exception:
                    pass
            # Reconstruir
            self.index = self._build_index(cur_fp["d"], self.index_type)
            self._texts, self._doc_ids, self.meta_fp = [], [], {}
            self._maybe_set_hnsw_params(self.index)
            # Volver a GPU si aplica
            if self.use_gpu and hasattr(faiss, "StandardGpuResources"):
                try:
                    if faiss.get_num_gpus() > 0:
                        res = faiss.StandardGpuResources()
                        self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
                        self.gpu_enabled = True
                except Exception:
                    self.gpu_enabled = False

        # 4) Entrenar si aplica (IVF/IVFPQ) → con X normalizado
        if train and hasattr(self.index, "train") and not self.index.is_trained:
            self.index.train(x)

        # 5) Añadir vectores
        self.index.add(x)

        # 6) Comprobación de sanidad mínima: self-search
        try:
            D_chk, I_chk = self.index.search(x[:1], 1)
            if I_chk.shape[0] == 0 or I_chk[0, 0] != 0:
                print("[ERROR] FAISS sanity check failed; rebuilding index")
                # Reconstruir y re-add
                if getattr(self, "gpu_enabled", False) and hasattr(faiss, "index_gpu_to_cpu"):
                    try:
                        self.index = faiss.index_gpu_to_cpu(self.index)
                    except Exception:
                        pass
                self.index = self._build_index(cur_fp["d"], self.index_type)
                self._maybe_set_hnsw_params(self.index)
                if train and hasattr(self.index, "train") and not self.index.is_trained:
                    self.index.train(x)
                self.index.add(x)
        except Exception:
            # Si algo falla en el sanity, reconstruimos igualmente
            self.index = self._build_index(cur_fp["d"], self.index_type)
            self._maybe_set_hnsw_params(self.index)
            if train and hasattr(self.index, "train") and not self.index.is_trained:
                self.index.train(x)
            self.index.add(x)

        # 7) Metadatos en memoria
        self._texts.extend(list(texts))
        self._doc_ids.extend(list(doc_ids) if doc_ids is not None else [-1] * len(texts))
        self.meta_fp = cur_fp

        # 8) Persistencia (si se configuró path). Siempre escribir en CPU.
        if self.path:
            self.path.parent.mkdir(parents=True, exist_ok=True)
            cpu_index = self.index
            if getattr(self, "gpu_enabled", False) and hasattr(faiss, "index_gpu_to_cpu"):
                try:
                    cpu_index = faiss.index_gpu_to_cpu(self.index)
                except Exception:
                    cpu_index = self.index  # fallback
            faiss.write_index(cpu_index, str(self.path))
            self._save_metadata()

        # 9) Log breve (útil para auditoría)
        try:
            ntotal = getattr(self.index, "ntotal", -1)
            print(f"[FAISS] type={type(self.index).__name__} d={cur_fp['d']} ntotal={ntotal} metric=IP normL2=True")
        except Exception:
            pass

    # ------------------------------ Retrieve ------------------------------- #
    def retrieve(self, query_emb: torch.Tensor, top_k: int = 10) -> Tuple[List[str], List[float], List[int]]:
        if query_emb.dim() == 1:
            query_emb = query_emb.unsqueeze(0)
        q = query_emb.detach().cpu().numpy().astype("float32", copy=False)
        self._normalize_l2_inplace(q)

        D, I = self.index.search(q, top_k)  # IP → D = similitudes (mayor=mejor)
        idxs = I[0].tolist()

        # Protección si faltan metadatos (compatibilidad)
        if not self._texts or not self._doc_ids:
            self._load_metadata()

        texts = [self._texts[i] for i in idxs]
        scores = D[0].tolist()
        docids = [self._doc_ids[i] for i in idxs]
        return texts, scores, docids

"""

retrieval/base.py
"""
from __future__ import annotations
from typing import Protocol, Sequence, Tuple, List

class BaseRetriever(Protocol):
    """Interface for all first-stage retrievers."""
    def build_index(self, corpus: Sequence[str]) -> None: ...
    def retrieve(self, query: str, k: int) -> List[Tuple[str, float]]: ...

"""

retrieval/bm25.py
"""
from pyserini.search import SimpleSearcher            # pip install pyserini>=0.21
from retrieval.base import BaseRetriever
import tempfile, os, json

class BM25Retriever(BaseRetriever):
    def __init__(self, bm25_k1: float = 0.9, b: float = 0.4):
        self.k1, self.b = bm25_k1, b
        self._searcher = None
        self._tmpdir   = tempfile.mkdtemp()

    def build_index(self, corpus):
        # 1) escribir cada doc en un fichero JSONL (id + text)
        tmp_jsonl = os.path.join(self._tmpdir, "docs.jsonl")
        with open(tmp_jsonl, "w", encoding="utf-8") as f:
            for i, doc in enumerate(corpus):
                f.write(json.dumps({"id": str(i), "contents": doc}) + "\n")

        # 2) invocar indexador Lucene
        from pyserini.index import build_index
        build_index(tmp_jsonl, self._tmpdir, overwrite=True)

        # 3) abrir buscador Lucene
        self._searcher = SimpleSearcher(self._tmpdir)
        self._searcher.set_bm25(self.k1, self.b)

    def retrieve(self, query, k):
        hits = self._searcher.search(query, k)
        return [(h.raw, float(h.score)) for h in hits]

"""

retrieval/bruteforce.py
"""
# retrieval/bruteforce.py
from __future__ import annotations
from typing import Sequence, Tuple, List, Literal

import torch
import torch.nn.functional as F

Similarity = Literal["cosine", "euclidean"]

class BruteForceRetriever:
    def __init__(
        self,
        embeddings: torch.Tensor,   # [N × D] CPU float32
        texts: Sequence[str],
        doc_ids: Sequence[int] | None = None,
        metric: Similarity = "cosine",
    ):
        if doc_ids is not None:
            assert len(texts) == len(doc_ids), "len mismatch (texts vs doc_ids)"

        if embeddings.device.type != "cpu":
            embeddings = embeddings.cpu()

        self.texts = list(texts)
        self.doc_ids = list(doc_ids) if doc_ids is not None else list(range(len(texts)))
        self.metric = metric

        self.emb = embeddings
        if metric == "cosine":
            self.emb = F.normalize(self.emb, p=2, dim=1)

    def retrieve(
        self, query_emb: torch.Tensor, top_k: int = 10
    ) -> Tuple[List[str], List[float], List[int]]:
        if query_emb.device.type != "cpu":
            query_emb = query_emb.cpu()

        if self.metric == "cosine":
            q = F.normalize(query_emb, p=2, dim=0)
            scores = torch.mv(self.emb, q)            # [N]
        elif self.metric == "euclidean":
            diff = self.emb - query_emb
            scores = -torch.norm(diff, dim=1)
        else:
            raise ValueError(self.metric)

        top_k = min(top_k, scores.numel())
        vals, idxs = torch.topk(scores, k=top_k)
        idxs = idxs.tolist()
        return (
            [self.texts[i] for i in idxs],
            vals.tolist(),
            [self.doc_ids[i] for i in idxs],
        )

"""

retrieval/dpr.py
"""
from retrieval.base import BaseRetriever
from sentence_transformers import SentenceTransformer
import faiss, numpy as np

class DPRRetriever(BaseRetriever):
    """Dense Passage Retrieval (dual-encoder)."""
    def __init__(self,
                 q_model: str = "facebook-dpr-question_encoder-single-nq-base",
                 p_model: str = "facebook-dpr-ctx_encoder-single-nq-base",
                 device: str | None = None):
        self.q_encoder = SentenceTransformer(q_model, device=device)
        self.p_encoder = SentenceTransformer(p_model, device=device)
        self.index = None
        self.docs  = []

    def build_index(self, corpus):
        self.docs = list(corpus)
        emb = self.p_encoder.encode(self.docs,
                                    batch_size=64,
                                    convert_to_numpy=True,
                                    normalize_embeddings=True)
        d = emb.shape[1]
        self.index = faiss.IndexHNSWFlat(d, 32)
        self.index.hnsw.efConstruction = 200
        self.index.add(emb.astype("float32"))

    def retrieve(self, query, k):
        q_emb = self.q_encoder.encode([query],
                                      convert_to_numpy=True,
                                      normalize_embeddings=True)
        dist, idx = self.index.search(q_emb.astype("float32"), k)
        return [(self.docs[i], float(-dist[0][j])) for j, i in enumerate(idx[0])]

"""

retrieval/embedder.py
"""
# retrieval/embedder.py

from sentence_transformers import SentenceTransformer
import torch


class EmbeddingCompressor:
    def __init__(
        self,
        base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        autoencoder: torch.nn.Module = None,
        device: str = None
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # Load pretrained SBERT model (includes pooling + normalization)
        self.model = SentenceTransformer(base_model_name, device=self.device)

        # Optional autoencoder for compression (VAE, DAE, etc.)
        self.autoencoder = autoencoder.to(self.device) if autoencoder else None
        if self.autoencoder:
            self.autoencoder.eval()

    def encode_text(self, texts: list[str], compress: bool = True) -> torch.Tensor:
        """Returns SBERT embeddings, optionally compressed with an autoencoder.

        Args:
            texts: List of input strings to encode.
            compress: Whether to apply the autoencoder (if available).

        Returns:
            A float32 tensor [N × D] on CPU.
        """
        with torch.no_grad():
            embeddings = self.model.encode(
                texts,
                batch_size=64,
                convert_to_tensor=True,
                normalize_embeddings=True
            ).to(self.device)

            if self.autoencoder and compress:
                encoded = self.autoencoder.encode(embeddings)
                if isinstance(encoded, tuple):  # VAE returns (mu, logvar)
                    encoded = encoded[0]        # use mean as latent code
                return encoded.cpu()

            return embeddings.cpu()

"""

retrieval/retriever.py
"""
# /retrieval/retriever.py

from __future__ import annotations

import torch
import torch.nn.functional as F
import numpy as np
from sklearn.covariance import EmpiricalCovariance
from typing import List, Tuple, Literal


# FAISS

from typing import Sequence
from retrieval.FAISSEmbeddingRetriever import FAISSEmbeddingRetriever

def build_retriever(
    embeddings: torch.Tensor,
    texts: Sequence[str],
    doc_ids: Sequence[int],
    cfg: dict,
):
    if cfg.get("backend", "faiss") == "faiss":
        ret = FAISSEmbeddingRetriever(
            embedding_dim=embeddings.size(1),
            index_path=cfg.get("index_path"),
            index_type=cfg.get("index_type", "hnsw"),
            use_gpu=cfg.get("use_gpu", False),
        )
        ret.build(embeddings, texts, doc_ids, train=True)
        return ret
    else:
        from retrieval.bruteforce import BruteForceRetriever
        return BruteForceRetriever(embeddings, texts, doc_ids)

"""

save_snapshot.sh
"""
#!/usr/bin/env bash
#
# save_snapshot.sh  –  Dump a folder’s tree plus every file’s contents
# Usage:
#   ./save_snapshot.sh [TARGET_DIR] [OUTPUT_TXT]
# Defaults:
#   TARGET_DIR="."                  (current directory)
#   OUTPUT_TXT="snapshot.txt"       (created/overwritten)

set -euo pipefail

###############################################################################
# 1. Input handling
###############################################################################
TARGET_DIR="${1:-.}"
OUTPUT_TXT="${2:-snapshot.txt}"

# Verify prerequisites
command -v tree >/dev/null 2>&1 || {
  printf 'Error: "tree" command not found. Please install it first.\n' >&2; exit 1; }

# Avoid accidental overwrite of important files
if [[ -e "$OUTPUT_TXT" && ! -w "$OUTPUT_TXT" ]]; then
  printf 'Error: Output file "%s" is not writable.\n' "$OUTPUT_TXT" >&2
  exit 1
fi

###############################################################################
# 2. Capture the directory tree
###############################################################################
# Truncate/overwrite existing output
: > "$OUTPUT_TXT"

printf '### Directory tree for: %s\n\n' "$TARGET_DIR" >> "$OUTPUT_TXT"
tree -F -I '__pycache__' "$TARGET_DIR" >> "$OUTPUT_TXT" || {
  printf 'Warning: Unable to generate tree for "%s".\n' "$TARGET_DIR" >&2; }

printf '\n\n### File contents\n\n' >> "$OUTPUT_TXT"

###############################################################################
# 3. Append each file (relative path + contents)
###############################################################################
# Absolute path to output for comparison
ABS_OUTPUT="$(realpath "$OUTPUT_TXT")"

# Exclude hidden files, __pycache__, and specific extensions
find "$TARGET_DIR" -type f \
  ! -path '*/.*/*' \
  ! -name '.*' \
  ! -path '*/__pycache__/*' \
  -print0 | sort -z |
while IFS= read -r -d '' FILE
do
  # Resolve absolute path of the file
  ABS_FILE="$(realpath "$FILE")"

  # Skip the output file itself
  if [[ "$ABS_FILE" == "$ABS_OUTPUT" ]]; then
    continue
  fi

  # Skip files with undesired extensions
  case "$FILE" in
    *.pt|*.pth|*.ipynb|*.parquet|*.log|*.json|*.tmp|*.bak|*.swp|*.zip|*.tar|*.gz|*.rar|*.DS_Store|*.png|*.jpg|*.jpeg|*.gif|*.bmp|*.tiff|*.ico|*.webp|*.svg|*.mp4|*.avi|*.mkv|*.mov|*.wmv|*.flv|*.mp3|*.wav|*.ogg|*.code-workspace|*.vscode|*.idea|*.git|*.svn|*.hg|*.faiss)
      continue
      ;;
  esac

  # Remove leading base path for relative display
  REL_PATH="${FILE#$TARGET_DIR/}"

  # Header
  printf '%s\n' "$REL_PATH" >> "$OUTPUT_TXT"
  printf '"""\n' >> "$OUTPUT_TXT"

  # File contents
  if ! cat "$FILE" >> "$OUTPUT_TXT" 2>/dev/null; then
    printf '[[[ Error reading file ]]]\n' >> "$OUTPUT_TXT"
    printf 'Warning: Could not read "%s".\n' "$REL_PATH" >&2
  fi

  # Footer
  printf '\n"""\n\n' >> "$OUTPUT_TXT"
done

printf 'Snapshot saved to: %s\n' "$OUTPUT_TXT"

"""

style_guide.md
"""
# Project-wide Code Style Guide (English)

> **Scope**  All Python modules, notebooks, shell scripts and configuration files located under the `tfm` repository.  New code **must** comply immediately.  Legacy code should be progressively refactored.

---

## 1  General Principles

| Principle                         | Rationale                                                        |
| --------------------------------- | ---------------------------------------------------------------- |
| **Consistency over cleverness**   | Readability and maintenance outweigh micro-optimisations.        |
| **Explicit > implicit**           | Follow *The Zen of Python*. Avoid hidden state and side effects. |
| **Fail fast, fail loud**          | Raise specific exceptions early; log context-rich messages.      |
| **Pure functions where possible** | Functions that depend only on arguments are easier to test.      |

---

## 2  File & Folder Layout

* Package modules use **snake\_case** filenames: `contrastive_autoencoder.py`.
* Public executables go under `/scripts` with a short shebang and CLI (`argparse`).
* Unit tests mirror the package tree under `/tests`.
* Keep data or model artefacts out of Git; place under `/data` or `/models/checkpoints` and add to `.gitignore`.

---

## 3  Imports

```
# 1. Standard library
import os
import json

# 2. Third-party
import numpy as np
import torch

# 3. First-party (this repo)
from utils.load_config import load_config
```

* Use **absolute imports** inside the package.
* Never use `from module import *`.
* Group imports and separate blocks with one blank line.

---

## 4  Naming Conventions

* **snake\_case** for variables, functions and methods.
* **PascalCase** for classes and `Enum` members.
* **UPPER\_SNAKE\_CASE** for module-level constants.
* Avoid Spanish identifiers; prefer descriptive English (`embedding_dim`, not `dim_emb`).

---

## 5  Docstrings & Comments

* **Every** public module, function, class and method **must** have a docstring in **English** using the **Google style**.
* Keep inline comments short; they explain *why*, not *what*.
* TODO/FIXME tags must include assignee or ticket reference.

```python
def recall_at_k(retrieved: Sequence[str], relevant: Sequence[str], k: int) -> float:
    """Return Recall@k.

    Args:
        retrieved: Ordered list of retrieved IDs.
        relevant: Set or list of relevant IDs.
        k: Cut-off rank.

    Returns:
        Fraction of relevant items found in the top-k.
    """
```

---

## 6  Type Annotations & Runtime Checks

* Use **PEP 484** type hints everywhere (functions, class attributes).
* **All functions must declare input and output types explicitly.**
* Validate external inputs with `assert` or explicit `if ... raise ValueError`.
* Run **mypy** in *strict* mode as a CI step.

---

## 7  Logging

* Initialise loggers via `utils.load_config.init_logger`.
* Use module-level loggers: `logger = logging.getLogger(__name__)`.
* Logging levels: `debug` (dev insights), `info` (milestones), `warning` (recoverable), `error` (cannot proceed), `critical` (program abort).
* Never hide exceptions; use `logger.exception` to preserve traceback.

---

## 8  Error Handling

* Prefer built-in exceptions (`ValueError`, `TypeError`) unless a custom domain error clarifies intent.
* Enrich messages with variable values.
* Do **not** swallow exceptions silently.

---

## 9  Configuration Management

* All hyper-parameters live in YAML under `/config`.
* Load configs **only** through `utils.load_config.load_config`.
* Functions accept an explicit `config: dict` argument instead of reading files internally, unless the function’s **sole purpose** is configuration loading.

---

## 10  CLI & Scripts

* Use `argparse` with long option names (`--batch_size`).
* Provide `--config` flag pointing to a YAML; CLI flags override YAML.
* Scripts must be import-safe (`if __name__ == "__main__":`).

---

## 11  Testing & Quality Gates

* Write **pytest** unit tests covering critical paths.
* Minimum coverage threshold: **80 %**.
* Run `black`, `ruff`, `isort`, `mypy` and tests in CI before merge.

---

## 12  Formatting Tools

| Tool      | Version | Role                                 |
| --------- | ------- | ------------------------------------ |
| **black** | 24.3+   | Code formatting (line length 88).    |
| **ruff**  | 0.3+    | Linter (select = "ALL", ignore = …). |
| **isort** | 5+      | Import ordering (profile = "black"). |

---

## 13  Dependencies & Virtual Envs

* Pin versions in `requirements.txt` (prod) and `requirements-dev.txt` (lint/test).
* Use **conda** or **venv**; never rely on system Python.
* Document GPU/CPU requirements in `README.md`.

---

## 14  Internationalisation

* **All code, comments and docstrings must be in English.**  Spanish is reserved for external documentation or academic writing outside the repository.

---

## 15  Example Module Skeleton

```python
"""Contrastive Autoencoder model.

Implements an encoder–decoder architecture trained with triplet loss.
"""
from __future__ import annotations

import torch
from torch import nn, Tensor

class ContrastiveAutoencoder(nn.Module):
    """Linear contrastive autoencoder with L2-normalised latent vectors."""

    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim),
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
        )

    def encode(self, x: Tensor) -> Tensor:  # noqa: D401
        """Return L2-normalised latent representation."""
        return torch.nn.functional.normalize(self.encoder(x), p=2, dim=-1)

    def decode(self, z: Tensor) -> Tensor:
        return self.decoder(z)

    def forward(self, x: Tensor) -> Tensor:    # type: ignore[override]
        return self.decode(self.encode(x))
```

---

## 16  Migration Plan for Legacy Code

1. **Phase 1**  — New code follows this guide immediately.
2. **Phase 2**  — Touch legacy modules only when editing; refactor headers, identifiers, comments to English.
3. **Phase 3**  — Run automated formatters; fix mypy errors; localised refactors.

---

*End of style guide.*

"""


training/loss_functions.py
"""
# /training/loss_functions.py

import torch
import torch.nn.functional as F

###############################################################################
#  VAE                                                                        #
###############################################################################

import torch
import torch.nn.functional as F

def vae_loss(
    x_reconstructed: torch.Tensor,
    x_target: torch.Tensor,
    mu: torch.Tensor,
    logvar: torch.Tensor,
    *,
    mse_reduction: str = "mean",   # "mean" or "sum"
    beta: float = 1.0,             # β-VAE (β=1 → classic VAE)
) -> torch.Tensor:
    """VAE loss = reconstruction + β·KL  (KL normalized by batch).

    Args:
        x_reconstructed: output from the decoder  ― shape [B, D]
        x_target:        original embeddings ― shape [B, D]
        mu, logvar:      parameters of the latent distribution ― shape [B, Z]
        mse_reduction:   "mean" (recommended) or "sum"
        beta:            weight of the KL term (β-VAE)
    """
    cos = F.cosine_similarity(x_reconstructed, x_target, dim=-1)
    recon = (1.0 - cos).mean()
    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()
    return recon + beta * kl


###############################################################################
#  DAE                                                                        #
###############################################################################

def dae_loss(
    x_reconstructed: torch.Tensor,
    x_clean: torch.Tensor,
    reduction: str = "mean",
) -> torch.Tensor:
    """Mean‑squared error for Denoising Auto‑Encoders."""
    return F.mse_loss(x_reconstructed, x_clean, reduction=reduction)

###############################################################################
#  CONTRASTIVE                                                                #
###############################################################################

def contrastive_loss(
    z_q: torch.Tensor,
    z_pos: torch.Tensor,
    *,
    margin: float = 0.2,
    hard_negatives: bool = True,
) -> torch.Tensor:
    """Triplet loss with negative selection within the batch.

    If `hard_negatives` is True, uses the closest negative; otherwise,
    permutes `z_pos` to obtain a random negative.
    """
    z_q = F.normalize(z_q, p=2, dim=1)
    z_pos = F.normalize(z_pos, p=2, dim=1)

    if hard_negatives:
        dist_mat = torch.cdist(z_q, z_pos, p=2)
        mask = torch.eye(dist_mat.size(0), dtype=torch.bool, device=z_q.device)
        dist_mat = dist_mat.masked_fill(mask, float("inf"))  # ← corrected
        neg_dist, _ = dist_mat.min(dim=1)

    else:
        idx = torch.randperm(z_pos.size(0), device=z_pos.device)
        neg_dist = torch.norm(z_q - z_pos[idx], dim=1)

    pos_dist = torch.norm(z_q - z_pos, dim=1)
    return F.relu(pos_dist - neg_dist + margin).mean()

"""

training/train_cae.py
"""
# training/train_cae.py ― Contrastive Auto-Encoder with negative mining and validation

from __future__ import annotations
import argparse, os, math
from typing import Optional

import torch
from torch.utils.data import DataLoader
from torch.nn.utils import clip_grad_norm_

from data.torch_datasets import EmbeddingTripletDataset
from models.contrastive_autoencoder import ContrastiveAutoencoder
from training.loss_functions import contrastive_loss        # in-batch mining
from utils.load_config import load_config, init_logger
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import prepare_datasets, split_dataset
from dotenv import load_dotenv

# --------------------------------------------------------------------------- #
#  AUX                                                                       #
# --------------------------------------------------------------------------- #

def _build_optimizer(model: torch.nn.Module, lr: float, weight_decay: float) -> torch.optim.Optimizer:
    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

def _build_scheduler(optim: torch.optim.Optimizer, patience: int, factor: float = 0.5):
    # Reduce LR if val_loss does not improve for `patience` consecutive epochs
    return torch.optim.lr_scheduler.ReduceLROnPlateau(
        optim, mode="min", factor=factor, patience=max(1, patience // 2)
    )

# --------------------------------------------------------------------------- #
#  TRAINING LOOP                                                             #
# --------------------------------------------------------------------------- #

def train_cae(
    *,
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    logger,
    hard_negatives: bool = True,
    margin: float = 0.2,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    min_delta: float = 0.003,                   # 0.3% relative improvement
    weight_decay: float = 1e-4,
    clip_grad_norm: float = 1.0,                # 0 = disable
    device: Optional[str] = None,
) -> None:

    device = device or resolve_device()
    log = logger.train if hasattr(logger, "train") else logger

    log.info("CAE | device=%s | hard_negatives=%s | margin=%.3f", device, hard_negatives, margin)

    # ---------------- Dataset ---------------------------
    full_ds = EmbeddingTripletDataset(dataset_path)
    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=True)
    dl_val   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)

    # ---------------- Model & Opt -----------------------
    model = ContrastiveAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = _build_optimizer(model, lr, weight_decay)
    scheduler = _build_scheduler(optim, patience or 4)

    best_val, epochs_no_improve = math.inf, 0

    # Triplet loss native
    triplet_fn = torch.nn.TripletMarginLoss(margin=margin, p=2)

    for epoch in range(1, epochs + 1):
        # ---------------- Train -------------------------
        model.train(); running = 0.0
        for batch in dl_train:
            z_q  = model.encode(batch["q"].to(device))
            z_p  = model.encode(batch["p"].to(device))
            z_n  = model.encode(batch["n"].to(device))

            if hard_negatives:
                loss = contrastive_loss(z_q, z_p, margin=margin, hard_negatives=True)
            else:
                loss = triplet_fn(z_q, z_p, z_n)

            optim.zero_grad()
            loss.backward()
            if clip_grad_norm > 0:
                clip_grad_norm_(model.parameters(), clip_grad_norm)
            optim.step()
            running += loss.item() * z_q.size(0)

        train_loss = running / len(train_ds)

        # ---------------- Validation --------------------
        model.eval(); val_running = 0.0
        with torch.no_grad():
            for batch in dl_val:
                z_q  = model.encode(batch["q"].to(device))
                z_p  = model.encode(batch["p"].to(device))
                z_n  = model.encode(batch["n"].to(device))

                if hard_negatives:
                    vloss = contrastive_loss(z_q, z_p, margin=margin, hard_negatives=True)
                else:
                    vloss = triplet_fn(z_q, z_p, z_n)

                val_running += vloss.item() * z_q.size(0)
        val_loss = val_running / len(val_ds)

        log.info("[Epoch %02d/%d] train=%.6f | val=%.6f", epoch, epochs, train_loss, val_loss)
        scheduler.step(val_loss)

        # ---------------- Early stop --------------------
        rel_improve = (best_val - val_loss) / best_val if best_val < math.inf else 1.0
        if rel_improve > min_delta:
            best_val, epochs_no_improve = val_loss, 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  -> New best val_loss. Checkpoint saved at {model_save_path}")
            logger.train.info("New best val_loss: %.6f → checkpoint %s", best_val, model_save_path)
        else:
            epochs_no_improve += 1
            if patience and epochs_no_improve >= patience:
                print("[EARLY STOP] No improvement in validation.")
                logger.train.info("[EARLY STOP] No improvement in validation.")
                break

    print(f"[DONE] Best val_loss = {best_val:.6f}")
    logger.main.info("[DONE] Best val_loss = %.6f", best_val)
    logger.main.info("")

# --------------------------------------------------------------------------- #
#  CLI                                                                       #
# --------------------------------------------------------------------------- #

if __name__ == "__main__":
    load_dotenv()

    p = argparse.ArgumentParser(description="Train Contrastive Auto-Encoder (CAE)")
    p.add_argument("--config", default="./config/config.yaml")
    p.add_argument("--dataset", choices=["uda", "squad"], help="Override YAML dataset")
    p.add_argument("--epochs",  type=int)
    p.add_argument("--batch_size", type=int)
    p.add_argument("--lr",      type=float)
    p.add_argument("--weight_decay", type=float, default=1e-4)
    p.add_argument("--clip_grad", type=float, default=1.0)
    p.add_argument("--margin",  type=float, default=0.2)
    p.add_argument("--val_split", type=float, default=0.1)
    p.add_argument("--patience", type=int, default=5)
    p.add_argument("--no-hard-negatives", action="store_true")
    p.add_argument("--save_path")
    args = p.parse_args()

    # ---------- Config & logging -----------------------------------------
    cfg       = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg["models"]["contrastive"]
    log       = init_logger(cfg["logging"])

    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False), logger=log.main)

    # ---------- Dataset ---------------------------------------------------
    ds_path = prepare_datasets(cfg, variant="cae", dataset_override=args.dataset)

    # ------------- model save path --------------
    checkpoints_dir = cfg["paths"]["checkpoints_dir"]
    checkpoint_file = model_cfg.get("checkpoint", "cae_text.pth")
    model_save_path = args.save_path or os.path.join(checkpoints_dir, checkpoint_file)

    # ---------- Hparams final ---------------------------------------------
    hparams = dict(
        dataset_path= ds_path,
        input_dim = model_cfg.get("input_dim", 384),
        latent_dim = model_cfg.get("latent_dim", 64),
        hidden_dim = model_cfg.get("hidden_dim", 512),
        batch_size = args.batch_size or train_cfg.get("batch_size", 256),
        epochs = args.epochs or train_cfg.get("epochs", 20),
        lr = args.lr or float(train_cfg.get("learning_rate", 1e-3)),
        weight_decay = args.weight_decay,
        clip_grad_norm = args.clip_grad,
        margin = args.margin,
        hard_negatives = not args.no_hard_negatives,
        val_split = args.val_split,
        patience = None if args.patience == 0 else args.patience,
        model_save_path = model_save_path,
        logger       = log,
    )

    train_cae(**hparams)

"""

training/train_dae.py
"""
# training/train_dae.py – Denoising Auto‑Encoder con validación y early‑stopping

from __future__ import annotations

import argparse
import os
from typing import Optional

import torch
from torch.utils.data import DataLoader

from data.torch_datasets import EmbeddingDAEDataset
from models.denoising_autoencoder import DenoisingAutoencoder
from training.loss_functions import dae_loss
from utils.load_config import load_config
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import split_dataset, prepare_datasets
from utils.load_config import init_logger
from dotenv import load_dotenv

###############################################################################
#  TRAINING FUNCTION                                                          #
###############################################################################

def train_dae(
    *,
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    logger,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    device: Optional[str] = None,
):
    """Run DAE training/validation loop."""
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Training DAE on {device} | val_split={val_split}")
    logger.main.info("")
    logger.main.info("Training DAE | device=%s", device)

    # ---------------- Dataset --------------------------
    full_ds = EmbeddingDAEDataset(dataset_path)
    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    dl_val = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)

    # ---------------- Model & Optimizer ----------------
    model = DenoisingAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = torch.optim.Adam(model.parameters(), lr=lr)

    best_val = float("inf")
    no_improve = 0

    # ---------------- Training Loop -------------------
    for epoch in range(1, epochs + 1):
        model.train()
        running = 0.0
        for batch in dl_train:
            x_noisy = batch["x"].to(device)
            x_clean = batch["y"].to(device)

            optim.zero_grad()
            x_rec = model(x_noisy)
            loss = dae_loss(x_rec, x_clean, reduction="mean")
            loss.backward()
            optim.step()
            running += loss.item() * x_noisy.size(0)

        train_loss = running / len(train_ds)

        # ---------------- Validation ------------------
        model.eval()
        with torch.no_grad():
            val_running = 0.0
            for batch in dl_val:
                x_noisy = batch["x"].to(device)
                x_clean = batch["y"].to(device)
                x_rec = model(x_noisy)
                vloss = dae_loss(x_rec, x_clean, reduction="mean")
                val_running += vloss.item() * x_noisy.size(0)
            val_loss = val_running / len(val_ds)

        print(
            f"[Epoch {epoch:02d}/{epochs}] train_loss={train_loss:.6f} | val_loss={val_loss:.6f}"
        )
        logger.train.info(
            "[Epoch %02d/%d] train=%.6f | val=%.6f", epoch, epochs, train_loss, val_loss
        )

        # ---------------- Early Stopping --------------
        if val_loss < best_val - 1e-4:
            best_val = val_loss
            no_improve = 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  -> New best val_loss. Checkpoint saved at {model_save_path}")
            logger.train.info("New best val_loss: %.6f → checkpoint %s", best_val, model_save_path)
        else:
            no_improve += 1
            if patience and no_improve >= patience:
                print("[EARLY STOP] No improvement in validation.")
                logger.train.info("[EARLY STOP] No improvement in validation.")
                break

    print(f"[DONE] Best val_loss = {best_val:.6f}")
    logger.main.info("[DONE] Best val_loss = %.6f", best_val)
    logger.main.info("")

###############################################################################
#  CLI                                                                        #
###############################################################################

if __name__ == "__main__":
    load_dotenv()

    parser = argparse.ArgumentParser(description="Train Denoising Auto‑Encoder (DAE)")
    parser.add_argument("--config", default="./config/config.yaml")
    parser.add_argument("--dataset", choices=["uda", "squad"], help="Override YAML dataset")
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--save_path")
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--val_split", type=float, default=0.1)
    parser.add_argument("--patience", type=int, default=5)
    args = parser.parse_args()

    # ---------------- Config & logging ----------------
    cfg = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg.get("models", {}).get("dae", {})
    log = init_logger(cfg["logging"])

    # ---------------- Reproducibility ----------------
    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False), logger=log.train)
    device = resolve_device(train_cfg.get("device"))

    # ---------------- Dataset prep -------------------
    dataset_path = prepare_datasets(cfg, variant="dae", dataset_override=args.dataset)

    # ------------- model save path --------------
    checkpoints_dir = cfg["paths"]["checkpoints_dir"]
    checkpoint_file = model_cfg.get("checkpoint", "dae_text.pth")
    model_save_path = args.save_path or os.path.join(checkpoints_dir, checkpoint_file)

    # ---------------- Training -----------------------
    train_dae(
        dataset_path=dataset_path,
        input_dim=model_cfg.get("input_dim", 384),
        latent_dim=model_cfg.get("latent_dim", 64),
        hidden_dim=model_cfg.get("hidden_dim", 512),
        batch_size=args.batch_size or train_cfg.get("batch_size", 256),
        epochs=args.epochs or train_cfg.get("epochs", 20),
        lr=args.lr if args.lr is not None else float(train_cfg.get("learning_rate", 1e-3)),
        model_save_path=model_save_path,
        val_split=args.val_split,
        patience=None if args.patience == 0 else args.patience,
        device=device,
        logger=log,
    )

"""

training/train_vae.py
"""
# training/train_vae.py – Variational Auto‑Encoder con validación y early‑stopping

import argparse
import os
from typing import Optional

import torch
from torch.utils.data import DataLoader

from data.torch_datasets import EmbeddingVAEDataset
from models.variational_autoencoder import VariationalAutoencoder
from training.loss_functions import vae_loss
from utils.load_config import load_config, init_logger
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import prepare_datasets
from dotenv import load_dotenv

###############################################################################
#  TRAINING LOOP                                                             #
###############################################################################

def train_vae(
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    device: Optional[str] = None,
):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Training VAE on {device} | val_split={val_split}")

    full_ds = EmbeddingVAEDataset(dataset_path)
    from utils.data_utils import split_dataset  # local import to avoid circular

    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    dl_val   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)

    model = VariationalAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = torch.optim.Adam(model.parameters(), lr=lr)

    best_val, no_improve = float("inf"), 0
    for epoch in range(1, epochs + 1):
        # ---------------- train ------------------
        model.train(); running = 0.0
        for batch in dl_train:
            x_in  = batch["input"].to(device)
            x_tar = batch["target"].to(device)
            optim.zero_grad()
            x_rec, mu, logvar = model(x_in)
            loss = vae_loss(x_rec, x_tar, mu, logvar, mse_reduction="mean")
            loss.backward(); optim.step()
            running += loss.item() * x_in.size(0)
        train_loss = running / len(train_ds)

        # ---------------- validation -------------
        model.eval(); val_running = 0.0
        with torch.no_grad():
            for batch in dl_val:
                x_in  = batch["input"].to(device)
                x_tar = batch["target"].to(device)
                x_rec, mu, logvar = model(x_in)
                vloss = vae_loss(x_rec, x_tar, mu, logvar, mse_reduction="mean")
                val_running += vloss.item() * x_in.size(0)
        val_loss = val_running / len(val_ds)

        print(f"[Epoch {epoch:02d}/{epochs}] train={train_loss:.6f} | val={val_loss:.6f}")

        if val_loss < best_val - 1e-4:
            best_val, no_improve = val_loss, 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
        else:
            no_improve += 1
            if patience and no_improve >= patience:
                print("[EARLY STOP] No improvement in validation."); break

    print(f"[DONE] best_val_loss = {best_val:.6f}")

###############################################################################
#  CLI                                                                       #
###############################################################################

if __name__ == "__main__":
    load_dotenv()

    parser = argparse.ArgumentParser(description="Train Variational Auto‑Encoder (VAE)")
    parser.add_argument("--config", default="./config/config.yaml")
    parser.add_argument("--dataset", choices=["uda", "squad"], help="Override dataset in config.yaml")
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--save_path")
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--val_split", type=float, default=0.1)
    parser.add_argument("--patience", type=int, default=5)
    args = parser.parse_args()

    # ------------- config & logging -------------
    cfg       = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg.get("models", {}).get("vae", {})
    log       = init_logger(cfg["logging"])

    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False))
    device = resolve_device(train_cfg.get("device"))

    # ------------- dataset paths -----------------
    dataset_path = prepare_datasets(cfg, variant="vae", dataset_override=args.dataset)

    # ------------- model save path --------------
    checkpoints_dir = cfg["paths"]["checkpoints_dir"]
    checkpoint_file = model_cfg.get("checkpoint", "vae_text.pth")
    model_save_path = args.save_path or os.path.join(checkpoints_dir, checkpoint_file)

    # ------------- training ----------------------
    train_vae(
        dataset_path=dataset_path,
        input_dim=model_cfg.get("input_dim", 384),
        latent_dim=model_cfg.get("latent_dim", 64),
        hidden_dim=model_cfg.get("hidden_dim", 512),
        batch_size=args.batch_size or train_cfg.get("batch_size", 256),
        epochs=args.epochs or train_cfg.get("epochs", 20),
        lr=float(args.lr) if args.lr is not None else float(train_cfg.get("learning_rate", 1e-3)),
        model_save_path=model_save_path,
        val_split=args.val_split,
        patience=None if args.patience == 0 else args.patience,
        device=device,
    )

"""

utils/chunk_utils.py
"""


# utils/chunk_utils.py
from __future__ import annotations

import logging
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
from typing import Dict, Iterable, List, Literal, Sequence, Tuple

import pandas as pd
from transformers import AutoTokenizer

LOGGER = logging.getLogger(__name__)

ChunkMode = Literal["sliding", "semantic"]
from tqdm import tqdm 

###############################################################################
# Main chunker                                                                #
###############################################################################

def chunk_context_with_alignment(
    context: str,
    answer_start: int,
    answer_end: int,
    *,
    max_tokens: int = 128,
    stride: int = 64,
    tokens_before: int = 32,
    tokens_after: int = 32,
    tokenizer_name: str = 'sentence-transformers/all-MiniLM-L6-v2',
) -> List[str]:
    """Chunk context ensuring answer span tokens appear in at least one chunk."""
    tok = _get_tokenizer(tokenizer_name)

    enc = tok(context, add_special_tokens=False, return_offsets_mapping=True)
    input_ids: List[int] = enc["input_ids"]
    offsets: List[Tuple[int, int]] = enc["offset_mapping"]

    try:
        t_start, t_end = _char_to_token_span(offsets, answer_start, answer_end)
    except ValueError:
        LOGGER.warning("[chunk_utils] Span alignment failed; using full context.")
        return [context.strip()]

    answer_ids = input_ids[t_start : t_end + 1]
    n_tokens = len(input_ids)

    # 1) Centred window ----------------------------------------------------
    win_start = max(0, t_start - tokens_before)
    win_end = min(n_tokens, t_end + tokens_after + 1)
    cur_len = win_end - win_start
    if cur_len < max_tokens:
        pad = max_tokens - cur_len
        pre = min(pad // 2, win_start)
        post = min(pad - pre, n_tokens - win_end)
        win_start -= pre
        win_end += post

    centred_ids = input_ids[win_start:win_end]
    centred_chunk = tok.decode(
        centred_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True,
    ).strip()

    # 2) Sliding windows ---------------------------------------------------
    sliding_chunks: List[Tuple[str, List[int]]] = []
    idx = 0
    while idx < n_tokens:
        sw_end = min(idx + max_tokens, n_tokens)
        ids_slice = input_ids[idx:sw_end]
        ch_str = tok.decode(
            ids_slice,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True,
        ).strip()
        sliding_chunks.append((ch_str, ids_slice))
        if sw_end == n_tokens:
            break
        idx += stride

    # 3) Combine + deduplicate -------------------------------------------
    out: List[str] = []
    out_ids: List[List[int]] = []
    seen = set()

    # first centred chunk
    if centred_chunk:
        out.append(centred_chunk)
        out_ids.append(centred_ids)
        seen.add(centred_chunk)

    for ch_str, ids_slice in sliding_chunks:
        if ch_str and ch_str not in seen:
            out.append(ch_str)
            out_ids.append(ids_slice)
            seen.add(ch_str)

    # 4) Integrity check on original ids ----------------------------------
    if not any(_has_subseq(ids, answer_ids) for ids in out_ids):
        LOGGER.warning(
            "[chunk_utils] Answer span missing after chunking; adding full context."
        )
        out.append(context.strip())

    return out


# ============================== Tokenizer cache ==============================

@lru_cache(maxsize=4)
def _get_tokenizer(model_name: str):
    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    if not tok.is_fast:
        raise ValueError(
            f"Tokenizer '{model_name}' must be a *fast* tokenizer to provide offset mappings."
        )
    return tok

# ============================== Data structures ==============================

@dataclass(frozen=True)
class ChunkRecord:
    """Lightweight record for a chunk with token/char spans."""
    doc_id: int
    tok_start: int
    tok_end: int           # inclusive
    char_start: int
    char_end: int          # exclusive
    text: str

# ============================== Helpers =====================================

def _has_subseq(hay: Sequence[int], needle: Sequence[int]) -> bool:
    n = len(needle)
    if n == 0 or n > len(hay):
        return False
    for i in range(len(hay) - n + 1):
        if hay[i : i + n] == list(needle):
            return True
    return False


def _char_to_token_span(
    offsets: Sequence[Tuple[int, int]],
    char_start: int,
    char_end: int,
) -> Tuple[int, int]:
    tok_start = tok_end = None
    for i, (s, e) in enumerate(offsets):
        if tok_start is None and s <= char_start < e:
            tok_start = i
        if s < char_end <= e:
            tok_end = i
            break
    if tok_start is None or tok_end is None:
        raise ValueError("Answer span could not be aligned to token offsets.")
    return tok_start, tok_end

# ============================== Inference chunkers ===========================

# utils/chunk_utils.py

def sliding_window_chunker(
    text: str,
    *,
    max_tokens: int = 128,
    stride: int = 64,
    tokenizer_name: str = "sentence-transformers/all-MiniLM-L6-v2",
) -> List[ChunkRecord]:
    """Return fixed-size overlapping chunks (token-based).

    The chunk text is built from character slices aligned to token boundaries
    to avoid WordPiece artifacts (e.g., '##suffix') and ensure that re-tokenising
    the chunk never exceeds `max_tokens`.
    """
    tok = _get_tokenizer(tokenizer_name)
    enc = tok(text, add_special_tokens=False, return_offsets_mapping=True)
    input_ids: List[int] = enc["input_ids"]
    offsets: List[Tuple[int, int]] = enc["offset_mapping"]
    n = len(input_ids)

    out: List[ChunkRecord] = []
    i = 0
    while i < n:
        end = min(i + max_tokens, n)
        # character-aligned slice covering tokens [i, end)
        char_start = offsets[i][0]
        char_end = offsets[end - 1][1]
        chunk_text = text[char_start:char_end].strip()
        if chunk_text:
            out.append(
                ChunkRecord(
                    doc_id=-1,
                    tok_start=i,
                    tok_end=end - 1,
                    char_start=char_start,
                    char_end=char_end,
                    text=chunk_text,
                )
            )
        if end == n:
            break
        i += stride
    return out


def semantic_window_chunker(
    text: str,
    *,
    max_tokens: int = 128,
    stride: int = 64,
    min_tokens: int = 48,
    tokenizer_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    boundary_chars: str = ".!?;:\n",
) -> List[ChunkRecord]:
    """Return chunks that try to end at punctuation boundaries near max_tokens.

    Heuristic:
      - Start a window at `start`.
      - Prefer ending at the nearest token (within [start+min_tokens, start+max_tokens])
        whose *last character* is in `boundary_chars`.
      - If none is found, end at `start+max_tokens`.
    Text is reconstructed via character slicing aligned to token boundaries.
    """
    tok = _get_tokenizer(tokenizer_name)
    enc = tok(text, add_special_tokens=False, return_offsets_mapping=True)
    input_ids: List[int] = enc["input_ids"]
    offsets: List[Tuple[int, int]] = enc["offset_mapping"]
    n = len(input_ids)

    out: List[ChunkRecord] = []
    seen_spans: set[Tuple[int, int]] = set()
    start = 0

    while start < n:
        hard_end = min(start + max_tokens, n)
        soft_floor = min(hard_end - 1, max(start + min_tokens, start + 1))

        # search backward for a punctuation boundary
        best_end = None
        j = hard_end - 1
        while j >= soft_floor:
            _, ce = offsets[j]
            if ce > 0:
                last_char = text[ce - 1]
                if last_char in boundary_chars:
                    best_end = j + 1  # make `end` exclusive
                    break
            j -= 1
        end = best_end or hard_end

        span = (start, end - 1)
        if span not in seen_spans:
            seen_spans.add(span)
            char_start = offsets[start][0]
            char_end = offsets[end - 1][1]
            chunk_text = text[char_start:char_end].strip()
            if chunk_text:
                out.append(
                    ChunkRecord(
                        doc_id=-1,
                        tok_start=start,
                        tok_end=end - 1,
                        char_start=char_start,
                        char_end=char_end,
                        text=chunk_text,
                    )
                )

        if end == n:
            break
        start += stride

    return out

# ============================== Builders ====================================
def build_chunked_corpus(
    squad_split,
    *,
    max_tokens: int = 128,
    stride: int = 64,
    tokens_before: int = 32,
    tokens_after: int = 32,
    tokenizer_name: str = "bert-base-uncased",
    store_chunk_text: bool = True,        # <-- NUEVO
) -> Tuple[List[str], pd.DataFrame]:
    """
    Devuelve (chunks, index).  
    Si *store_chunk_text* es False la columna `chunk_text NO se guarda,
    reduciendo drásticamente el peso en disco.
    """
    tok = _get_tokenizer(tokenizer_name)
    chunks: List[str] = []
    records: List[Dict] = []

    for doc_id, ex in tqdm(
        enumerate(squad_split),
        total=len(squad_split),
        desc="Chunking SQuAD",
    ):
        ctx = ex["context"].rstrip()
        if not ctx:
            continue
        answers = ex["answers"]
        if not answers["text"]:
            continue
        ans_text = answers["text"][0]
        a_start = answers["answer_start"][0]
        a_end = a_start + len(ans_text)

        doc_chunks = chunk_context_with_alignment(
            ctx,
            a_start,
            a_end,
            max_tokens=max_tokens,
            stride=stride,
            tokens_before=tokens_before,
            tokens_after=tokens_after,
            tokenizer_name=tokenizer_name,
        )

        answer_ids = tok(ans_text, add_special_tokens=False)["input_ids"]
        for ch in doc_chunks:
            cid = len(chunks)
            chunks.append(ch)
            ch_ids = tok(ch, add_special_tokens=False)["input_ids"]
            contains = _has_subseq(ch_ids, answer_ids)

            rec = {
                "chunk_id": cid,
                "doc_id": doc_id,
                "contains_answer": contains,
            }
            if store_chunk_text:            
                rec["chunk_text"] = ch
            records.append(rec)

    index = pd.DataFrame.from_records(records).set_index("chunk_id")
    return chunks, index

def build_inference_corpus(
    docs: Sequence[str],
    *,
    mode: ChunkMode = "sliding",
    max_tokens: int = 128,
    stride: int = 64,
    min_tokens: int = 48,
    tokenizer_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    store_chunk_text: bool = True,
) -> Tuple[List[str], pd.DataFrame]:
    """Materialise a chunk-level corpus for test/inference.

    Returns:
        chunks: list of chunk texts (for embedding/FAISS).
        index:  DataFrame indexed by chunk_id with columns:
                ['doc_id','tok_start','tok_end','char_start','char_end', 'chunk_text?'].
    """
    chunks: List[str] = []
    records: List[Dict] = []
    chunker = sliding_window_chunker if mode == "sliding" else semantic_window_chunker

    for doc_id, text in enumerate(docs):
        if not text:
            continue
        recs = chunker(
            text,
            max_tokens=max_tokens,
            stride=stride,
            tokenizer_name=tokenizer_name,
            **({} if mode == "sliding" else {"min_tokens": min_tokens}),
        )
        for r in recs:
            cid = len(chunks)
            chunks.append(r.text)
            row = {
                "chunk_id": cid,
                "doc_id": doc_id,
                "tok_start": r.tok_start,
                "tok_end": r.tok_end,
                "char_start": r.char_start,
                "char_end": r.char_end,
            }
            if store_chunk_text:
                row["chunk_text"] = r.text
            records.append(row)

    df = pd.DataFrame.from_records(records).set_index("chunk_id")
    return chunks, df

# ============================== Persistence =================================

def save_chunk_index(path, df: pd.DataFrame):
    path = str(path)
    Path(path).parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(path)


def load_chunk_index(path):
    return pd.read_parquet(path)

__all__ = [
    "ChunkRecord",
    "sliding_window_chunker",
    "semantic_window_chunker",
    "build_inference_corpus",
    "chunk_context_with_alignment",
    "build_chunked_corpus",
    "save_chunk_index",
    "load_chunk_index",
]

"""

utils/data_utils.py
"""
# /utils/data_utils.py
from __future__ import annotations
import os
from typing import List, Tuple, Optional, Dict, Sequence

import torch
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm
import random
from torch.utils.data import Subset

from pathlib import Path
from hashlib import sha1

import pandas as pd

from utils.chunk_utils import (
    sliding_window_chunker,
    semantic_window_chunker,
)


import pandas as pd
from utils.chunk_utils import build_inference_corpus, save_chunk_index

###############################################################################
# SBERT caching                                                               #
###############################################################################
def text_hash(text: str) -> str:
    import hashlib
    return hashlib.sha1(text.encode("utf-8")).hexdigest()


def _compute_embeddings(
    texts: Sequence[str],
    model: SentenceTransformer,
    batch_size: int = 64,
) -> torch.Tensor:
    """Return CLS embeddings as a `[N × D]` *float32* CPU tensor."""
    acc: List[torch.Tensor] = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding"):
        batch = texts[i : i + batch_size]
        with torch.no_grad():
            emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
            acc.append(torch.from_numpy(emb))
    return torch.cat(acc).float()


def _texts_fingerprint(texts: Sequence[str]) -> str:
    h = sha1()
    for t in texts:
        h.update(t.encode("utf-8"))
    return h.hexdigest()[:10]


def ensure_sbert_cache(
    texts: Sequence[str],
    *,
    model_name: str,
    cache_dir: str = "./data/SBERT",
    batch_size: int = 64,
    force: bool = False,
) -> torch.Tensor:
    os.makedirs(cache_dir, exist_ok=True)
    fp = _texts_fingerprint(texts)
    tag = model_name.split("/")[-1]
    path = os.path.join(cache_dir, f"sbert_{fp}_{tag}.pt")

    if not force and os.path.exists(path):
        return torch.load(path, map_location="cpu")

    print(f"[INFO] SBERT cache miss → encoding {len(texts):,} texts …")
    model = SentenceTransformer(model_name)
    emb = _compute_embeddings(texts, model, batch_size=batch_size)
    torch.save(emb, path)
    print(f"[OK]  SBERT embeddings saved → {path}")
    return emb

def _jaccard_sim(a: str, b: str) -> float:
    a_set = set(a.lower().split())
    b_set = set(b.lower().split())
    inter = a_set & b_set
    union = a_set | b_set
    return len(inter) / len(union) if union else 0.0

def _texts_fingerprint(texts: List[str]) -> str:
    """
    Devuelve un hash abreviado (10 hex) de la secuencia de textos.
    El orden de los textos importa, así garantizamos reproducibilidad.
    """
    h = sha1()
    for t in texts:
        h.update(t.encode("utf-8"))
    return h.hexdigest()[:10]                # 40 bits bastan para colisiones muy raras

def prepare_inference_chunks(
    docs: Sequence[str],
    *,
    mode: str = "sliding",                     # "sliding" | "semantic"
    max_tokens: int = 128,
    stride: int = 64,
    min_tokens: int = 48,                      # solo usado en semantic
    tokenizer_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    index_out: str | None = None,              # parquet opcional
    store_chunk_text: bool = True,
) -> Tuple[List[str], pd.DataFrame]:
    """Chunk the given documents for test/inference and (optionally) persist an index.

    Returns:
        chunks: list[str] with chunk texts in corpus order.
        index : DataFrame indexed by chunk_id with:
                ['doc_id','tok_start','tok_end','char_start','char_end', 'chunk_text?'].
    """
    chunks: List[str] = []
    records: List[Dict] = []

    chunker = sliding_window_chunker if mode == "sliding" else semantic_window_chunker

    for doc_id, text in enumerate(docs):
        if not text:
            continue
        recs = (
            chunker(
                text,
                max_tokens=max_tokens,
                stride=stride,
                tokenizer_name=tokenizer_name,
            )
            if mode == "sliding"
            else semantic_window_chunker(
                text,
                max_tokens=max_tokens,
                stride=stride,
                min_tokens=min_tokens,
                tokenizer_name=tokenizer_name,
            )
        )
        for r in recs:
            cid = len(chunks)
            chunks.append(r.text)
            row = {
                "chunk_id": cid,
                "doc_id": doc_id,
                "tok_start": r.tok_start,
                "tok_end": r.tok_end,
                "char_start": r.char_start,
                "char_end": r.char_end,
            }
            if store_chunk_text:
                row["chunk_text"] = r.text
            records.append(row)

    index = pd.DataFrame.from_records(records).set_index("chunk_id")

    if index_out:
        outp = Path(index_out)
        outp.parent.mkdir(parents=True, exist_ok=True)
        index.to_parquet(outp)

    return chunks, index


def ensure_sbert_cache(
    texts: List[str],
    *,
    model_name: str,
    cache_dir: str = "./data/SBERT",
    batch_size: int = 64,
    force: bool = False,
) -> torch.Tensor:
    """
    Calcula (o reutiliza) los embeddings de SBERT y los persiste en disco.

    Args
    ----
    texts       : Lista de cadenas a codificar.
    model_name  : Identificador HuggingFace / Sentence-Transformers del modelo.
    cache_dir   : Carpeta donde almacenar los .pt (creada si no existe).
    batch_size  : Tamaño de lote para _compute_embeddings.
    force       : Si True, rehace el cálculo aunque exista el fichero.

    Returns
    -------
    Tensor CPU float32 de dimensión [N × D].
    """
    os.makedirs(cache_dir, exist_ok=True)

    fp        = _texts_fingerprint(texts)
    model_tag = model_name.split("/")[-1]
    fname     = f"sbert_{fp}_{model_tag}.pt"
    path      = os.path.join(cache_dir, fname)

    if not force and os.path.exists(path):
        return torch.load(path, map_location="cpu")

    print(f"[INFO] SBERT cache miss → codificando {len(texts):,} textos …")
    st_model  = SentenceTransformer(model_name)
    emb       = _compute_embeddings(texts, st_model, batch_size=batch_size)
    torch.save(emb, path)
    print(f"[OK]  SBERT embeddings guardados → {path}")
    return emb

def ensure_uda_data( # EN DESUSO
    *,
    output_dir: str = "./data/",
    max_samples: Optional[int] = None,
    base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    noise_std: float = 0.05,
    force: bool = False,
) -> None:
    """Genera (o reutiliza) los ficheros de embeddings para VAE, DAE y contraste."""
    os.makedirs(output_dir, exist_ok=True)

    vae_path = os.path.join(output_dir, "uda_vae_embeddings.pt")
    dae_path = os.path.join(output_dir, "uda_dae_embeddings.pt")
    contrastive_path = os.path.join(output_dir, "uda_contrastive_embeddings.pt")

    if (
        not force
        and os.path.exists(vae_path)
        and os.path.exists(dae_path)
        and os.path.exists(contrastive_path)
    ):
        print("[INFO] UDA embeddings ya preparados — nada que hacer.")
        return

    print("[INFO] Descargando / cargando UDA…")
    uda = load_dataset("qinchuanhui/UDA-QA", "nq")
    if max_samples is not None:
        uda = uda.select(range(min(max_samples, len(uda))))
    print(f"[INFO] UDA listo con {len(uda):,} ejemplos.")

    clean_texts: List[str] = []
    contrastive_triples: List[Tuple[str, str, str]] = []

    for i, ex in enumerate(uda["test"]): # TEMPORAL ******************************************************* CAMBIAR URGENTEMENTE
        q = ex.get("question", "").strip()
        pos = ex.get("long_answer", "").strip()
        if not q or not pos:
            continue

        neg = None
        for _ in range(10):
            j = random.randint(0, len(uda["test"]) - 1)
            if j == i:
                continue
            neg_cand = uda["test"][j].get("long_answer", "").strip()
            if not neg_cand:
                continue
            if _jaccard_sim(q, neg_cand) < 0.1:
                neg = neg_cand
                break

        if neg is None:
            continue

        clean_texts.extend((q, pos))             # query + positive answer
        contrastive_triples.append((q, pos, neg))


    print(f"[INFO] Tripletas contrastivas generadas: {len(contrastive_triples):,}")

    print(f"[INFO] Cargando SentenceTransformer '{base_model_name}' …")
    st_model = SentenceTransformer(base_model_name)

    print("[INFO] Generando embeddings VAE/DAE (positivos)…")
    target_emb = _compute_embeddings(clean_texts, st_model)

    if force or not os.path.exists(vae_path):
        torch.save({"input": target_emb, "target": target_emb.clone()}, vae_path)
        print(f"[OK]  VAE embeddings guardados → {vae_path}")

    if force or not os.path.exists(dae_path):
        input_emb = target_emb + torch.randn_like(target_emb) * noise_std
        torch.save({"input": input_emb, "target": target_emb}, dae_path)
        print(f"[OK]  DAE embeddings guardados → {dae_path}")

    if force or not os.path.exists(contrastive_path):
        print("[INFO] Generando embeddings de triples (query/pos/neg)…")
        qs, ps, ns = zip(*contrastive_triples)
        q_emb = _compute_embeddings(list(qs), st_model)
        p_emb = _compute_embeddings(list(ps), st_model)
        n_emb = _compute_embeddings(list(ns), st_model)
        torch.save({"query": q_emb, "positive": p_emb, "negative": n_emb}, contrastive_path)
        print(f"[OK]  Contrastive embeddings guardados → {contrastive_path}")

    print("[DONE] Preprocesado de UDA completo.")

def split_dataset(dataset: torch.utils.data.Dataset, val_split: float = 0.1, seed: int = 42) -> Tuple[Subset, Subset]:
    n_total = len(dataset)
    idx = list(range(n_total))
    random.Random(seed).shuffle(idx)
    n_val = int(n_total * val_split)
    val_idx = idx[:n_val]
    train_idx = idx[n_val:]
    return Subset(dataset, train_idx), Subset(dataset, val_idx)

def ensure_squad_data(
    *,
    output_dir: str = "./data/SQUAD",
    max_samples: Optional[int] = None,
    base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    noise_std: float = 0.05,
    include_unanswerable: bool = False,
    force: bool = False,
    chunk_max_tokens: int = 128,
    chunk_stride: int = 64,
    tokens_before: int = 32,
    tokens_after: int = 32,
    tokenizer_name: str | None = None,
    visualize: bool = False  # New parameter to control visualization
) -> None:
    import matplotlib.pyplot as plt
    import seaborn as sns
    from collections import defaultdict

    os.makedirs(output_dir, exist_ok=True)

    vae_path = Path(output_dir, "squad_vae_embeddings.pt")
    dae_path = Path(output_dir, "squad_dae_embeddings.pt")
    con_path = Path(output_dir, "squad_contrastive_embeddings.pt")
    idx_path = Path(output_dir, "chunk_index.parquet")

    if (
        not force
        and all(p.exists() for p in (vae_path, dae_path, con_path, idx_path))
    ):
        print("[INFO] SQuAD embeddings already prepared — nothing to do.")
        return

    ds_name = "squad_v2" if include_unanswerable else "squad"
    print(f"[INFO] Loading {ds_name} …")
    squad = load_dataset(ds_name, split="train")
    if max_samples is not None:
        squad = squad.select(range(min(max_samples, len(squad))))
    print(f"[INFO] SQuAD loaded with {len(squad):,} examples.")

    print("[INFO] Chunking contexts (answer‑aware) …")
    chunks, chunk_index = build_chunked_corpus(
        squad,
        max_tokens=chunk_max_tokens,
        stride=chunk_stride,
        tokens_before=tokens_before,
        tokens_after=tokens_after,
        tokenizer_name=tokenizer_name or base_model_name,
    )

    save_chunk_index(idx_path, chunk_index)
    print(f"[OK]  Chunk index saved → {idx_path}")

    # Preindexar: {doc_id → [chunk_id con respuesta]}
    doc_chunks: Dict[int, List[int]] = defaultdict(list)
    for cid, row in chunk_index.iterrows():
        if row["contains_answer"]:
            doc_chunks[row["doc_id"]].append(cid)

    clean_texts: List[str] = []
    pos_chunks: List[str] = []
    doc_ids_skipped: List[int] = []

    for doc_id, ex in enumerate(squad):
        q = ex["question"].strip()
        if doc_id in doc_chunks and doc_chunks[doc_id]:
            cid = doc_chunks[doc_id][0]
            pos_chunk = chunks[cid]
            clean_texts.extend((q, pos_chunk))
            pos_chunks.append(pos_chunk)
        else:
            doc_ids_skipped.append(doc_id)

    if doc_ids_skipped and visualize:
        print(f"[WARN] No chunk found with answer for {len(doc_ids_skipped)} documents.")
        sns.set_theme()
        plt.figure(figsize=(8, 4))
        sns.histplot(doc_ids_skipped, bins=50, kde=False)
        plt.title("Distribución de documentos sin chunk con respuesta")
        plt.xlabel("doc_id")
        plt.ylabel("Frecuencia")
        plt.tight_layout()
        plt.show()

    neg_chunks: List[str] = []
    rng = random.Random(42)
    for doc_id, pos in enumerate(pos_chunks):
        while True:
            cand_id = rng.randrange(len(chunks))
            if chunk_index.loc[cand_id, "doc_id"] != doc_id:
                cand_chunk = chunks[cand_id]
                if _jaccard_sim(pos, cand_chunk) < 0.1:
                    neg_chunks.append(cand_chunk)
                    break

    print("[INFO] Encoding SBERT embeddings …")
    print(f"[INFO] Codificando queries + positivos: {len(clean_texts)//2} pares.")
    cache_dir = Path(output_dir, "sbert_cache")
    target_emb = ensure_sbert_cache(
        clean_texts,
        model_name=base_model_name,
        cache_dir=str(cache_dir),
        batch_size=64,
    )

    q_emb = target_emb[0::2]
    p_emb = target_emb[1::2]

    print(f"[INFO] Codificando negativos: {len(neg_chunks)} chunks.")
    n_emb = ensure_sbert_cache(
        neg_chunks,
        model_name=base_model_name,
        cache_dir=str(cache_dir),
        batch_size=64,
    )

    if force or not vae_path.exists():
        torch.save({"input": target_emb, "target": target_emb.clone()}, vae_path)
        print(f"[OK]  VAE embeddings   → {vae_path}")

    if force or not dae_path.exists():
        noisy = target_emb + torch.randn_like(target_emb) * noise_std
        torch.save({"input": noisy, "target": target_emb}, dae_path)
        print(f"[OK]  DAE embeddings   → {dae_path}")

    if force or not con_path.exists():
        torch.save({"query": q_emb, "positive": p_emb, "negative": n_emb}, con_path)
        print(f"[OK]  Contrastive embeddings → {con_path}")

    print("[DONE] SQuAD preprocessing finished (chunk‑level).")



def _prepare_uda(cfg: dict) -> Dict[str, str]: # NOT IN USE
    common = dict(
        output_dir="./data/UDA",
        max_samples=cfg["data"].get("max_samples"),
        base_model_name=cfg["embedding_model"]["name"],
        force=False,
    )
    ensure_uda_data(**common)
    output_dir = common["output_dir"]
    return {
        "vae": os.path.join(output_dir, cfg["models"]["vae"]["dataset_file"]),
        "dae": os.path.join(output_dir, cfg["models"]["dae"]["dataset_file"]),
        "cae": os.path.join(output_dir, cfg["models"]["contrastive"]["dataset_file"]),
    }


def _prepare_squad(cfg: dict) -> Dict[str, str]:
    data_cfg = cfg["data"]
    common = dict(
        output_dir=cfg["paths"]["data_dir"],
        max_samples=data_cfg.get("max_samples"),
        base_model_name=cfg["embedding_model"]["name"],
        noise_std=0.05,
        include_unanswerable=data_cfg.get("include_unanswerable", False),
        force=False,
    )
    ensure_squad_data(**common)
    output_dir = common["output_dir"]
    return {
        "vae": os.path.join(output_dir, cfg["models"]["vae"]["dataset_file"]),
        "dae": os.path.join(output_dir, cfg["models"]["dae"]["dataset_file"]),
        "cae": os.path.join(output_dir, cfg["models"]["contrastive"]["dataset_file"]),
    }


def prepare_datasets(
    cfg: dict,
    *,
    variant: str,
    dataset_override: Optional[str] = None,
) -> str:
    """Ensure dataset tensors exist and return path for requested variant.

    Args:
        cfg: Parsed YAML config dict (must include data and embedding_model).
        variant: One of `{"vae", "dae", "cae"}.
        dataset_override: If provided, forces `"uda" or "squad"

    Returns:
        The filesystem path to the tensor file corresponding to the *variant*.
    """
    variant = variant.lower()
    assert variant in {"vae", "dae", "cae"}, "variant must be vae, dae or cae"

    ds_name = (dataset_override or cfg.get("data", {}).get("dataset", "squad")).lower()
    if ds_name == "squad":
        paths = _prepare_squad(cfg)
    elif ds_name == "uda":
        paths = _prepare_uda(cfg)
    else:
        raise ValueError(f"Unknown dataset: {ds_name}")

    path = paths[variant]
    if not Path(path).exists():
        raise FileNotFoundError(f"Expected dataset file not found: {path}")
    return path




def load_eval_queries_from_squad(
    version: str = "v1",
    split: str = "validation",
    max_samples: Optional[int] = None,
    dedup: bool = True,
) -> Tuple[List[str], List[str], List[List[str]]]:
    """
    Prepara triples (queries, corpus, relevantes) para evaluación de retrieval.

    Args:
        version: "v1" o "v2"
        split: "train" o "validation"
        max_samples: límite de queries
        dedup: si True, elimina contextos repetidos del corpus

    Returns:
        queries, corpus, relevant_docs (1 a 1 con queries)
    """
    ds_name = "squad_v2" if version == "v2" else "squad"
    ds = load_dataset(ds_name, split=split)

    queries, contexts, relevant = [], [], []

    for ex in ds:
        q = ex["question"].strip()
        c = ex["context"].strip()

        # descartar preguntas sin respuesta si es v2
        if version == "v2":
            has_answer = bool(ex["answers"]["answer_start"])
            if not has_answer:
                continue

        queries.append(q)
        contexts.append(c)
        relevant.append([c])  # relevante = ese contexto

        if max_samples and len(queries) >= max_samples:
            break

    corpus = list(set(contexts)) if dedup else contexts
    return queries, corpus, relevant


def load_evaluation_data(dataset: str, max_samples: int = 200):
    if dataset == "squad":
        return load_eval_queries_from_squad(
            version="v1", split="validation", max_samples=max_samples
        )
    elif dataset == "uda":
        raise NotImplementedError("TODO: soporte UDA")
    else:
        raise ValueError(f"Dataset desconocido: {dataset}")
"""

utils/load_config.py
"""
import numpy as np
import yaml
import os, sys
from pathlib import Path
from types import SimpleNamespace   
import logging

def load_config(path: str) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def init_logger(cfg_logging: dict) -> SimpleNamespace:

    if cfg_logging.get("log_to_file", False):
        Path(cfg_logging["log_file"]).parent.mkdir(parents=True, exist_ok=True)

    handlers = [logging.StreamHandler(sys.stdout)]
    if cfg_logging.get("log_to_file", False):
        handlers.append(logging.FileHandler(cfg_logging["log_file"], encoding="utf-8"))

    logging.basicConfig(
        level=getattr(logging, cfg_logging.get("level", "INFO")),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        handlers=handlers,
        force=True,                        
    )

    return SimpleNamespace(
        main = logging.getLogger("main"),
        train = logging.getLogger("train"),
        utils = logging.getLogger("utils"),
    )
"""

utils/training_utils.py
"""
# utils/training_utils.py
import os, random, logging
import numpy as np
import torch

def set_seed( seed: int, deterministic: bool = False, logger: logging.Logger | None = None ) -> None:
    """
    Fija todas las semillas y el modo determinista de cuDNN.

    Args:
        seed (int): valor de la semilla.
        deterministic (bool): True → reproducibilidad completa
                              (más lento en GPU).
        logger (logging.Logger | None): instancia de logger principal;
                                        si es None se usa el del módulo.
    """
    logger = logger or logging.getLogger(__name__)

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.deterministic = deterministic
    torch.backends.cudnn.benchmark     = not deterministic
    torch.use_deterministic_algorithms(deterministic)

    if deterministic:
        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
        logger.info("cuDNN deterministic mode  ACTIVE (desactivated benchmark mode)")
    else:
        logger.info("cuDNN benchmark mode ACTIVE (desactivated deterministic mode)")


def resolve_device(device_str: str | None = None) -> str:
    if device_str is not None:
        return device_str
    return "cuda" if torch.cuda.is_available() else "cpu"

"""

utils/visualization_exp.py
"""
"""CLI helper — visualise compressed vs. original SBERT embeddings.

Supports both t‑SNE and PCA as low‑dimensional projections and 2‑D or 3‑D
scatter plots.  Latent embeddings are **recomputed in memory** (no cache for
compressed codes).  Works with the three AE checkpoints in the repo
(`contrastive`, `dae`, `vae`).

Example

python -m utils.visualization_exp \
  --sbert-cache data/SQUAD/sbert_cache/sbert_2254a38d6b_all-MiniLM-L6-v2.pt \
  --checkpoint  models/checkpoints/coe_text.pth \
  --projection  tsne \
  --components  2 \
  --sample-size 1200 \
  --k-near 10 

  
"""
from __future__ import annotations

import argparse
from pathlib import Path
from typing import Tuple

import torch

from evaluation.embedding_visualization import visualize_compressed_vs_original

# ---------------------------------------------------------------------------
#  Auto‑encoder loader                                                        
# ---------------------------------------------------------------------------

def _load_autoencoder(ckpt: str, *, device: torch.device | str = "cpu") -> torch.nn.Module:
    """Instantiate the proper AE subclass and load weights from *ckpt*."""
    name = Path(ckpt).name.lower()
    if "contrastive" in name or "cae" in name:
        from models.contrastive_autoencoder import ContrastiveAutoencoder as AE
    elif "dae" in name:
        from models.denoising_autoencoder import DenoisingAutoencoder as AE
    elif "vae" in name:
        from models.variational_autoencoder import VariationalAutoencoder as AE
    else:
        raise ValueError(f"Cannot infer AE type from checkpoint name: {ckpt}")

    model = AE(input_dim=384, latent_dim=64, hidden_dim=512)
    state = torch.load(ckpt, map_location="cpu")
    model.load_state_dict(state)
    return model.to(device).eval()

# ---------------------------------------------------------------------------
#  SBERT pairs loader                                                         
# ---------------------------------------------------------------------------

def _load_sbert_pairs(path: str | Path, n: int, seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor]:
    """Return `(queries, positives)` tensors of shape [n, 384] (CPU)."""
    emb = torch.load(path, map_location="cpu")
    if emb.dim() != 2 or emb.size(0) % 2 != 0:
        raise ValueError("SBERT cache must have shape [2N, D]")
    n_pairs = emb.size(0) // 2
    n = min(n, n_pairs)
    idx = torch.randperm(n_pairs, generator=torch.Generator().manual_seed(seed))[:n]
    q_idx = 2 * idx
    d_idx = q_idx + 1
    return emb[q_idx], emb[d_idx]

# ---------------------------------------------------------------------------
#  Helpers
# ---------------------------------------------------------------------------


def _infer_ae_type(ckpt_name: str) -> str:
    """Return a short tag identifying the AE flavour."""
    lower = ckpt_name.lower()
    if "contrastive" in lower or "cae" in lower:
        return "cae"
    if "dae" in lower:
        return "dae"
    if "vae" in lower:
        return "vae"
    return "ae"


def _build_default_path(
    ckpt: str,
    *,
    projection: str,
    n_components: int,
    sample_size: int,
    k_near: int,
    perplexity: float,
) -> Path:
    """Compose a descriptive file name and ensure ``fig/`` exists.

    The directory is created with parents if missing.
    """
    ae_tag = _infer_ae_type(Path(ckpt).stem)
    fname = f"{ae_tag}_{projection}_{n_components}d_{sample_size}s_{k_near}k"
    if projection == "tsne":
        fname += f"_perp{int(perplexity)}"
    fname += ".png"

    fig_dir = Path("fig")  # ← default relative folder
    fig_dir.mkdir(parents=True, exist_ok=True)
    return fig_dir / fname


# ---------------------------------------------------------------------------
#  Main
# ---------------------------------------------------------------------------


def main() -> None:
    parser = argparse.ArgumentParser(
        "Visualise AE-compressed vs. original embeddings"
    )

    # required paths
    parser.add_argument(
        "--sbert-cache",
        required=True,
        help=".pt file with SBERT cache (queries/ctx interleaved)",
    )
    parser.add_argument(
        "--checkpoint",
        required=True,
        help="Path to trained AE checkpoint (.pth)",
    )

    # visual options
    parser.add_argument(
        "--projection",
        choices=["tsne", "pca"],
        default="tsne",
        help="Low-D projection method",
    )
    parser.add_argument(
        "--components",
        type=int,
        choices=[2, 3],
        default=2,
        help="Number of projection dimensions",
    )
    parser.add_argument(
        "--perplexity",
        type=float,
        default=30.0,
        help="t-SNE perplexity (ignored for PCA)",
    )
    parser.add_argument(
        "--k-near",
        type=int,
        default=5,
        help="Nearest-neighbour threshold for hits",
    )
    parser.add_argument(
        "--bins",
        type=int,
        default=30,
        help="Histogram bins for distance plot",
    )

    # sampling & io
    parser.add_argument(
        "--sample-size",
        type=int,
        default=1000,
        help="Number of query–doc pairs to sample",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed",
    )
    parser.add_argument(
        "--out",
        default=None,  # ← NEW: None triggers automatic path generation
        help=(
            "Output figure path (PNG/PDF).  If omitted, the file is "
            "saved to ./fig/<params>.png"
        ),
    )

    args = parser.parse_args()

    # ------------------------------------------------------------------ #
    # 1. Load SBERT originals
    # ------------------------------------------------------------------ #
    q_orig, d_orig = _load_sbert_pairs(args.sbert_cache, args.sample_size, seed=args.seed)

    # ------------------------------------------------------------------ #
    # 2. Compress with AE (on-the-fly)
    # ------------------------------------------------------------------ #
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    ae = _load_autoencoder(args.checkpoint, device=device)

    with torch.no_grad():
        q_comp = ae.encode(q_orig.to(device))
        if isinstance(q_comp, tuple):
            q_comp = q_comp[0]
        d_comp = ae.encode(d_orig.to(device))
        if isinstance(d_comp, tuple):
            d_comp = d_comp[0]
        q_comp, d_comp = q_comp.cpu(), d_comp.cpu()

    # ------------------------------------------------------------------ #
    # 3. Determine output path                                           #
    # ------------------------------------------------------------------ #
    if args.out is None:  # ← NEW
        args.out = _build_default_path(
            args.checkpoint,
            projection=args.projection,
            n_components=args.components,
            sample_size=args.sample_size,
            k_near=args.k_near,
            perplexity=args.perplexity,
        )

    # ------------------------------------------------------------------ #
    # 4. Visualise                                                       #
    # ------------------------------------------------------------------ #
    metrics = visualize_compressed_vs_original(
        q_orig,
        d_orig,
        q_comp,
        d_comp,
        projection=args.projection,
        n_components=args.components,
        sample_size=args.sample_size,
        k_near=args.k_near,
        perplexity=args.perplexity,
        bins=args.bins,
        random_state=args.seed,
        save_path=str(args.out),  # ensure Path → str
        save_negatives_path=str(args.out).replace(".png", "_negatives_distribution.png")  # Save negatives plot
    )



    print(f"Figure saved  {args.out}\n")


if __name__ == "__main__":
    main()
"""

