### Directory tree for: .

./
├── AUTORAG.code-workspace
├── README.md
├── config/
│   ├── config.yaml
│   └── prompts/
│       └── system_prompt.txt
├── data/
│   ├── SQUAD/
│   │   ├── sbert_cache/
│   │   │   ├── sbert_2254a38d6b_all-MiniLM-L6-v2.pt
│   │   │   └── sbert_7eb48b423e_all-MiniLM-L6-v2.pt
│   │   ├── squad_contrastive_embeddings.pt
│   │   ├── squad_dae_embeddings.pt
│   │   └── squad_vae_embeddings.pt
│   ├── SQUAD2/
│   │   ├── squad_contrastive_embeddings.pt
│   │   ├── squad_dae_embeddings.pt
│   │   └── squad_vae_embeddings.pt
│   ├── UDA/
│   │   ├── uda_contrastive_embeddings.pt
│   │   ├── uda_dae_embeddings.pt
│   │   └── uda_vae_embeddings.pt
│   ├── data_processing.py
│   └── torch_datasets.py
├── evaluation/
│   ├── autoencoder_metrics.py
│   ├── benchmark.py
│   ├── embedding_visualization.py
│   ├── generation_metrics.py
│   └── retrieval_metrics.py
├── fig/
│   ├── cae_tsne_2d_1200s_10k_perp30.png
│   ├── dae_tsne_2d_1200s_10k_perp30.png
│   ├── dae_tsne_2d_1200s_10k_perp30_negatives_distribution.png
│   └── old/
│       ├── pca_3d.png
│       ├── tsne.png
│       ├── tsne_pairs.png
│       ├── tsne_rank.png
│       ├── tsne_squad.png
│       ├── tsne_summary.png
│       └── viz.png
├── generation/
│   └── generator.py
├── logs/
│   └── run.log
├── main.py
├── models/
│   ├── base_autoencoder.py
│   ├── checkpoints/
│   │   ├── contrastive_ae.pth
│   │   ├── dae_text.pth
│   │   └── vae_text.pth
│   ├── contrastive_autoencoder.py
│   ├── denoising_autoencoder.py
│   └── variational_autoencoder.py
├── requeriments.txt
├── retrieval/
│   ├── base.py
│   ├── bm25.py
│   ├── dpr.py
│   ├── embedder.py
│   └── retriever.py
├── save_snapshot.sh*
├── snapshot.txt
├── style_guide.md
├── test/
│   ├── test_data_processing.py
│   ├── test_evaluation.py
│   ├── test_loss_functions.py
│   ├── test_models.py
│   ├── test_retrieval.py
│   ├── test_train_scripts.py
│   └── test_visualization.py
├── tests.ipynb
├── training/
│   ├── loss_functions.py
│   ├── train_cae.py
│   ├── train_dae.py
│   └── train_vae.py
└── utils/
    ├── data_utils.py
    ├── load_config.py
    ├── training_utils.py
    └── visualization_exp.py

18 directories, 67 files


### File contents

README.md
"""
# rag\_autoencoder\_tfm

A repository for training and evaluating retrieval-augmented generation (RAG) pipelines enhanced by various autoencoder compression methods. Supported variants include:

* Variational Autoencoder (VAE)
* Denoising Autoencoder (DAE)
* Contrastive Autoencoder (CAE)

This framework covers from data preparation, model training, retrieval to optional generation with LLMs, and comprehensive evaluation metrics.

---

## Table of Contents

1. [Features](#features)
2. [Prerequisites](#prerequisites)
3. [Setup](#setup)
4. [Configuration](#configuration)
5. [Data Preparation](#data-preparation)
6. [Training](#training)

   * [VAE](#vae)
   * [DAE](#dae)
   * [CAE](#cae)
7. [Pipeline Execution](#pipeline-execution)
8. [Evaluation](#evaluation)
10. [Project Structure](#project-structure)
11. [Testing](#testing)

---

## Features

* Encode text embeddings using SBERT and compress with VAE/DAE/CAE.
* Retrieve top‑k relevant documents using cosine, Euclidean or Mahalanobis similarity.
* Generate answers via RAG using OpenAI API with custom system prompts.
* Evaluate retrieval (Recall\@k, MRR, nDCG) and generation (BLEU, ROUGE-L, METEOR) with bootstrap CIs.
* Configurable via YAML; extensible for other datasets or API providers.

## Prerequisites

* Python ≥ 3.10
* GPU recommended for large-scale embedding and training. (Trained ona 4060 8GB VRAM)
* OpenAI API key (set in `.env` as `OPENAI_API_KEY`).

Install dependencies:

```bash
pip install -r requirements.txt
```

## Setup

1. Clone the repository:

```bash

git clone https://github.com/engares/latent-rag.git
cd latent-rag
```

2. Create `.env` with your OpenAI key:
```ini
OPENAI_API_KEY=your_api_key_here
````

3. Adjust paths and hyperparameters in **`config/config.yaml`** as needed.

## Configuration

**`config/config.yaml`** contains:

* Project metadata (name, version)
* Directory paths (`data_dir`, `checkpoints_dir`, `logs_dir`)
* Embedding model settings
* Autoencoder parameters and checkpoints
* Training hyperparameters (batch size, epochs, LR)
* Retrieval & generation options
* Evaluation metrics
* Logging level and file

System prompt for generation is located in **`config/prompts/system_prompt.txt`**.

## Data Preparation

Data tensors for SQuAD are generated automatically when running training or pipeline. To prepare manually:

```bash
python -c "from utils.data_utils import ensure_squad_data; ensure_squad_data(output_dir='./data/SQUAD_DELETE')"
```

This creates:

* `data/SQUAD/squad_vae_embeddings.pt`
* `data/SQUAD/squad_dae_embeddings.pt`
* `data/SQUAD/squad_contrastive_embeddings.pt`

## Training

### VAE

```bash
python training/train_vae.py \
  --dataset squad \
  --epochs 50 \
  --batch_size 128 \
  --lr 1e-3 \
  --save_path models/checkpoints/vae_text.pth
```

### DAE

```bash
python training/train_dae.py \
  --dataset squad \
  --epochs 50 \
  --batch_size 128 \
  --lr 1e-3 \
  --save_path models/checkpoints/dae_text.pth
```

### CAE

```bash
python training/train_cae.py \
  --dataset squad \
  --epochs 50 \
  --batch_size 128 \
  --lr 1e-3 \
  --save_path models/checkpoints/contrastive_ae.pth
```

All training scripts support early stopping, checkpointing and configurable device.

## Pipeline Execution

Run the end-to-end RAG pipeline:

```bash
python main.py --config config/config.yaml --ae_type vae
```

Replace `--ae_type` with `dae`, `contrastive`, `all` or `none`. The pipeline will:

1. Encode corpus and queries
2. Retrieve top‑k documents
3. Optionally generate answers via GPT-4o-mini (if `--generate` is specified)
4. Evaluate retrieval and generation metrics

To enable the generation step, include the `--generate` flag:

```bash
python main.py --config config/config.yaml --ae_type vae --generate
```

## Evaluation

* Retrieval metrics: per-query and aggregated Recall\@k, MRR, nDCG.
* Generation metrics: BLEU, ROUGE-L, METEOR with 95% bootstrap CIs.
* Visualise embeddings via **`evaluation/autoencoder_metrics.py`** (t-SNE plots).

Commands:

```python
from evaluation.retrieval_metrics import evaluate_retrieval
from evaluation.generation_metrics import evaluate_generation_bootstrap
```


Perfecto. Aquí tienes un nuevo apartado **[9. Embedding Visualisation](#embedding-visualisation)** para añadir en tu `README.md`, totalmente integrado con el estilo del proyecto y basado en los módulos que ya has implementado:


### Experimental Embedding Visualisation

This module provides intuitive **visual diagnostics** of how well autoencoder-compressed embeddings preserve semantic similarity. It allows direct comparison between **original SBERT embeddings** and their **compressed representations** (via VAE, DAE or CAE).

Supported plots include:

* Low-dimensional **t-SNE or PCA projections** (2D or 3D) of (query, document) pairs.
* **Colour-coded scatter** plots showing cosine distances.
* **Histogram + CDF** of pairwise distances before and after compression.
* **Positive vs. Negative distance distributions**, to visualise separation margins.

Each visualisation is saved to disk automatically.

### Example

```bash
python -m utils.visualization_exp \
  --sbert-cache data/SQUAD/sbert_cache/sbert_2254a38d6b_all-MiniLM-L6-v2.pt \
  --checkpoint  models/checkpoints/contrastive_ae.pth \
  --projection  tsne \
  --components  2 \
  --sample-size 1200 \
  --k-near 10
```

This generates:

* `fig/cae_tsne_2d_1200s_10k_perp30.png`:
  Low-dimensional visualisation + recall and distance histograms.
* `fig/cae_tsne_2d_1200s_10k_perp30_negatives_distribution.png`:
  Side-by-side histogram comparing cosine distances:

  * $q \to d^+$ (positives)
  * $q \to d^-$ (negatives)


<br>

## Project Structure

```text
src/
├── config/           # YAML and prompts
├── data/             # Embedding tensors and loaders
├── evaluation/       # Metrics and visualisations
├── generation/       # RAG generator
├── models/           # AE implementations
├── retrieval/        # Embeddings & retriever
├── training/         # Training scripts and loss functions
├── utils/            # Helpers (config, data, logging)
├── test/             # Unit tests (pytest)
├── main.py           # CLI orchestration
├── requirements.txt
└── style_guide.md
```

## Testing

Run all tests via pytest:

```bash
PYTHONPATH=. pytest -q
```

Coverage threshold: 80% (unit tests for data processing, models, retrieval, evaluation, training scripts).


"""

config/config.yaml
"""
project:
  name: rag_autoencoder_tfm
  version: "0.1"

paths:
  data_dir: "./data/SQUAD"
  checkpoints_dir: "./models/checkpoints"
  logs_dir: "./logs"

embedding_model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  max_length: 256

models:

# VARIATIONAL AUTOENCODER

  vae:
    input_dim: 384 # S- BERT embedding dimension
    latent_dim: 64 
    hidden_dim: 512
    dataset_file: "squad_vae_embeddings.pt"          
    checkpoint: "vae_text.pth"

# DENOISING AUTOENCODER
  dae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_file: "squad_dae_embeddings.pt"
    checkpoint: "dae_text.pth"

# CONTRASTIVE AUTOENCODER
  contrastive:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_file: "squad_cae_embeddings.pt"
    checkpoint: "contrastive_ae.pth"

data:
  dataset: "squad"            #  "uda"  |  "squad"
  version: "v1"          #  v1, v2  ONly for squad
  max_samples:         #  optional, blankk to use all samples
  include_unanswerable: false   #  Only Squad v2 has this option

training:
  batch_size: 128
  epochs: 50
  learning_rate: 1e-3
  seed: 42
  device: "cuda"  # "cpu"
  deterministic: false      # (true = modo debug)

retrieval:
  similarity_metric: "cosine"   # cosine, mahalanobis
  top_k: 20
  compress_embeddings: true 

generation:
  provider: "openai"       # Just OpenAI by now
  model: "gpt-4o-mini"
  temperature: 0.3
  max_tokens: 256
  system_prompt_path: "./config/prompts/system_prompt.txt"


evaluation:
  retrieval_metrics: ["Recall@5", "MRR@10", "nDCG@10"] # You can change the k top  simply by changing the @k e.j Recall@10, "MRR@20"
  generation_metrics: ["ROUGE-L", "BLEU"]

logging:
  level: "INFO"
  log_to_file: true
  log_file: "./logs/run.log"

"""

config/prompts/system_prompt.txt
"""
Here is the user query and relevant text chunks. Step 1: Summarize user question in simpler words. Step 2: Decide which retrieved text chunks directly apply. Step 3: Combine those chunks into an outline. Step 4: Draft a single, coherent answer. Show all steps, then provide a final refined answer.
"""

data/data_processing.py
"""
#/data_processing.py

import random
import json
import re
from typing import List, Dict
from datasets import load_dataset

# ---------------------------------------------------------
# UDA Dataset Preprocessing for Autoencoder Training
# ---------------------------------------------------------
# Supports: Denoising AE (with artificial noise), VAE, Contrastive AE
# ---------------------------------------------------------

def clean_text(text: str) -> str:
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def add_noise(text: str, removal_prob=0.1, swap_prob=0.05) -> str:
    words = text.split()
    # Remove tokens
    words = [w for w in words if random.random() > removal_prob]
    # Swap nearby tokens
    for i in range(len(words)-1):
        if random.random() < swap_prob:
            words[i], words[i+1] = words[i+1], words[i]
    return " ".join(words)

def build_dae_dataset(samples: List[str]) -> List[Dict[str, str]]:
    dataset = []
    for original in samples:
        noisy = add_noise(original)
        dataset.append({"input": noisy, "target": original})
    return dataset

def build_contrastive_pairs(dataset, max_negatives=1) -> List[Dict]:
    pairs = []
    for example in dataset:
        q = example["query"]
        pos = example["positive_passages"][0]["text"]
        negs = [n["text"] for n in example["negative_passages"][:max_negatives]]
        for neg in negs:
            pairs.append({"query": q, "positive": pos, "negative": neg})
    return pairs

"""

data/torch_datasets.py
"""
# /data/torch_datasets.py
import torch
from torch.utils.data import Dataset
from typing import Dict, List, Tuple


# ---------- UTILIDADES COMUNES ------------------------------------------------
def _load_pt(path: str) -> Dict[str, torch.Tensor]:
    """
    Carga un fichero .pt con tensores y asegura dtype = float32 en CPU.
    El fichero se espera como un dict { name: Tensor }.
    """
    data = torch.load(path, map_location="cpu")
    return {k: v.float() for k, v in data.items()}


# ---------- DATASETS ---------------------------------------------------------


class EmbeddingVAEDataset(Dataset):
    """
    Carga el fichero .pt generado por `ensure_uda_data`.
    Estructura esperada:
        {"input": <tensor [N×D]>, "target": <tensor [N×D]>}
    """
    def __init__(self, path: str):
        data = torch.load(path, map_location="cpu")
        self.input  = data["input"].float()
        self.target = data["target"].float()
        assert self.input.shape == self.target.shape, "input/target tamaño desigual"

    def __len__(self):
        return self.input.size(0)

    def __getitem__(self, idx):
        return {
            "input":  self.input[idx],
            "target": self.target[idx],
        }


class EmbeddingDAEDataset(Dataset):
    """
    Carga 'uda_dae_embeddings.pt' producido por `ensure_uda_data`.

    Estructura:
        {
            "input":  Tensor [N × D]  (embeddings con ruido)
            "target": Tensor [N × D]  (embeddings limpios)
        }
    """
    def __init__(self, path: str):
        d = torch.load(path, map_location="cpu")
        self.x  = d["input" ].float()
        self.y  = d["target"].float()
        assert self.x.shape == self.y.shape, "Input / target mismatch"

    def __len__(self):          return self.x.size(0)
    def __getitem__(self, idx): return {"x": self.x[idx], "y": self.y[idx]}

    

class EmbeddingTripletDataset(Dataset):
    """
    Carga 'uda_contrastive_embeddings.pt' generado por `ensure_uda_data`.

    Estructura esperada:
        {
            "query":     Tensor [N × D],
            "positive":  Tensor [N × D],
            "negative":  Tensor [N × D]
        }
    """
    def __init__(self, path: str):
        data = torch.load(path, map_location="cpu")
        self.q  = data["query"].float()
        self.p  = data["positive"].float()
        self.n  = data["negative"].float()
        assert self.q.shape == self.p.shape == self.n.shape, "Dimensiones incompatibles"

    def __len__(self) -> int:          return self.q.size(0)

    def __getitem__(self, idx):        # devuelvo tensores individuales
        return {"q": self.q[idx],
                "p": self.p[idx],
                "n": self.n[idx]}


# ---------- PRUEBA RÁPIDA -----------------------------------------------------
if __name__ == "__main__":
    dae_ds = EmbeddingDAEDataset("./data/squad_dae_embeddings.pt")
    vae_ds = EmbeddingVAEDataset("./data/squad_vae_embeddings.pt")
    con_ds = EmbeddingTripletDataset("./data/squad_contrastive_embeddings.pt")

    print("DAE sample ⇒", {k: v.shape for k, v in dae_ds[0].items()})
    print("Contrastive sample ⇒", {k: v.shape for k, v in con_ds[0].items()})
    print("VAE sample ⇒", {k: v.shape for k, v in vae_ds[0].items()})

"""

evaluation/autoencoder_metrics.py
"""
import torch



def evaluate_reconstruction_loss(x: torch.Tensor, x_reconstructed: torch.Tensor, reduction: str = "mean") -> float:
    """Calcula el error de reconstrucción (MSE)."""
    loss_fn = torch.nn.MSELoss(reduction=reduction)
    return loss_fn(x_reconstructed, x).item()
"""

evaluation/benchmark.py
"""
# /evalaution/benchmark.py
from typing import Dict, Sequence
from retrieval.bm25 import BM25Retriever
from retrieval.dpr  import DPRRetriever
from retrieval.embedder import EmbeddingCompressor
from utils.load_config import load_config
from evaluation.retrieval_metrics import evaluate_retrieval, paired_bootstrap_test

Retrievers = {
    "bm25":  BM25Retriever(),
    "dpr":   DPRRetriever(),
    "sbert": EmbeddingCompressor(),          # no AE
    "vae":   EmbeddingCompressor(ae_type="vae"),
    "dae":   EmbeddingCompressor(ae_type="dae"),
    "cae":   EmbeddingCompressor(ae_type="contrastive"),
}

def run_benchmark(queries: Sequence[str],
                  corpus:  Sequence[str],
                  gold:    Sequence[Sequence[str]],
                  cfg_path="./config/config.yaml") -> None:
    cfg = load_config(cfg_path)
    metrics = cfg["evaluation"]["retrieval_metrics"]
    results: Dict[str, Dict[str, float]] = {}

    for name, retr in Retrievers.items():
        retr.build_index(corpus)
        retrieved = [retr.retrieve(q, k=cfg["retrieval"]["top_k"]) for q in queries]
        summary   = evaluate_retrieval(retrieved, gold, metrics)   # media + sd
        results[name] = {m: v["mean"] for m, v in summary.items()}
        print(f"{name.upper():<6}", summary)

    # Significancia (ejemplos)
    _print_sig(results, queries, gold, metrics[0])

def _print_sig(res, queries, gold, metric):
    from itertools import combinations
    for a, b in combinations(res.keys(), 2):
        pa, pb = res[a][metric], res[b][metric]
        # bootstrap paired significance (aproveche evaluate_generation_bootstrap si lo desea)
        print(f"{a} vs {b}: Δ={pa-pb:+.4f}")

"""

evaluation/embedding_visualization.py
"""
from __future__ import annotations

from collections import defaultdict
from itertools import cycle
from typing import Dict, Iterable, List, Mapping, MutableSequence, Optional, Sequence

import matplotlib.cm as cm
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from matplotlib.colors import Colormap
from matplotlib.ticker import PercentFormatter
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 – required for 3-D scatter
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
Tensor = torch.Tensor  

import matplotlib.cm as cm
from matplotlib.colors import Normalize


# ---------------------------------------------------------------------------
# Helper utilities
# ---------------------------------------------------------------------------



def _to_numpy(x: Tensor) -> "np.ndarray":  # type: ignore[name-defined]
    """Detach a tensor and move to CPU as *float32* NumPy array."""
    return x.detach().cpu().float().numpy()


def _rank_positive(q: Tensor, d: Tensor) -> Tensor:
    """Return **1-based** rank of each positive document using cosine similarity."""
    sim = F.cosine_similarity(q.unsqueeze(1), d.unsqueeze(0), dim=-1)
    return sim.argsort(dim=1, descending=True).argsort(dim=1).diagonal() + 1


def _project(
    x: Tensor,
    *,
    method: str,
    n_components: int,
    perplexity: float,
    seed: int,
) -> Tensor:
    """Return a low-dimensional projection via t-SNE or PCA."""
    if method == "tsne":
        tsne = TSNE(
            n_components=n_components,
            perplexity=perplexity,
            metric="cosine",
            init="pca",
            max_iter=1_000,
            random_state=seed,
        )
        return torch.from_numpy(tsne.fit_transform(_to_numpy(x))).float()
    if method == "pca":
        pca = PCA(n_components=n_components, random_state=seed)
        return torch.from_numpy(pca.fit_transform(_to_numpy(x))).float()
    raise ValueError("method must be 'tsne' or 'pca'")





# ---------------------------------------------------------------------------
# Plotting primitives
# ---------------------------------------------------------------------------


def _link(ax, p: Tensor, q: Tensor, colour: str):
    if p.numel() == 2:            # 2-D
        ax.plot([p[0], q[0]], [p[1], q[1]],
                color=colour, linewidth=0.8, alpha=0.8)
    else:                         # 3-D
        ax.plot([p[0], q[0]], [p[1], q[1]], [p[2], q[2]],
                color=colour, linewidth=0.8, alpha=0.8)


def _scatter_pairs(
    ax,
    q_emb: Tensor,
    d_emb: Tensor,
    dist: Tensor,
    *,
    cmap_name: str = "viridis_r",
    alpha: float = 0.3,
    s: int = 30,
    max_links: int = 10,
):
    """
    Dibuja las parejas (query-doc) coloreadas según la distancia del coseno.

    Params
    ------
    q_emb, d_emb : (N, 2|3)  Embeddings proyectados de queries y docs.
    dist         : (N,)      Distancias del coseno ||q - d|| en el espacio original.
    max_links    : int       Número máximo de enlaces (los más disimilares).
    """
    dim = q_emb.size(1)
    norm = Normalize(vmin=float(dist.min()), vmax=float(dist.max()))
    cmap = cm.get_cmap(cmap_name)

    # Matriz RGBA con alpha constante
    colors = cmap(norm(_to_numpy(dist)))
    colors[:, -1] = alpha

    # Dispersión de queries
    ax.scatter(
        q_emb[:, 0], q_emb[:, 1],
        *(q_emb[:, 2].T,) if dim == 3 else (),
        c=colors, marker="o", s=s, label="Query"
    )
    # Dispersión de docs
    ax.scatter(
        d_emb[:, 0], d_emb[:, 1],
        *(d_emb[:, 2].T,) if dim == 3 else (),
        c=colors, marker="^", s=s, label="Doc"
    )

    # Índices de las distancias más altas
    top_idx = dist.topk(min(max_links, len(dist))).indices

    # Enlaces coloreados (solo top-k)
    for i in top_idx:
        _link(ax, q_emb[i], d_emb[i], colour=colors[i])

    ax.set_xticks([]); ax.set_yticks([])
    if dim == 3:
        ax.set_zticks([])
    ax.legend(frameon=False, fontsize=7, loc="upper right")


def _hist_cdf(ax: "plt.Axes", d1: Tensor, d2: Tensor, *, bins: int) -> None:
    """Histogram + CDF of two 1-D distance distributions."""
    cmap = cm.get_cmap("viridis")  # Use the viridis colormap
    color1 = cmap(0.3)  # A color from the lower range of viridis
    color2 = cmap(0.7)  # A color from the higher range of viridis

    # Histogram
    ax.hist(d1.numpy(), bins=bins, alpha=0.5, label="Original dist.", color=color1)
    ax.hist(d2.numpy(), bins=bins, alpha=0.5, label="Compressed dist.", color=color2)
    ax.set_xlabel("Pair cosine distance ")
    ax.set_ylabel("Frequency")
    ax.legend(frameon=False, fontsize=7)

    # CDF
    ax2 = ax.twinx()
    for data, lbl, color in ((d1, "Original CDF", color1), (d2, "Compressed CDF", color2)):
        sorted_vals = torch.sort(data).values
        cdf = torch.arange(1, len(data) + 1) / len(data)
        ax2.plot(sorted_vals, cdf, label=lbl, color=color)
    ax2.yaxis.set_major_formatter(PercentFormatter(1.0))
    ax2.set_ylabel("CDF")
    ax2.legend(frameon=False, fontsize=7, loc="lower right")


def visualize_compressed_vs_original(
    q_orig: Tensor,
    d_orig: Tensor,
    q_comp: Tensor,
    d_comp: Tensor,
    *,
    projection: str = "tsne",
    n_components: int = 2,
    sample_size: int = 1_000,
    k_near: int = 5,
    perplexity: float = 30.0,
    bins: int = 30,
    random_state: int = 42,
    save_path: Optional[str] = None,
    save_negatives_path: Optional[str] = None,
) -> Dict[str, float]:
    """Visualise original vs. compressed embeddings and save two figures:
      1) scatter + hist/CDF
      2) dist positives vs. negatives."""

    torch.manual_seed(random_state)
    N = len(q_orig)
    if sample_size < N:
        idx = torch.randperm(N)[:sample_size]
        q_o, d_o, q_c, d_c = q_orig[idx], d_orig[idx], q_comp[idx], d_comp[idx]
    else:
        q_o, d_o, q_c, d_c = q_orig, d_orig, q_comp, d_comp


    # Recall
    rank_orig = _rank_positive(q_o, d_o)
    recall_orig = (rank_orig <= k_near).float().mean()
    rank_comp = _rank_positive(q_c, d_c)
    recall_comp = (rank_comp <= k_near).float().mean()

    # Projections (only for figure 1)
    orig_proj = _project(torch.cat([q_o, d_o]), method=projection,
                         n_components=n_components, perplexity=perplexity, seed=random_state)
    comp_proj = _project(torch.cat([q_c, d_c]), method=projection,
                         n_components=n_components, perplexity=perplexity, seed=random_state)
    q_proj_o, d_proj_o = orig_proj[: len(q_o)], orig_proj[len(q_o):]
    q_proj_c, d_proj_c = comp_proj[: len(q_o)], comp_proj[len(q_o):]

    # Cosine distances
    dist_o = 1 - F.cosine_similarity(q_o, d_o, dim=1)
    dist_c = 1 - F.cosine_similarity(q_c, d_c, dim=1)

    # ========== FIGURA 1: scatter + hist/CDF ==========
    fig = plt.figure(figsize=(14, 10 if n_components==2 else 12))
    gs = gridspec.GridSpec(2, 3, width_ratios=[1,1,0.05], height_ratios=[3,2])

    ax1 = fig.add_subplot(gs[0,0], projection="3d" if n_components==3 else None)
    ax2 = fig.add_subplot(gs[0,1], projection="3d" if n_components==3 else None)
    cbar_ax = fig.add_subplot(gs[0,2])
    ax_hist = fig.add_subplot(gs[1,:2])

    _scatter_pairs(ax1, q_proj_o, d_proj_o, dist_o)
    _scatter_pairs(ax2, q_proj_c, d_proj_c, dist_c)
    ax1.set_title(f"Original {projection.upper()} – Recall@{k_near}: {recall_orig:.1%}", fontsize=10)
    ax2.set_title(f"Compressed {projection.upper()} – Recall@{k_near}: {recall_comp:.1%}", fontsize=10)
    _hist_cdf(ax_hist, dist_o, dist_c, bins=bins)
    ax_hist.set_title(f"Pair distance distribution ({projection.upper()} {n_components}-D)")

    sm = cm.ScalarMappable(norm=Normalize(vmin=float(min(dist_o.min(), dist_c.min())),
                                         vmax=float(max(dist_o.max(), dist_c.max()))),
                           cmap="viridis_r")
    sm.set_array([])
    fig.colorbar(sm, cax=cbar_ax).set_label("Cosine distance", rotation=270, labelpad=15)

    fig.tight_layout()
    if save_path:
        fig.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

    plot_positive_vs_negative_distances(
        q_o, d_o, q_c, d_c,
        bins=bins,
        save_negatives_path=save_negatives_path
    )

    return {
        "recall_original": float(recall_orig),
        "recall_compressed": float(recall_comp),
    }


def plot_positive_vs_negative_distances(
    q_o: Tensor,
    d_o: Tensor,
    q_c: Tensor,
    d_c: Tensor,
    *,
    bins: int,
    save_negatives_path: Optional[str] = None
) -> None:
    """Plot positive vs. negative distances for original and compressed embeddings."""
    # Muestreo de negativos (una permutación aleatoria)
    perm = torch.randperm(len(d_o))
    neg_idx = torch.where(perm == torch.arange(len(d_o)), (perm + 1) % len(d_o), perm)
    d_neg_o = d_o[neg_idx]
    d_neg_c = d_c[neg_idx]

    # Calcular distancias del coseno para positivos y negativos
    dist_o = 1 - F.cosine_similarity(q_o, d_o, dim=1)
    dist_c = 1 - F.cosine_similarity(q_c, d_c, dim=1)
    dist_neg_o = 1 - F.cosine_similarity(q_o, d_neg_o, dim=1)
    dist_neg_c = 1 - F.cosine_similarity(q_c, d_neg_c, dim=1)

    fig, (axp, axn) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Distribución originales
    axp.hist(dist_o.numpy(), bins=bins, alpha=0.6, label="Positives", color="yellowgreen")
    axp.hist(dist_neg_o.numpy(), bins=bins, alpha=0.6, label="Negatives", color="firebrick")
    axp.set_title("Original: q–d⁺ vs q–d⁻")
    axp.set_xlabel("Cosine distance")
    axp.set_ylabel("Frequency")
    axp.legend(frameon=False)

    # Distribución comprimidas
    axn.hist(dist_c.numpy(), bins=bins, alpha=0.6, label="Positives", color="yellowgreen")
    axn.hist(dist_neg_c.numpy(), bins=bins, alpha=0.6, label="Negatives", color="firebrick")
    axn.set_title("Compressed: q–d⁺ vs q–d⁻")
    axn.set_xlabel("Cosine distance")
    axn.legend(frameon=False)

    fig.tight_layout()
    if save_negatives_path:
        fig.savefig(save_negatives_path, dpi=300, bbox_inches="tight")
    plt.show()

"""

evaluation/generation_metrics.py
"""
from __future__ import annotations

"""Métricas de generación con *bootstrap* y prueba de significancia emparejada.

Uso principal:
--------------
>>> mean_ci = evaluate_generation_bootstrap(refs, cands, metrics=["BLEU", "ROUGE-L"])
>>> pval = paired_bootstrap_test(refs, sys_a, sys_b, metric="BLEU")
"""

from collections.abc import Callable
from typing import List, Dict, Tuple
import numpy as np
from sacrebleu.metrics import BLEU as _BLEUMetric
from rouge_score import rouge_scorer
import random

###############################################################################
#  MÉTRICAS BÁSICAS                                                           #
###############################################################################

_bleu = _BLEUMetric()
_scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)


def compute_bleu(candidates: List[str], references: List[str]) -> float:
    """BLEU corpus‐level sacreBLEU (0‑100). Handles both flat and nested reference lists."""
    # Flatten references if it's a list of lists (shouldn't be for single-ref BLEU)
    if references and isinstance(references[0], list):
        # If already nested, flatten one level
        references = [ref for sublist in references for ref in (sublist if isinstance(sublist, list) else [sublist])]
    return _bleu.corpus_score(candidates, [references]).score


def compute_rouge_l(candidates: List[str], references: List[str]) -> float:
    """Promedio de ROUGE‑L (F1) ×100."""
    def to_str(x):
        if isinstance(x, list):
            return " ".join(map(str, x))
        return str(x)
    scores = [
        _scorer.score(to_str(ref), to_str(cand))["rougeL"].fmeasure * 100
        for ref, cand in zip(references, candidates)
    ]
    return float(np.mean(scores))


_metric_fn: Dict[str, Callable[[List[str], List[str]], float]] = {
    "BLEU": compute_bleu,
    "ROUGE-L": compute_rouge_l,
}

###############################################################################
#  BOOTSTRAP                                                                  #
###############################################################################

def _bootstrap_ci(
    func: Callable[[List[str], List[str]], float],
    refs: List[str],
    cands: List[str],
    n_samples: int = 2000,
    alpha: float = 0.05,
    seed: int | None = None,
) -> Tuple[float, float, float]:
    """Devuelve media, límite inferior y superior del IC al (1‑alpha)."""
    if seed is not None:
        random.seed(seed)
    N = len(refs)
    stats = []
    for _ in range(n_samples):
        idx = [random.randint(0, N - 1) for _ in range(N)]
        stats.append(func([cands[i] for i in idx], [refs[i] for i in idx]))
    stats_np = np.array(stats)
    mean = stats_np.mean()
    lower = np.percentile(stats_np, 100 * alpha / 2)
    upper = np.percentile(stats_np, 100 * (1 - alpha / 2))
    return mean, lower, upper


def evaluate_generation_bootstrap(
    references: List[str],
    candidates: List[str],
    metrics: List[str] | None = None,
    n_samples: int = 2000,
    alpha: float = 0.05,
    seed: int | None = None,
) -> Dict[str, Dict[str, float]]:
    """Calcula métricas + IC al 95 % mediante bootstrap.

    Retorna: {metric: {"mean": m, "ci_lower": l, "ci_upper": u}}
    """
    if metrics is None:
        metrics = ["BLEU", "ROUGE-L"]

    assert len(references) == len(candidates) >= 100, ( 
        "Se requieren al menos 100 pares ref‑cand para un IC mínimo; se recomienda ≥1000."
    )

    results: Dict[str, Dict[str, float]] = {}
    for m in metrics:
        if m not in _metric_fn:
            raise ValueError(f"Non soported metric '{m}'")
        mean, lo, hi = _bootstrap_ci(_metric_fn[m], references, candidates, n_samples, alpha, seed)
        results[m] = {"mean": mean, "ci_lower": lo, "ci_upper": hi}
    return results

###############################################################################
#  PAIRED BOOTSTRAP SIGNIFICANCE TEST                                         #
###############################################################################

def paired_bootstrap_test(
    references: List[str],
    sys_a: List[str],
    sys_b: List[str],
    metric: str = "BLEU",
    n_samples: int = 10000,
    seed: int | None = None,
) -> Dict[str, float]:
    """Prueba de significancia emparejada (bootstrap) entre dos sistemas.

    Devuelve un dict: {"diff_mean": d, "ci_lower": lo, "ci_upper": hi, "p_value": p}
    p‑value ≈ proporción de muestras con diferencia ≤0 (o ≥0, según el signo).
    """
    assert len(references) == len(sys_a) == len(sys_b)
    if seed is not None:
        random.seed(seed)

    if metric not in _metric_fn:
        raise ValueError(f"Métrica '{metric}' no soportada.")
    fn = _metric_fn[metric]

    N = len(references)
    diffs = []
    for _ in range(n_samples):
        idx = [random.randint(0, N - 1) for _ in range(N)]
        a_score = fn([sys_a[i] for i in idx], [references[i] for i in idx])
        b_score = fn([sys_b[i] for i in idx], [references[i] for i in idx])
        diffs.append(a_score - b_score)

    diffs_np = np.array(diffs)
    diff_mean = diffs_np.mean()
    ci_lower = np.percentile(diffs_np, 2.5)
    ci_upper = np.percentile(diffs_np, 97.5)
    # p‑value: H0: diff <= 0  (si diff_mean>0) ó diff >=0 (si diff_mean<0)
    if diff_mean >= 0:
        p_val = (diffs_np <= 0).mean()
    else:
        p_val = (diffs_np >= 0).mean()

    return {
        "diff_mean": diff_mean,
        "ci_lower": ci_lower,
        "ci_upper": ci_upper,
        "p_value": p_val,
    }

"""

evaluation/retrieval_metrics.py
"""
# evaluation/retrieval_metrics.py

from __future__ import annotations

import numpy as np
from typing import List, Sequence, Dict, Tuple, Union
              
ID = Union[int, str]

###############################################################################
#  Métricas elementales (1 consulta)
###############################################################################

def recall_at_k(retrieved: Sequence[ID], relevant: Sequence[ID], k: int) -> float:
    if not relevant:
        return 0.0
    return len(set(retrieved[:k]) & set(relevant)) / len(relevant)

def mrr(retrieved: Sequence[ID], relevant: Sequence[ID]) -> float:
    for i, d in enumerate(retrieved, 1):
        if d in relevant:
            return 1.0 / i
    return 0.0

def ndcg_at_k(retrieved: Sequence[ID], relevant: Sequence[ID], k: int) -> float:
    dcg = sum(
        1.0 / np.log2(i + 2) if d in relevant else 0.0
        for i, d in enumerate(retrieved[:k])
    )
    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), k)))
    return dcg / idcg if idcg else 0.0

###############################################################################
#  Vectorización sobre lotes
###############################################################################

def _parse_metric(m: str) -> Tuple[str, int | None]:
    return (m.split("@")[0], int(m.split("@")[1])) if "@" in m else (m, None)

def _score_single(
    retrieved: Sequence[ID],
    relevant: Sequence[ID],
    name: str,
    k: int | None,
) -> float:
    name = name.lower()
    if name == "recall" and k is not None:
        return recall_at_k(retrieved, relevant, k)
    if name == "mrr":
        return mrr(retrieved[: (k or len(retrieved))], relevant)
    if name == "ndcg" and k is not None:
        return ndcg_at_k(retrieved, relevant, k)
    raise ValueError(f"Metric '{name}' not found.")

def evaluate_retrieval(retrieved_batch: List[Sequence[ID]] | Sequence[ID],
    relevant_batch: List[Sequence[ID]] | Sequence[ID],
    metrics: List[str] | None = None,
    *,
    return_per_query: bool = False,
) -> Dict[str, Dict[str, float]] | Tuple[Dict[str, Dict[str, float]],
                                         List[Dict[str, float]]]:
    
    # ── Normalizar entrada a lote ──────────────────────────────────────────
    single = isinstance(retrieved_batch[0], (str, int))  # type: ignore[index]
    if single:
        retrieved_batch = [retrieved_batch]              # type: ignore[assignment]
        relevant_batch  = [relevant_batch]               # type: ignore[assignment]

    assert len(retrieved_batch) == len(relevant_batch), \
        "retrieved_batch and relevant_batch must have the same length."

    if not metrics:
        raise ValueError("No metrics specified.")

    Q = len(retrieved_batch)
    per_query: List[Dict[str, float]] = [{} for _ in range(Q)]
    summary: Dict[str, Dict[str, float]] = {}

    for m in metrics:
        name, k = _parse_metric(m)
        vals = [
            _score_single(r, rel, name, k)
            for r, rel in zip(retrieved_batch, relevant_batch)
        ]
        summary[m] = {
            "mean": float(np.mean(vals)),
            "std":  float(np.std(vals, ddof=1)) if Q > 1 else 0.0,
        }
        for d, v in zip(per_query, vals):
            d[m] = v

    if return_per_query:
        return summary, per_query
    if single:
        return {k: v["mean"] for k, v in summary.items()}      # compat.
    return summary

"""

generation/generator.py
"""
# generation/generator.py  – RAG (Refactor estilo train_vae)

from __future__ import annotations
import os, textwrap, logging
from typing import List, Dict, Any
from dataclasses import dataclass, field
from openai import OpenAI, AsyncOpenAI

def _load_prompt(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        logging.getLogger(__name__).warning("Prompt no encontrado: %s", path)
        return ""

@dataclass
class LLMSettings:
    model: str = "gpt-4o-mini"
    temperature: float = 0.3
    top_p: float = 1.0
    max_tokens: int = 512
    system_prompt_path: str = "./config/prompts/system_prompt.txt"
    system_prompt: str = field(init=False)

    def __post_init__(self):
        self.system_prompt = _load_prompt(self.system_prompt_path)

@dataclass
class GeneratorConfig:
    llm: LLMSettings = field(default_factory=LLMSettings)
    max_context_tokens: int = 4096
    provider: str = "openai"          
    extras: Dict[str, Any] = field(default_factory=dict)


###############################################################################
#  RAG GENERATOR                                                               #
###############################################################################

class RAGGenerator:
    """Generador basado en LLM + documentos recuperados."""

    def __init__(self, cfg: Dict[str, Any], **overrides):
        # fusinon YAML + overrides CLI
        gen_cfg_dict = {**cfg.get("generation", {}), **overrides}

        llm_cfg_dict = gen_cfg_dict.pop("llm", {})
        self.cfg = GeneratorConfig(
            llm=LLMSettings(**llm_cfg_dict),
            **{k: v for k, v in gen_cfg_dict.items() if k in {"max_context_tokens", "provider"}},
            extras={k: v for k, v in gen_cfg_dict.items() if k not in {"max_context_tokens", "provider"}}
        )

        self.logger = logging.getLogger(self.__class__.__name__)
        self._init_openai()

    # ---------------- PUBLIC API -------------------------------------------
    def generate(self, query: str, retrieved_docs: List[str]) -> str:
        prompt = self._build_prompt(query, retrieved_docs)
        self.logger.debug("Prompt (%d chars) construido.", len(prompt))

        response = self.client.chat.completions.create(
            model=self.cfg.llm.model,
            temperature=self.cfg.llm.temperature,
            top_p=self.cfg.llm.top_p,
            max_tokens=self.cfg.llm.max_tokens,
            messages=[
                {"role": "system", "content": self.cfg.llm.system_prompt},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()


    async def generate_async(self, query: str, retrieved_docs: List[str]) -> str:
        prompt = self._build_prompt(query, retrieved_docs)
        self.logger.debug("Prompt async (%d chars).", len(prompt))

        client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = await client.chat.completions.create(
            model=self.cfg.llm.model,
            temperature=self.cfg.llm.temperature,
            top_p=self.cfg.llm.top_p,
            max_tokens=self.cfg.llm.max_tokens,
            messages=[
                {"role": "system", "content": self.cfg.llm.system_prompt},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()

    # ---------------- INTERNALS -------------------------------------------
    def _init_openai(self) -> None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise EnvironmentError(
                "Variable OPENAI_API_KEY no definida. Añádela a tu .env o al entorno."
            )
        self.client = OpenAI(api_key=api_key)
        self.logger.info("API Key OpenAI cargada correctamente.")

    def _build_prompt(self, query: str, docs: List[str]) -> str:
        context = self._truncate_docs(docs)
        joined = "\n\n".join(f"Doc {i+1}: {d}" for i, d in enumerate(context))
        return textwrap.dedent(
            f"""\
            Utiliza exclusivamente la siguiente información para responder.\n\n{joined}\n\n
            Pregunta: {query}\n\nRespuesta:"""
        )

    def _truncate_docs(self, docs: List[str]) -> List[str]:
        max_chars = self.cfg.max_context_tokens * 4   # heuristic ≈ tokens*4
        out, acc = [], 0
        for d in docs:
            if acc + len(d) > max_chars:
                break
            out.append(d)
            acc += len(d)
        return out

"""

main.py
"""
from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple

import torch
from dotenv import load_dotenv
from utils.data_utils import load_evaluation_data

# Third‑party
from rich import print as rprint
from sentence_transformers import SentenceTransformer  # lazy‑loaded by embedder

# First‑party (repository) -----------------------------------------------------
from utils.load_config import init_logger, load_config
from utils.training_utils import resolve_device, set_seed
from retrieval.embedder import EmbeddingCompressor
from retrieval.retriever import retrieve_top_k
from evaluation.retrieval_metrics import evaluate_retrieval
from evaluation.generation_metrics import (
    evaluate_generation_bootstrap as eval_generation,
)
from generation.generator import RAGGenerator
from models.variational_autoencoder import VariationalAutoencoder
from models.denoising_autoencoder import DenoisingAutoencoder
from models.contrastive_autoencoder import ContrastiveAutoencoder

# ---------------------------------------------------------------------------
# Helper factories
# ---------------------------------------------------------------------------

def _load_autoencoder(
    cfg_models: Dict[str, Dict[str, Any]],
    ae_type: str,
    device: str,
) -> Optional[torch.nn.Module]:
    """Instantiate and load the requested auto‑encoder.

    Args:
        cfg_models: Dict extracted from YAML under `models`.
        ae_type:    "vae", "dae", "contrastive" or "none".
        device:     "cpu" | "cuda".

    Returns:
        A `torch.nn.Module` in eval mode, or *None* if `ae_type == "none"`.
    """

    if ae_type == "none":
        return None

    if ae_type not in cfg_models:
        raise ValueError(
            f"[CONFIG] Auto‑encoder '{ae_type}' not found under 'models' in config."
        )

    mcfg = cfg_models[ae_type]
    input_dim = mcfg.get("input_dim", 384)
    latent_dim = mcfg.get("latent_dim", 64)
    hidden_dim = mcfg.get("hidden_dim", 512)
    checkpoint = mcfg.get("checkpoint")

    if ae_type == "vae":
        model: torch.nn.Module = VariationalAutoencoder(  # type: ignore[assignment]
            input_dim, latent_dim, hidden_dim
        )
    elif ae_type == "dae":
        model = DenoisingAutoencoder(input_dim, latent_dim, hidden_dim)
    elif ae_type == "contrastive":
        model = ContrastiveAutoencoder(input_dim, latent_dim, hidden_dim)
    else:
        raise RuntimeError("Unreachable branch – ae_type already validated.")

    if checkpoint and Path(checkpoint).exists():
        model.load_state_dict(torch.load(checkpoint, map_location=device))
    else:
        raise FileNotFoundError(f"Checkpoint for '{ae_type}' not found: {checkpoint}")

    return model.to(device).eval()


# ---------------------------------------------------------------------------
# Pipeline steps
# ---------------------------------------------------------------------------

def _encode_corpus(
    compressor: EmbeddingCompressor,
    texts: Sequence[str],
    compress: bool = True,
) -> torch.Tensor:
    """Return document embeddings as `[N × D]` float32 CPU tensor."""

    return compressor.encode_text(list(texts), compress=compress)


def _retrieve_documents(
    query_emb: torch.Tensor,
    doc_emb: torch.Tensor,
    corpus: Sequence[str],
    retr_cfg: Dict[str, Any],
) -> Tuple[List[str], List[float]]:
    """Retrieve top‑k docs and similarity scores for a *single* query."""

    top_k = retr_cfg.get("top_k", 10)
    metric = retr_cfg.get("similarity_metric", "cosine")
    results = retrieve_top_k(query_emb, doc_emb, list(corpus), k=top_k, metric=metric)
    docs, scores = zip(*results)
    return list(docs), list(scores)


def _evaluate_retrieval(
    retrieved: Sequence[Sequence[str]],
    relevant: Sequence[Sequence[str]] | Sequence[str],
    metrics: List[str],
) -> Dict[str, Dict[str, float]]:
    """Wrapper around `evaluate_retrieval` with sensible defaults."""

    return evaluate_retrieval(retrieved, relevant, metrics=metrics)


# ---------------------------------------------------------------------------
# Core runner
# ---------------------------------------------------------------------------

class PipelineRunner:  # noqa: D101 – simple orchestrator
    def __init__(self, cfg: Dict[str, Any], ae_type: str, logger):
        self.cfg = cfg
        self.ae_type = ae_type
        self.logger = logger

        device = resolve_device(cfg.get("training", {}).get("device"))
        self.device = device
        self.logger.main.info("Device resolved → %s", device)

        # Auto‑encoder & compressor ------------------------------------------------
        ae_model = _load_autoencoder(cfg["models"], ae_type, device)
        self.compressor = EmbeddingCompressor(
            base_model_name=cfg["embedding_model"]["name"],
            autoencoder=ae_model,
            device=device,
        )
        self.logger.main.info("Compressor ready (AE = %s)", ae_type)

        # Retriever ---------------------------------------------------------------
        self.retr_cfg = cfg.get("retrieval", {})

        # Generator ---------------------------------------------------------------
        self.generator = RAGGenerator(cfg)

    # ---------------------------------------------------------------------
    def process(
        self,
        queries: Sequence[str],
        corpus: Sequence[str],
        relevant_docs: Optional[Sequence[str]] = None,
        generate: bool = False,
    ) -> None:
        """Run encode → retrieve → generate → evaluate for *all* queries."""

        self.logger.main.info("Running pipeline: |queries|=%d |docs|=%d", len(queries), len(corpus))
        doc_embeddings = _encode_corpus(self.compressor, corpus, compress=True)
        query_embeddings = _encode_corpus(self.compressor, queries, compress=True)

        all_retrieved: List[Sequence[str]] = []
        answers: List[str] = []

        for idx, (q, q_emb) in enumerate(zip(queries, query_embeddings)):
            docs_k, _ = _retrieve_documents(q_emb, doc_embeddings, corpus, self.retr_cfg)
            all_retrieved.append(docs_k)
            if generate:
                ans = self.generator.generate(q, docs_k)
                answers.append(ans)
                self.logger.main.debug("[%d] Q: %s | A: %s", idx, q, ans[:60] + "…")

        # ----------------------------------------------------------------- EVAL
        eval_cfg = self.cfg.get("evaluation", {})
        if relevant_docs:
            ret_metrics = _evaluate_retrieval(
                all_retrieved,
                relevant_docs,
                metrics=eval_cfg.get("retrieval_metrics", ["Recall@5"]),
            )
            rprint("[bold magenta]\n[Retrieval evaluation]\n[/]")
            for k, v in ret_metrics.items():
                rprint(f"{k}: {v['mean']:.4f} ± {v['std']:.4f}")

        if generate and relevant_docs and len(queries) >= 100:
            # If the user provided refs with ≥100 samples we can bootstrap.
            gen_metrics = eval_generation(
                references=list(relevant_docs),
                candidates=answers,
                metrics=eval_cfg.get("generation_metrics", ["ROUGE-L"]),
            )
            rprint("[bold magenta]\n[Generation evaluation]\n[/]")
            for m, d in gen_metrics.items():
                rprint(f"{m}: {d['mean']:.2f} (CI 95%: {d['ci_lower']:.2f}–{d['ci_upper']:.2f})")


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def _parse_args() -> argparse.Namespace:  # noqa: D401
    """Return command‑line arguments."""

    pre_parser = argparse.ArgumentParser(add_help=False)
    pre_parser.add_argument("--config", default="./config/config.yaml")
    known, _ = pre_parser.parse_known_args(sys.argv[1:])

    cfg = load_config(known.config)
    valid_ae = list(cfg.get("models", {}).keys()) + ["none", "all"]

    parser = argparse.ArgumentParser(description="Run RAG‑AE experimental pipeline")
    parser.add_argument("--config", default="./config/config.yaml", help="Path to YAML config")
    parser.add_argument("--ae_type", default="vae", choices=valid_ae, help="Select auto‑encoder variant")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")

    parser.add_argument("--dataset", choices=["squad", "uda"], default="squad",
                    help="Dataset for evaluation (SQuAD or UDA)")
    parser.add_argument("--max_samples", type=int, default=200,
                        help="Maximum number of queries to use")
    parser.add_argument("--benchmark", action="store_true",
                        help="Compare against BM25, DPR, SBERT, AE...")
    parser.add_argument("--generate", action="store_true", help="Run generation step (RAG)")

    return parser.parse_args()


# ---------------------------------------------------------------------------
# Entry‑point
# ---------------------------------------------------------------------------

def main() -> None:  # noqa: D401 – standard script
    args = _parse_args()

    cfg = load_config(args.config)
    log = init_logger(cfg.get("logging", {}))
    set_seed(args.seed, cfg.get("training", {}).get("deterministic", False), logger=log.train)
    load_dotenv()

    ae_variants = (
        [args.ae_type]
        if args.ae_type != "all"
        else [k for k in cfg.get("models", {}).keys() if k in {"vae", "dae", "contrastive"}]
    )

    # --------------------------------------------------------------------- Toy corpus (replace with real dataset) --
    queries, corpus, relevant = load_evaluation_data(args.dataset, max_samples=args.max_samples)

    # --------------------------------------------------------------------- Run each variant
    for ae in ae_variants:
        rprint(f"\n[bold cyan]==== PIPELINE ({ae.upper()}) ====\n[/]")
        runner = PipelineRunner(cfg, ae, log)
        runner.process(queries, corpus, relevant_docs=relevant, generate=args.generate)


if __name__ == "__main__":
    main()

"""

models/base_autoencoder.py
"""
import torch
import torch.nn as nn
from abc import ABC, abstractmethod

class BaseAutoencoder(nn.Module, ABC):
    def __init__(self, input_dim: int, latent_dim: int):
        super(BaseAutoencoder, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim

    @abstractmethod
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        pass

    @abstractmethod
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        pass

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encode(x)
        return self.decode(z)

"""

models/contrastive_autoencoder.py
"""
# /models/contrastive_autoencoder.py
import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder

class ContrastiveAutoencoder(BaseAutoencoder):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super(ContrastiveAutoencoder, self).__init__(input_dim, latent_dim)

        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # Useful if input vectors are normalized
        )

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encoder(x)
        return torch.nn.functional.normalize(z, p=2, dim=-1)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)
      
    def forward(self, x: torch.Tensor):
        z = self.encode(x)
        x_recon = self.decode(z)
        return x_recon, z

"""

models/denoising_autoencoder.py
"""
# /models/denoising_autoencoder.py

import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder
import torch.nn.functional as F

class DenoisingAutoencoder(BaseAutoencoder):
    """Feed‑forward Denoising Autoencoder.

    The dataset must supply *noisy* inputs; the model learns to reconstruct the
    clean version. Use `dae_loss` (MSE) during training.
    """

    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super().__init__(input_dim, latent_dim)

        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # assume inputs ∈ [0,1]; change if different scale
        )

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        return self.encoder(x)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encode(x)
        return self.decode(z)

"""

models/variational_autoencoder.py
"""
# / models/variational_autoencoder.py
import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder

class VariationalAutoencoder(BaseAutoencoder):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super(VariationalAutoencoder, self).__init__(input_dim, latent_dim)
        
        # Encoder: proyecciones a la media y desviación estándar
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.mu_layer = nn.Linear(hidden_dim, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # Asumimos entrada normalizada (0-1)
        )

    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        h = self.encoder(x)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        return mu, logvar

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_reconstructed = self.decode(z)
        return x_reconstructed, mu, logvar

"""

requeriments.txt
"""
# Core ML & Data Processing
torch>=2.0.0
transformers>=4.38.0
sentence-transformers>=2.2.2
scikit-learn>=1.3.0
numpy>=1.24.0
pandas>=2.0.0

datasets>=2.19.0
scipy>=1.10.0
openai>=1.14.0
pyserini>=0.21


# Visualization & Analysis
matplotlib>=3.7.0
seaborn>=0.12.0

# Evaluation Metrics (Efficient and Torch-Compatible)
rouge-score >=0.1.2
sacrebleu>=2.5.1

# Configuration & Utilities
pyyaml>=6.0
python-dotenv>=1.0.0

# Optional CLI Enhancements
rich>=13.0.0

pytest 
"""

retrieval/base.py
"""
from __future__ import annotations
from typing import Protocol, Sequence, Tuple, List

class BaseRetriever(Protocol):
    """Interface for all first-stage retrievers."""
    def build_index(self, corpus: Sequence[str]) -> None: ...
    def retrieve(self, query: str, k: int) -> List[Tuple[str, float]]: ...

"""

retrieval/bm25.py
"""
from pyserini.search import SimpleSearcher            # pip install pyserini>=0.21
from retrieval.base import BaseRetriever
import tempfile, os, json

class BM25Retriever(BaseRetriever):
    def __init__(self, bm25_k1: float = 0.9, b: float = 0.4):
        self.k1, self.b = bm25_k1, b
        self._searcher = None
        self._tmpdir   = tempfile.mkdtemp()

    def build_index(self, corpus):
        # 1) escribir cada doc en un fichero JSONL (id + text)
        tmp_jsonl = os.path.join(self._tmpdir, "docs.jsonl")
        with open(tmp_jsonl, "w", encoding="utf-8") as f:
            for i, doc in enumerate(corpus):
                f.write(json.dumps({"id": str(i), "contents": doc}) + "\n")

        # 2) invocar indexador Lucene
        from pyserini.index import build_index
        build_index(tmp_jsonl, self._tmpdir, overwrite=True)

        # 3) abrir buscador Lucene
        self._searcher = SimpleSearcher(self._tmpdir)
        self._searcher.set_bm25(self.k1, self.b)

    def retrieve(self, query, k):
        hits = self._searcher.search(query, k)
        return [(h.raw, float(h.score)) for h in hits]

"""

retrieval/dpr.py
"""
from retrieval.base import BaseRetriever
from sentence_transformers import SentenceTransformer
import faiss, numpy as np

class DPRRetriever(BaseRetriever):
    """Dense Passage Retrieval (dual-encoder)."""
    def __init__(self,
                 q_model: str = "facebook-dpr-question_encoder-single-nq-base",
                 p_model: str = "facebook-dpr-ctx_encoder-single-nq-base",
                 device: str | None = None):
        self.q_encoder = SentenceTransformer(q_model, device=device)
        self.p_encoder = SentenceTransformer(p_model, device=device)
        self.index = None
        self.docs  = []

    def build_index(self, corpus):
        self.docs = list(corpus)
        emb = self.p_encoder.encode(self.docs,
                                    batch_size=64,
                                    convert_to_numpy=True,
                                    normalize_embeddings=True)
        d = emb.shape[1]
        self.index = faiss.IndexHNSWFlat(d, 32)
        self.index.hnsw.efConstruction = 200
        self.index.add(emb.astype("float32"))

    def retrieve(self, query, k):
        q_emb = self.q_encoder.encode([query],
                                      convert_to_numpy=True,
                                      normalize_embeddings=True)
        dist, idx = self.index.search(q_emb.astype("float32"), k)
        return [(self.docs[i], float(-dist[0][j])) for j, i in enumerate(idx[0])]

"""

retrieval/embedder.py
"""
# retrieval/embedder.py

from sentence_transformers import SentenceTransformer
import torch


class EmbeddingCompressor:
    def __init__(
        self,
        base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        autoencoder: torch.nn.Module = None,
        device: str = None
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # Load pretrained SBERT model (includes pooling + normalization)
        self.model = SentenceTransformer(base_model_name, device=self.device)

        # Optional autoencoder for compression (VAE, DAE, etc.)
        self.autoencoder = autoencoder.to(self.device) if autoencoder else None
        if self.autoencoder:
            self.autoencoder.eval()

    def encode_text(self, texts: list[str], compress: bool = True) -> torch.Tensor:
        """Returns SBERT embeddings, optionally compressed with an autoencoder.

        Args:
            texts: List of input strings to encode.
            compress: Whether to apply the autoencoder (if available).

        Returns:
            A float32 tensor [N × D] on CPU.
        """
        with torch.no_grad():
            embeddings = self.model.encode(
                texts,
                batch_size=64,
                convert_to_tensor=True,
                normalize_embeddings=True
            ).to(self.device)

            if self.autoencoder and compress:
                encoded = self.autoencoder.encode(embeddings)
                if isinstance(encoded, tuple):  # VAE returns (mu, logvar)
                    encoded = encoded[0]        # use mean as latent code
                return encoded.cpu()

            return embeddings.cpu()

"""

retrieval/retriever.py
"""
# /retrieval/retriever.py

from __future__ import annotations

import torch
import torch.nn.functional as F
import numpy as np
from sklearn.covariance import EmpiricalCovariance
from typing import List, Tuple, Literal

SimilarityMetric = Literal["cosine", "euclidean", "mahalanobis"]

###############################################################################
#  MATRICES DE SIMILITUD                                                      #
###############################################################################

def cosine_similarity_matrix(
    query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor
) -> np.ndarray:
    """Matriz [Q × N] con similitud coseno."""
    q_norm = F.normalize(query_embeddings, p=2, dim=1)
    d_norm = F.normalize(doc_embeddings, p=2, dim=1)
    sim = torch.mm(q_norm, d_norm.T)  # [Q, N]
    return sim.cpu().numpy()


def euclidean_similarity_matrix(
    query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor
) -> np.ndarray:
    """Matriz [Q × N] con "+similitud" inversa a la distancia euclídea."""
    q = query_embeddings.unsqueeze(1)  # [Q, 1, D]
    d = doc_embeddings.unsqueeze(0)  # [1, N, D]
    dist = torch.norm(q - d, dim=2)  # [Q, N]
    return (-dist).cpu().numpy()  # valores altos = más similares


def mahalanobis_similarity_matrix(
    query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor, eps: float = 1e-6
) -> np.ndarray:
    """Matriz [Q × N] con similitud inversa de la distancia de Mahalanobis.

    *Se ajusta la matriz de covarianza exclusivamente sobre los documentos* para
    preservar la simetría deseada en el espacio de recuperación.
    """
    # -- Datos a NumPy -------------------------------------------------------
    d_np: np.ndarray = doc_embeddings.cpu().numpy()
    q_np: np.ndarray = query_embeddings.cpu().numpy()

    # -- Precisión (inversa de la covarianza) -------------------------------
    # EmpiricalCovariance añade regularización de Ledoit‑Wolf si el sistema lo
    # necesita, pero incluimos un término eps para garantizar invertibilidad.
    emp = EmpiricalCovariance(assume_centered=False).fit(d_np)
    VI: np.ndarray = emp.precision_ + eps * np.eye(d_np.shape[1], dtype=np.float64)

    # -- Distancias ----------------------------------------------------------
    diff = q_np[:, None, :] - d_np[None, :, :]  # [Q, N, D]
    # einsum: (q n d, d d, q n d) → (q n)
    dist = np.einsum("qnd,dd,qnd->qn", diff, VI, diff, optimize=True)

    # -- Convertir a "similitud" (negativo de la distancia) -----------------
    sim = -dist  # altos valores ⇒ mayor similitud
    return sim.astype(np.float32)


###############################################################################
#  FRONT‑END DE RECUPERACIÓN                                                  #
###############################################################################

def compute_similarity(
    query_embeddings: torch.Tensor,
    doc_embeddings: torch.Tensor,
    metric: SimilarityMetric = "cosine",
) -> np.ndarray:
    """Devuelve matriz [Q × N] según la métrica solicitada y valida formas."""

    if query_embeddings.dim() == 1:
        query_embeddings = query_embeddings.unsqueeze(0)
    if doc_embeddings.dim() == 1:
        doc_embeddings = doc_embeddings.unsqueeze(0)

    funcs = {
        "cosine": cosine_similarity_matrix,
        "euclidean": euclidean_similarity_matrix,
        "mahalanobis": mahalanobis_similarity_matrix,
    }

    if metric not in funcs:
        raise ValueError(f"Métrica de similitud '{metric}' no soportada.")

    sim = funcs[metric](query_embeddings, doc_embeddings)

    # -------- Validación de forma -----------------------------------------
    q, d = query_embeddings.shape[0], doc_embeddings.shape[0]
    if sim.shape != (q, d):
        raise RuntimeError(
            f"Shape mismatch: expected ({q}, {d}) got {sim.shape} for metric '{metric}'."
        )
    return sim


def retrieve_top_k(
    query_embedding: torch.Tensor,
    doc_embeddings: torch.Tensor,
    doc_texts: List[str],
    k: int = 5,
    metric: SimilarityMetric = "cosine",
) -> List[Tuple[str, float]]:
    """Recupera los *k* documentos con mayor similitud."""

    sim_scores = compute_similarity(query_embedding, doc_embeddings, metric)  # [Q, N]
    # Suponemos única consulta (Q = 1) para esta utilidad.
    if sim_scores.shape[0] != 1:
        raise ValueError("Esta función está pensada para una única consulta.")

    top_idx = sim_scores[0].argsort()[::-1][:k]
    return [(doc_texts[i], float(sim_scores[0, i])) for i in top_idx]

"""

save_snapshot.sh
"""
#!/usr/bin/env bash
#
# save_snapshot.sh  –  Dump a folder’s tree plus every file’s contents
# Usage:
#   ./save_snapshot.sh [TARGET_DIR] [OUTPUT_TXT]
# Defaults:
#   TARGET_DIR="."                  (current directory)
#   OUTPUT_TXT="snapshot.txt"       (created/overwritten)

set -euo pipefail

###############################################################################
# 1. Input handling
###############################################################################
TARGET_DIR="${1:-.}"
OUTPUT_TXT="${2:-snapshot.txt}"

# Verify prerequisites
command -v tree >/dev/null 2>&1 || {
  printf 'Error: "tree" command not found. Please install it first.\n' >&2; exit 1; }

# Avoid accidental overwrite of important files
if [[ -e "$OUTPUT_TXT" && ! -w "$OUTPUT_TXT" ]]; then
  printf 'Error: Output file "%s" is not writable.\n' "$OUTPUT_TXT" >&2
  exit 1
fi

###############################################################################
# 2. Capture the directory tree
###############################################################################
# Truncate/overwrite existing output
: > "$OUTPUT_TXT"

printf '### Directory tree for: %s\n\n' "$TARGET_DIR" >> "$OUTPUT_TXT"
tree -F -I '__pycache__' "$TARGET_DIR" >> "$OUTPUT_TXT" || {
  printf 'Warning: Unable to generate tree for "%s".\n' "$TARGET_DIR" >&2; }

printf '\n\n### File contents\n\n' >> "$OUTPUT_TXT"

###############################################################################
# 3. Append each file (relative path + contents)
###############################################################################
# Absolute path to output for comparison
ABS_OUTPUT="$(realpath "$OUTPUT_TXT")"

# Exclude hidden files, __pycache__, and specific extensions
find "$TARGET_DIR" -type f \
  ! -path '*/.*/*' \
  ! -name '.*' \
  ! -path '*/__pycache__/*' \
  -print0 | sort -z |
while IFS= read -r -d '' FILE
do
  # Resolve absolute path of the file
  ABS_FILE="$(realpath "$FILE")"

  # Skip the output file itself
  if [[ "$ABS_FILE" == "$ABS_OUTPUT" ]]; then
    continue
  fi

  # Skip files with undesired extensions
  case "$FILE" in
    *.pt|*.pth|*.ipynb|*.log|*.tmp|*.bak|*.swp|*.zip|*.tar|*.gz|*.rar|*.DS_Store|*.png|*.jpg|*.jpeg|*.gif|*.bmp|*.tiff|*.ico|*.webp|*.svg|*.mp4|*.avi|*.mkv|*.mov|*.wmv|*.flv|*.mp3|*.wav|*.ogg|*.code-workspace|*.vscode|*.idea|*.git|*.svn|*.hg)
      continue
      ;;
  esac

  # Remove leading base path for relative display
  REL_PATH="${FILE#$TARGET_DIR/}"

  # Header
  printf '%s\n' "$REL_PATH" >> "$OUTPUT_TXT"
  printf '"""\n' >> "$OUTPUT_TXT"

  # File contents
  if ! cat "$FILE" >> "$OUTPUT_TXT" 2>/dev/null; then
    printf '[[[ Error reading file ]]]\n' >> "$OUTPUT_TXT"
    printf 'Warning: Could not read "%s".\n' "$REL_PATH" >&2
  fi

  # Footer
  printf '\n"""\n\n' >> "$OUTPUT_TXT"
done

printf 'Snapshot saved to: %s\n' "$OUTPUT_TXT"

"""

style_guide.md
"""
# Project-wide Code Style Guide (English)

> **Scope**  All Python modules, notebooks, shell scripts and configuration files located under the `tfm` repository.  New code **must** comply immediately.  Legacy code should be progressively refactored.

---

## 1  General Principles

| Principle                         | Rationale                                                        |
| --------------------------------- | ---------------------------------------------------------------- |
| **Consistency over cleverness**   | Readability and maintenance outweigh micro-optimisations.        |
| **Explicit > implicit**           | Follow *The Zen of Python*. Avoid hidden state and side effects. |
| **Fail fast, fail loud**          | Raise specific exceptions early; log context-rich messages.      |
| **Pure functions where possible** | Functions that depend only on arguments are easier to test.      |

---

## 2  File & Folder Layout

* Package modules use **snake\_case** filenames: `contrastive_autoencoder.py`.
* Public executables go under `/scripts` with a short shebang and CLI (`argparse`).
* Unit tests mirror the package tree under `/tests`.
* Keep data or model artefacts out of Git; place under `/data` or `/models/checkpoints` and add to `.gitignore`.

---

## 3  Imports

```
# 1. Standard library
import os
import json

# 2. Third-party
import numpy as np
import torch

# 3. First-party (this repo)
from utils.load_config import load_config
```

* Use **absolute imports** inside the package.
* Never use `from module import *`.
* Group imports and separate blocks with one blank line.

---

## 4  Naming Conventions

* **snake\_case** for variables, functions and methods.
* **PascalCase** for classes and `Enum` members.
* **UPPER\_SNAKE\_CASE** for module-level constants.
* Avoid Spanish identifiers; prefer descriptive English (`embedding_dim`, not `dim_emb`).

---

## 5  Docstrings & Comments

* **Every** public module, function, class and method **must** have a docstring in **English** using the **Google style**.
* Keep inline comments short; they explain *why*, not *what*.
* TODO/FIXME tags must include assignee or ticket reference.

```python
def recall_at_k(retrieved: Sequence[str], relevant: Sequence[str], k: int) -> float:
    """Return Recall@k.

    Args:
        retrieved: Ordered list of retrieved IDs.
        relevant: Set or list of relevant IDs.
        k: Cut-off rank.

    Returns:
        Fraction of relevant items found in the top-k.
    """
```

---

## 6  Type Annotations & Runtime Checks

* Use **PEP 484** type hints everywhere (functions, class attributes).
* **All functions must declare input and output types explicitly.**
* Validate external inputs with `assert` or explicit `if ... raise ValueError`.
* Run **mypy** in *strict* mode as a CI step.

---

## 7  Logging

* Initialise loggers via `utils.load_config.init_logger`.
* Use module-level loggers: `logger = logging.getLogger(__name__)`.
* Logging levels: `debug` (dev insights), `info` (milestones), `warning` (recoverable), `error` (cannot proceed), `critical` (program abort).
* Never hide exceptions; use `logger.exception` to preserve traceback.

---

## 8  Error Handling

* Prefer built-in exceptions (`ValueError`, `TypeError`) unless a custom domain error clarifies intent.
* Enrich messages with variable values.
* Do **not** swallow exceptions silently.

---

## 9  Configuration Management

* All hyper-parameters live in YAML under `/config`.
* Load configs **only** through `utils.load_config.load_config`.
* Functions accept an explicit `config: dict` argument instead of reading files internally, unless the function’s **sole purpose** is configuration loading.

---

## 10  CLI & Scripts

* Use `argparse` with long option names (`--batch_size`).
* Provide `--config` flag pointing to a YAML; CLI flags override YAML.
* Scripts must be import-safe (`if __name__ == "__main__":`).

---

## 11  Testing & Quality Gates

* Write **pytest** unit tests covering critical paths.
* Minimum coverage threshold: **80 %**.
* Run `black`, `ruff`, `isort`, `mypy` and tests in CI before merge.

---

## 12  Formatting Tools

| Tool      | Version | Role                                 |
| --------- | ------- | ------------------------------------ |
| **black** | 24.3+   | Code formatting (line length 88).    |
| **ruff**  | 0.3+    | Linter (select = "ALL", ignore = …). |
| **isort** | 5+      | Import ordering (profile = "black"). |

---

## 13  Dependencies & Virtual Envs

* Pin versions in `requirements.txt` (prod) and `requirements-dev.txt` (lint/test).
* Use **conda** or **venv**; never rely on system Python.
* Document GPU/CPU requirements in `README.md`.

---

## 14  Internationalisation

* **All code, comments and docstrings must be in English.**  Spanish is reserved for external documentation or academic writing outside the repository.

---

## 15  Example Module Skeleton

```python
"""Contrastive Autoencoder model.

Implements an encoder–decoder architecture trained with triplet loss.
"""
from __future__ import annotations

import torch
from torch import nn, Tensor

class ContrastiveAutoencoder(nn.Module):
    """Linear contrastive autoencoder with L2-normalised latent vectors."""

    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim),
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
        )

    def encode(self, x: Tensor) -> Tensor:  # noqa: D401
        """Return L2-normalised latent representation."""
        return torch.nn.functional.normalize(self.encoder(x), p=2, dim=-1)

    def decode(self, z: Tensor) -> Tensor:
        return self.decoder(z)

    def forward(self, x: Tensor) -> Tensor:    # type: ignore[override]
        return self.decode(self.encode(x))
```

---

## 16  Migration Plan for Legacy Code

1. **Phase 1**  — New code follows this guide immediately.
2. **Phase 2**  — Touch legacy modules only when editing; refactor headers, identifiers, comments to English.
3. **Phase 3**  — Run automated formatters; fix mypy errors; localised refactors.

---

*End of style guide.*

"""

###############################################################################
#  VAE                                                                        #
###############################################################################

import torch
import torch.nn.functional as F

def vae_loss(
    x_reconstructed: torch.Tensor,
    x_target: torch.Tensor,
    mu: torch.Tensor,
    logvar: torch.Tensor,
    *,
    mse_reduction: str = "mean",   # "mean" or "sum"
    beta: float = 1.0,             # β-VAE (β=1 → classic VAE)
) -> torch.Tensor:
    """VAE loss = reconstruction + β·KL  (KL normalized by batch).

    Args:
        x_reconstructed: output from the decoder  ― shape [B, D]
        x_target:        original embeddings ― shape [B, D]
        mu, logvar:      parameters of the latent distribution ― shape [B, Z]
        mse_reduction:   "mean" (recommended) or "sum"
        beta:            weight of the KL term (β-VAE)
    """
    # ── 1. reconstruction error ─────────────────────────────────────────
    recon = F.mse_loss(x_reconstructed, x_target, reduction=mse_reduction)

    # ── 2. KL (normalized) ───────────────────────────────────────────────
    #   KL(q(z|x) || N(0,1))  =  -½ Σ_i (1 + logσ²_i − μ²_i − σ²_i)
    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()  # ← .mean() ≈ /B/Z

    return recon + beta * kl

###############################################################################
#  DAE                                                                        #
###############################################################################

def dae_loss(
    x_reconstructed: torch.Tensor,
    x_clean: torch.Tensor,
    reduction: str = "mean",
) -> torch.Tensor:
    """Mean‑squared error for Denoising Auto‑Encoders."""
    return F.mse_loss(x_reconstructed, x_clean, reduction=reduction)

###############################################################################
#  CONTRASTIVE                                                                #
###############################################################################

def contrastive_loss(
    z_q: torch.Tensor,
    z_pos: torch.Tensor,
    *,
    margin: float = 0.2,
    hard_negatives: bool = True,
) -> torch.Tensor:
    """Triplet loss with negative selection within the batch.

    If `hard_negatives` is True, uses the closest negative; otherwise,
    permutes `z_pos` to obtain a random negative.
    """
    z_q = F.normalize(z_q, p=2, dim=1)
    z_pos = F.normalize(z_pos, p=2, dim=1)

    if hard_negatives:
        dist_mat = torch.cdist(z_q, z_pos, p=2)
        mask = torch.eye(dist_mat.size(0), dtype=torch.bool, device=z_q.device)
        dist_mat = dist_mat.masked_fill(mask, float("inf"))  # ← corrected
        neg_dist, _ = dist_mat.min(dim=1)

    else:
        idx = torch.randperm(z_pos.size(0), device=z_pos.device)
        neg_dist = torch.norm(z_q - z_pos[idx], dim=1)

    pos_dist = torch.norm(z_q - z_pos, dim=1)
    return F.relu(pos_dist - neg_dist + margin).mean()

"""

training/train_cae.py
"""
# training/train_cae.py ― Contrastive Auto-Encoder with negative mining and validation

from __future__ import annotations
import argparse, os, math
from typing import Optional

import torch
from torch.utils.data import DataLoader
from torch.nn.utils import clip_grad_norm_

from data.torch_datasets import EmbeddingTripletDataset
from models.contrastive_autoencoder import ContrastiveAutoencoder
from training.loss_functions import contrastive_loss        # in-batch mining
from utils.load_config import load_config, init_logger
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import prepare_datasets, split_dataset
from dotenv import load_dotenv

# --------------------------------------------------------------------------- #
#  AUX                                                                       #
# --------------------------------------------------------------------------- #

def _build_optimizer(model: torch.nn.Module, lr: float, weight_decay: float) -> torch.optim.Optimizer:
    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

def _build_scheduler(optim: torch.optim.Optimizer, patience: int, factor: float = 0.5):
    # Reduce LR if val_loss does not improve for `patience` consecutive epochs
    return torch.optim.lr_scheduler.ReduceLROnPlateau(
        optim, mode="min", factor=factor, patience=max(1, patience // 2)
    )

# --------------------------------------------------------------------------- #
#  TRAINING LOOP                                                             #
# --------------------------------------------------------------------------- #

def train_cae(
    *,
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    logger,
    hard_negatives: bool = True,
    margin: float = 0.2,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    min_delta: float = 0.003,                   # 0.3% relative improvement
    weight_decay: float = 1e-4,
    clip_grad_norm: float = 1.0,                # 0 = disable
    device: Optional[str] = None,
) -> None:

    device = device or resolve_device()
    log = logger.train if hasattr(logger, "train") else logger

    log.info("CAE | device=%s | hard_negatives=%s | margin=%.3f", device, hard_negatives, margin)

    # ---------------- Dataset ---------------------------
    full_ds = EmbeddingTripletDataset(dataset_path)
    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=True)
    dl_val   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)

    # ---------------- Model & Opt -----------------------
    model = ContrastiveAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = _build_optimizer(model, lr, weight_decay)
    scheduler = _build_scheduler(optim, patience or 4)

    best_val, epochs_no_improve = math.inf, 0

    # Triplet loss native
    triplet_fn = torch.nn.TripletMarginLoss(margin=margin, p=2)

    for epoch in range(1, epochs + 1):
        # ---------------- Train -------------------------
        model.train(); running = 0.0
        for batch in dl_train:
            z_q  = model.encode(batch["q"].to(device))
            z_p  = model.encode(batch["p"].to(device))
            z_n  = model.encode(batch["n"].to(device))

            if hard_negatives:
                loss = contrastive_loss(z_q, z_p, margin=margin, hard_negatives=True)
            else:
                loss = triplet_fn(z_q, z_p, z_n)

            optim.zero_grad()
            loss.backward()
            if clip_grad_norm > 0:
                clip_grad_norm_(model.parameters(), clip_grad_norm)
            optim.step()
            running += loss.item() * z_q.size(0)

        train_loss = running / len(train_ds)

        # ---------------- Validation --------------------
        model.eval(); val_running = 0.0
        with torch.no_grad():
            for batch in dl_val:
                z_q  = model.encode(batch["q"].to(device))
                z_p  = model.encode(batch["p"].to(device))
                z_n  = model.encode(batch["n"].to(device))

                if hard_negatives:
                    vloss = contrastive_loss(z_q, z_p, margin=margin, hard_negatives=True)
                else:
                    vloss = triplet_fn(z_q, z_p, z_n)

                val_running += vloss.item() * z_q.size(0)
        val_loss = val_running / len(val_ds)

        log.info("[Epoch %02d/%d] train=%.6f | val=%.6f", epoch, epochs, train_loss, val_loss)
        scheduler.step(val_loss)

        # ---------------- Early stop --------------------
        rel_improve = (best_val - val_loss) / best_val if best_val < math.inf else 1.0
        if rel_improve > min_delta:
            best_val, epochs_no_improve = val_loss, 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  -> New best val_loss. Checkpoint saved at {model_save_path}")
            logger.train.info("New best val_loss: %.6f → checkpoint %s", best_val, model_save_path)
        else:
            epochs_no_improve += 1
            if patience and epochs_no_improve >= patience:
                print("[EARLY STOP] No improvement in validation.")
                logger.train.info("[EARLY STOP] No improvement in validation.")
                break

    print(f"[DONE] Best val_loss = {best_val:.6f}")
    logger.main.info("[DONE] Best val_loss = %.6f", best_val)
    logger.main.info("")

# --------------------------------------------------------------------------- #
#  CLI                                                                       #
# --------------------------------------------------------------------------- #

if __name__ == "__main__":
    load_dotenv()

    p = argparse.ArgumentParser(description="Train Contrastive Auto-Encoder (CAE)")
    p.add_argument("--config", default="./config/config.yaml")
    p.add_argument("--dataset", choices=["uda", "squad"], help="Override YAML dataset")
    p.add_argument("--epochs",  type=int)
    p.add_argument("--batch_size", type=int)
    p.add_argument("--lr",      type=float)
    p.add_argument("--weight_decay", type=float, default=1e-4)
    p.add_argument("--clip_grad", type=float, default=1.0)
    p.add_argument("--margin",  type=float, default=0.2)
    p.add_argument("--val_split", type=float, default=0.1)
    p.add_argument("--patience", type=int, default=5)
    p.add_argument("--no-hard-negatives", action="store_true")
    p.add_argument("--save_path")
    args = p.parse_args()

    # ---------- Config & logging -----------------------------------------
    cfg       = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg["models"]["contrastive"]
    log       = init_logger(cfg["logging"])

    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False), logger=log.main)

    # ---------- Dataset ---------------------------------------------------
    ds_path = prepare_datasets(cfg, variant="cae", dataset_override=args.dataset)

    # ------------- model save path --------------
    checkpoints_dir = cfg["paths"]["checkpoints_dir"]
    checkpoint_file = model_cfg.get("checkpoint", "cae_text.pth")
    model_save_path = args.save_path or os.path.join(checkpoints_dir, checkpoint_file)

    # ---------- Hparams final ---------------------------------------------
    hparams = dict(
        dataset_path= ds_path,
        input_dim = model_cfg.get("input_dim", 384),
        latent_dim = model_cfg.get("latent_dim", 64),
        hidden_dim = model_cfg.get("hidden_dim", 512),
        batch_size = args.batch_size or train_cfg.get("batch_size", 256),
        epochs = args.epochs or train_cfg.get("epochs", 20),
        lr = args.lr or float(train_cfg.get("learning_rate", 1e-3)),
        weight_decay = args.weight_decay,
        clip_grad_norm = args.clip_grad,
        margin = args.margin,
        hard_negatives = not args.no_hard_negatives,
        val_split = args.val_split,
        patience = None if args.patience == 0 else args.patience,
        model_save_path = model_save_path,
        logger       = log,
    )

    train_cae(**hparams)

"""

training/train_dae.py
"""
# training/train_dae.py – Denoising Auto‑Encoder con validación y early‑stopping

from __future__ import annotations

import argparse
import os
from typing import Optional

import torch
from torch.utils.data import DataLoader

from data.torch_datasets import EmbeddingDAEDataset
from models.denoising_autoencoder import DenoisingAutoencoder
from training.loss_functions import dae_loss
from utils.load_config import load_config
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import split_dataset, prepare_datasets
from utils.load_config import init_logger
from dotenv import load_dotenv

###############################################################################
#  TRAINING FUNCTION                                                          #
###############################################################################

def train_dae(
    *,
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    logger,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    device: Optional[str] = None,
):
    """Run DAE training/validation loop."""
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Training DAE on {device} | val_split={val_split}")
    logger.main.info("")
    logger.main.info("Training DAE | device=%s", device)

    # ---------------- Dataset --------------------------
    full_ds = EmbeddingDAEDataset(dataset_path)
    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    dl_val = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)

    # ---------------- Model & Optimizer ----------------
    model = DenoisingAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = torch.optim.Adam(model.parameters(), lr=lr)

    best_val = float("inf")
    no_improve = 0

    # ---------------- Training Loop -------------------
    for epoch in range(1, epochs + 1):
        model.train()
        running = 0.0
        for batch in dl_train:
            x_noisy = batch["x"].to(device)
            x_clean = batch["y"].to(device)

            optim.zero_grad()
            x_rec = model(x_noisy)
            loss = dae_loss(x_rec, x_clean, reduction="mean")
            loss.backward()
            optim.step()
            running += loss.item() * x_noisy.size(0)

        train_loss = running / len(train_ds)

        # ---------------- Validation ------------------
        model.eval()
        with torch.no_grad():
            val_running = 0.0
            for batch in dl_val:
                x_noisy = batch["x"].to(device)
                x_clean = batch["y"].to(device)
                x_rec = model(x_noisy)
                vloss = dae_loss(x_rec, x_clean, reduction="mean")
                val_running += vloss.item() * x_noisy.size(0)
            val_loss = val_running / len(val_ds)

        print(
            f"[Epoch {epoch:02d}/{epochs}] train_loss={train_loss:.6f} | val_loss={val_loss:.6f}"
        )
        logger.train.info(
            "[Epoch %02d/%d] train=%.6f | val=%.6f", epoch, epochs, train_loss, val_loss
        )

        # ---------------- Early Stopping --------------
        if val_loss < best_val - 1e-4:
            best_val = val_loss
            no_improve = 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  -> New best val_loss. Checkpoint saved at {model_save_path}")
            logger.train.info("New best val_loss: %.6f → checkpoint %s", best_val, model_save_path)
        else:
            no_improve += 1
            if patience and no_improve >= patience:
                print("[EARLY STOP] No improvement in validation.")
                logger.train.info("[EARLY STOP] No improvement in validation.")
                break

    print(f"[DONE] Best val_loss = {best_val:.6f}")
    logger.main.info("[DONE] Best val_loss = %.6f", best_val)
    logger.main.info("")

###############################################################################
#  CLI                                                                        #
###############################################################################

if __name__ == "__main__":
    load_dotenv()

    parser = argparse.ArgumentParser(description="Train Denoising Auto‑Encoder (DAE)")
    parser.add_argument("--config", default="./config/config.yaml")
    parser.add_argument("--dataset", choices=["uda", "squad"], help="Override YAML dataset")
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--save_path")
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--val_split", type=float, default=0.1)
    parser.add_argument("--patience", type=int, default=5)
    args = parser.parse_args()

    # ---------------- Config & logging ----------------
    cfg = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg.get("models", {}).get("dae", {})
    log = init_logger(cfg["logging"])

    # ---------------- Reproducibility ----------------
    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False), logger=log.train)
    device = resolve_device(train_cfg.get("device"))

    # ---------------- Dataset prep -------------------
    dataset_path = prepare_datasets(cfg, variant="dae", dataset_override=args.dataset)

    # ------------- model save path --------------
    checkpoints_dir = cfg["paths"]["checkpoints_dir"]
    checkpoint_file = model_cfg.get("checkpoint", "dae_text.pth")
    model_save_path = args.save_path or os.path.join(checkpoints_dir, checkpoint_file)

    # ---------------- Training -----------------------
    train_dae(
        dataset_path=dataset_path,
        input_dim=model_cfg.get("input_dim", 384),
        latent_dim=model_cfg.get("latent_dim", 64),
        hidden_dim=model_cfg.get("hidden_dim", 512),
        batch_size=args.batch_size or train_cfg.get("batch_size", 256),
        epochs=args.epochs or train_cfg.get("epochs", 20),
        lr=args.lr if args.lr is not None else float(train_cfg.get("learning_rate", 1e-3)),
        model_save_path=model_save_path,
        val_split=args.val_split,
        patience=None if args.patience == 0 else args.patience,
        device=device,
        logger=log,
    )

"""

training/train_vae.py
"""
# training/train_vae.py – Variational Auto‑Encoder con validación y early‑stopping

import argparse
import os
from typing import Optional

import torch
from torch.utils.data import DataLoader

from data.torch_datasets import EmbeddingVAEDataset
from models.variational_autoencoder import VariationalAutoencoder
from training.loss_functions import vae_loss
from utils.load_config import load_config, init_logger
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import prepare_datasets
from dotenv import load_dotenv

###############################################################################
#  TRAINING LOOP                                                             #
###############################################################################

def train_vae(
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    device: Optional[str] = None,
):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Training VAE on {device} | val_split={val_split}")

    full_ds = EmbeddingVAEDataset(dataset_path)
    from utils.data_utils import split_dataset  # local import to avoid circular

    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    dl_val   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)

    model = VariationalAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = torch.optim.Adam(model.parameters(), lr=lr)

    best_val, no_improve = float("inf"), 0
    for epoch in range(1, epochs + 1):
        # ---------------- train ------------------
        model.train(); running = 0.0
        for batch in dl_train:
            x_in  = batch["input"].to(device)
            x_tar = batch["target"].to(device)
            optim.zero_grad()
            x_rec, mu, logvar = model(x_in)
            loss = vae_loss(x_rec, x_tar, mu, logvar, mse_reduction="mean")
            loss.backward(); optim.step()
            running += loss.item() * x_in.size(0)
        train_loss = running / len(train_ds)

        # ---------------- validation -------------
        model.eval(); val_running = 0.0
        with torch.no_grad():
            for batch in dl_val:
                x_in  = batch["input"].to(device)
                x_tar = batch["target"].to(device)
                x_rec, mu, logvar = model(x_in)
                vloss = vae_loss(x_rec, x_tar, mu, logvar, mse_reduction="mean")
                val_running += vloss.item() * x_in.size(0)
        val_loss = val_running / len(val_ds)

        print(f"[Epoch {epoch:02d}/{epochs}] train={train_loss:.6f} | val={val_loss:.6f}")

        if val_loss < best_val - 1e-4:
            best_val, no_improve = val_loss, 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
        else:
            no_improve += 1
            if patience and no_improve >= patience:
                print("[EARLY STOP] No improvement in validation."); break

    print(f"[DONE] best_val_loss = {best_val:.6f}")

###############################################################################
#  CLI                                                                       #
###############################################################################

if __name__ == "__main__":
    load_dotenv()

    parser = argparse.ArgumentParser(description="Train Variational Auto‑Encoder (VAE)")
    parser.add_argument("--config", default="./config/config.yaml")
    parser.add_argument("--dataset", choices=["uda", "squad"], help="Override dataset in config.yaml")
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--save_path")
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--val_split", type=float, default=0.1)
    parser.add_argument("--patience", type=int, default=5)
    args = parser.parse_args()

    # ------------- config & logging -------------
    cfg       = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg.get("models", {}).get("vae", {})
    log       = init_logger(cfg["logging"])

    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False))
    device = resolve_device(train_cfg.get("device"))

    # ------------- dataset paths -----------------
    dataset_path = prepare_datasets(cfg, variant="vae", dataset_override=args.dataset)

    # ------------- model save path --------------
    checkpoints_dir = cfg["paths"]["checkpoints_dir"]
    checkpoint_file = model_cfg.get("checkpoint", "vae_text.pth")
    model_save_path = args.save_path or os.path.join(checkpoints_dir, checkpoint_file)

    # ------------- training ----------------------
    train_vae(
        dataset_path=dataset_path,
        input_dim=model_cfg.get("input_dim", 384),
        latent_dim=model_cfg.get("latent_dim", 64),
        hidden_dim=model_cfg.get("hidden_dim", 512),
        batch_size=args.batch_size or train_cfg.get("batch_size", 256),
        epochs=args.epochs or train_cfg.get("epochs", 20),
        lr=float(args.lr) if args.lr is not None else float(train_cfg.get("learning_rate", 1e-3)),
        model_save_path=model_save_path,
        val_split=args.val_split,
        patience=None if args.patience == 0 else args.patience,
        device=device,
    )

"""

utils/data_utils.py
"""
# /utils/data_utils.py
from __future__ import annotations
import os
from typing import List, Tuple, Optional, Dict

import torch
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm
import random
from torch.utils.data import Subset

from pathlib import Path
from hashlib import sha1

def _compute_embeddings(
    texts: List[str],
    model: SentenceTransformer,
    batch_size: int = 64,
) -> torch.Tensor:
    """Devuelve un tensor CPU float32 [N × D] con los CLS-embeddings."""
    chunks: List[torch.Tensor] = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding"):
        batch = texts[i : i + batch_size]
        with torch.no_grad():
            emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
            chunks.append(torch.from_numpy(emb))
    return torch.cat(chunks, dim=0).float()

def _jaccard_sim(a: str, b: str) -> float:
    a_set = set(a.lower().split())
    b_set = set(b.lower().split())
    inter = a_set & b_set
    union = a_set | b_set
    return len(inter) / len(union) if union else 0.0

def _texts_fingerprint(texts: List[str]) -> str:
    """
    Devuelve un hash abreviado (10 hex) de la secuencia de textos.
    El orden de los textos importa, así garantizamos reproducibilidad.
    """
    h = sha1()
    for t in texts:
        h.update(t.encode("utf-8"))
    return h.hexdigest()[:10]                # 40 bits bastan para colisiones muy raras


def ensure_sbert_cache(
    texts: List[str],
    *,
    model_name: str,
    cache_dir: str = "./data/SBERT",
    batch_size: int = 64,
    force: bool = False,
) -> torch.Tensor:
    """
    Calcula (o reutiliza) los embeddings de SBERT y los persiste en disco.

    Args
    ----
    texts       : Lista de cadenas a codificar.
    model_name  : Identificador HuggingFace / Sentence-Transformers del modelo.
    cache_dir   : Carpeta donde almacenar los .pt (creada si no existe).
    batch_size  : Tamaño de lote para _compute_embeddings.
    force       : Si True, rehace el cálculo aunque exista el fichero.

    Returns
    -------
    Tensor CPU float32 de dimensión [N × D].
    """
    os.makedirs(cache_dir, exist_ok=True)

    fp        = _texts_fingerprint(texts)
    model_tag = model_name.split("/")[-1]
    fname     = f"sbert_{fp}_{model_tag}.pt"
    path      = os.path.join(cache_dir, fname)

    if not force and os.path.exists(path):
        return torch.load(path, map_location="cpu")

    print(f"[INFO] SBERT cache miss → codificando {len(texts):,} textos …")
    st_model  = SentenceTransformer(model_name)
    emb       = _compute_embeddings(texts, st_model, batch_size=batch_size)
    torch.save(emb, path)
    print(f"[OK]  SBERT embeddings guardados → {path}")
    return emb

def ensure_uda_data( # EN DESUSO
    *,
    output_dir: str = "./data/",
    max_samples: Optional[int] = None,
    base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    noise_std: float = 0.05,
    force: bool = False,
) -> None:
    """Genera (o reutiliza) los ficheros de embeddings para VAE, DAE y contraste."""
    os.makedirs(output_dir, exist_ok=True)

    vae_path = os.path.join(output_dir, "uda_vae_embeddings.pt")
    dae_path = os.path.join(output_dir, "uda_dae_embeddings.pt")
    contrastive_path = os.path.join(output_dir, "uda_contrastive_embeddings.pt")

    if (
        not force
        and os.path.exists(vae_path)
        and os.path.exists(dae_path)
        and os.path.exists(contrastive_path)
    ):
        print("[INFO] UDA embeddings ya preparados — nada que hacer.")
        return

    print("[INFO] Descargando / cargando UDA…")
    uda = load_dataset("qinchuanhui/UDA-QA", "nq")
    if max_samples is not None:
        uda = uda.select(range(min(max_samples, len(uda))))
    print(f"[INFO] UDA listo con {len(uda):,} ejemplos.")

    clean_texts: List[str] = []
    contrastive_triples: List[Tuple[str, str, str]] = []

    for i, ex in enumerate(uda["test"]): # TEMPORAL ******************************************************* CAMBIAR URGENTEMENTE
        q = ex.get("question", "").strip()
        pos = ex.get("long_answer", "").strip()
        if not q or not pos:
            continue

        neg = None
        for _ in range(10):
            j = random.randint(0, len(uda["test"]) - 1)
            if j == i:
                continue
            neg_cand = uda["test"][j].get("long_answer", "").strip()
            if not neg_cand:
                continue
            if _jaccard_sim(q, neg_cand) < 0.1:
                neg = neg_cand
                break

        if neg is None:
            continue

        clean_texts.extend((q, pos))             # query + positive answer
        contrastive_triples.append((q, pos, neg))


    print(f"[INFO] Tripletas contrastivas generadas: {len(contrastive_triples):,}")

    print(f"[INFO] Cargando SentenceTransformer '{base_model_name}' …")
    st_model = SentenceTransformer(base_model_name)

    print("[INFO] Generando embeddings VAE/DAE (positivos)…")
    target_emb = _compute_embeddings(clean_texts, st_model)

    if force or not os.path.exists(vae_path):
        torch.save({"input": target_emb, "target": target_emb.clone()}, vae_path)
        print(f"[OK]  VAE embeddings guardados → {vae_path}")

    if force or not os.path.exists(dae_path):
        input_emb = target_emb + torch.randn_like(target_emb) * noise_std
        torch.save({"input": input_emb, "target": target_emb}, dae_path)
        print(f"[OK]  DAE embeddings guardados → {dae_path}")

    if force or not os.path.exists(contrastive_path):
        print("[INFO] Generando embeddings de triples (query/pos/neg)…")
        qs, ps, ns = zip(*contrastive_triples)
        q_emb = _compute_embeddings(list(qs), st_model)
        p_emb = _compute_embeddings(list(ps), st_model)
        n_emb = _compute_embeddings(list(ns), st_model)
        torch.save({"query": q_emb, "positive": p_emb, "negative": n_emb}, contrastive_path)
        print(f"[OK]  Contrastive embeddings guardados → {contrastive_path}")

    print("[DONE] Preprocesado de UDA completo.")

def split_dataset(dataset: torch.utils.data.Dataset, val_split: float = 0.1, seed: int = 42) -> Tuple[Subset, Subset]:
    n_total = len(dataset)
    idx = list(range(n_total))
    random.Random(seed).shuffle(idx)
    n_val = int(n_total * val_split)
    val_idx = idx[:n_val]
    train_idx = idx[n_val:]
    return Subset(dataset, train_idx), Subset(dataset, val_idx)


def ensure_squad_data(
    *,
    output_dir: str = "./data",
    max_samples: Optional[int] = None,
    base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    noise_std: float = 0.05,
    include_unanswerable: bool = False,
    force: bool = False,
) -> None:
    """
    Pre-procesa **SQuAD v1/v2** y genera los tensores de entrenamiento:

        squad_vae_embeddings.pt
        squad_dae_embeddings.pt
        squad_contrastive_embeddings.pt

    Ahorra tiempo reutilizando una única caché SBERT para queries y
    contextos positivos; sólo los negativos se codifican aparte.
    """
    os.makedirs(output_dir, exist_ok=True)

    vae_path         = os.path.join(output_dir, "squad_vae_embeddings.pt")
    dae_path         = os.path.join(output_dir, "squad_dae_embeddings.pt")
    contrastive_path = os.path.join(output_dir, "squad_contrastive_embeddings.pt")

    # ------------------------------------------------------------------ #
    #  0. Early-exit si todo existe                                       #
    # ------------------------------------------------------------------ #
    if (
        not force
        and all(os.path.exists(p) for p in (vae_path, dae_path, contrastive_path))
    ):
        print("[INFO] SQuAD embeddings already prepared — nothing to do.")
        return

    # ------------------------------------------------------------------ #
    #  1. Cargar SQuAD                                                   #
    # ------------------------------------------------------------------ #
    ds_name = "squad_v2" if include_unanswerable else "squad"
    print(f"[INFO] Loading {ds_name} …")
    squad = load_dataset(ds_name, split="train")
    if max_samples is not None:
        squad = squad.select(range(min(max_samples, len(squad))))
    print(f"[INFO] SQuAD loaded with {len(squad):,} examples.")

    # ------------------------------------------------------------------ #
    #  2. Construir textos y tripletas                                   #
    # ------------------------------------------------------------------ #
    clean_texts: List[str] = []
    contrastive_triples: List[Tuple[str, str, str]] = []

    for i, ex in enumerate(squad):
        q   = ex["question"].strip()
        ctx = ex["context"].strip()
        if not q or not ctx:
            continue

        # -------- obtener negativo poco parecido ----------------------- #
        neg = None
        for _ in range(10):
            j = random.randint(0, len(squad) - 1)
            if j == i:
                continue
            neg_ctx = squad[j]["context"].strip()
            if neg_ctx and _jaccard_sim(q, neg_ctx) < 0.1:
                neg = neg_ctx
                break
        if neg is None:
            continue

        clean_texts.extend((q, ctx))               # [q1, ctx1, q2, ctx2, ...]
        contrastive_triples.append((q, ctx, neg))

    print(f"[INFO] Contrastive triples generated: {len(contrastive_triples):,}")

    # ------------------------------------------------------------------ #
    #  3. Embeddings SBERT (una sola llamada)                            #
    # ------------------------------------------------------------------ #
    print("[INFO] Fetching / caching SBERT embeddings …")
    cache_dir  = os.path.join(output_dir, "sbert_cache")
    target_emb = ensure_sbert_cache(
        clean_texts,
        model_name=base_model_name,
        cache_dir=cache_dir,
        batch_size=64,
    )  # shape [2·N, D]

    # desinterlevar: pares (q, ctx) -> índices pares / impares
    q_emb_pos = target_emb[0::2]   # consultas
    p_emb_pos = target_emb[1::2]   # contextos positivos

    # ------------------------------------------------------------------ #
    #  4. Guardar VAE / DAE                                              #
    # ------------------------------------------------------------------ #
    if force or not os.path.exists(vae_path):
        torch.save({"input": target_emb, "target": target_emb.clone()}, vae_path)
        print(f"[OK]  VAE embeddings   → {vae_path}")

    if force or not os.path.exists(dae_path):
        input_emb = target_emb + torch.randn_like(target_emb) * noise_std
        torch.save({"input": input_emb, "target": target_emb}, dae_path)
        print(f"[OK]  DAE embeddings   → {dae_path}")

    # ------------------------------------------------------------------ #
    #  5. Guardar tripletas contrastivas                                 #
    # ------------------------------------------------------------------ #
    if force or not os.path.exists(contrastive_path):
        print("[INFO] Encoding negatives and saving triplets …")
        # solo los negativos requieren consulta a la caché (puede existir)
        ns = [neg for _, _, neg in contrastive_triples]
        n_emb = ensure_sbert_cache(ns, model_name=base_model_name, cache_dir=cache_dir)

        torch.save(
            {
                "query":     q_emb_pos,
                "positive":  p_emb_pos,
                "negative":  n_emb,
            },
            contrastive_path,
        )
        print(f"[OK]  Contrastive embeddings → {contrastive_path}")

    print("[DONE] SQuAD preprocessing finished.")



def _prepare_uda(cfg: dict) -> Dict[str, str]:
    common = dict(
        output_dir="./data/UDA",
        max_samples=cfg["data"].get("max_samples"),
        base_model_name=cfg["embedding_model"]["name"],
        force=False,
    )
    ensure_uda_data(**common)
    output_dir = common["output_dir"]
    return {
        "vae": os.path.join(output_dir, cfg["models"]["vae"]["dataset_file"]),
        "dae": os.path.join(output_dir, cfg["models"]["dae"]["dataset_file"]),
        "cae": os.path.join(output_dir, cfg["models"]["contrastive"]["dataset_file"]),
    }


def _prepare_squad(cfg: dict) -> Dict[str, str]:
    data_cfg = cfg["data"]
    common = dict(
        output_dir=cfg["paths"]["data_dir"],
        max_samples=data_cfg.get("max_samples"),
        base_model_name=cfg["embedding_model"]["name"],
        noise_std=0.05,
        include_unanswerable=data_cfg.get("include_unanswerable", False),
        force=False,
    )
    ensure_squad_data(**common)
    output_dir = common["output_dir"]
    return {
        "vae": os.path.join(output_dir, cfg["models"]["vae"]["dataset_file"]),
        "dae": os.path.join(output_dir, cfg["models"]["dae"]["dataset_file"]),
        "cae": os.path.join(output_dir, cfg["models"]["contrastive"]["dataset_file"]),
    }


def prepare_datasets(
    cfg: dict,
    *,
    variant: str,
    dataset_override: Optional[str] = None,
) -> str:
    """Ensure dataset tensors exist and return path for requested variant.

    Args:
        cfg: Parsed YAML config dict (must include data and embedding_model).
        variant: One of `{"vae", "dae", "cae"}.
        dataset_override: If provided, forces `"uda" or "squad"

    Returns:
        The filesystem path to the tensor file corresponding to the *variant*.
    """
    variant = variant.lower()
    assert variant in {"vae", "dae", "cae"}, "variant must be vae, dae or cae"

    ds_name = (dataset_override or cfg.get("data", {}).get("dataset", "squad")).lower()
    if ds_name == "squad":
        paths = _prepare_squad(cfg)
    elif ds_name == "uda":
        paths = _prepare_uda(cfg)
    else:
        raise ValueError(f"Unknown dataset: {ds_name}")

    path = paths[variant]
    if not Path(path).exists():
        raise FileNotFoundError(f"Expected dataset file not found: {path}")
    return path




def load_eval_queries_from_squad(
    version: str = "v1",
    split: str = "validation",
    max_samples: Optional[int] = None,
    dedup: bool = True,
) -> Tuple[List[str], List[str], List[List[str]]]:
    """
    Prepara triples (queries, corpus, relevantes) para evaluación de retrieval.

    Args:
        version: "v1" o "v2"
        split: "train" o "validation"
        max_samples: límite de queries
        dedup: si True, elimina contextos repetidos del corpus

    Returns:
        queries, corpus, relevant_docs (1 a 1 con queries)
    """
    ds_name = "squad_v2" if version == "v2" else "squad"
    ds = load_dataset(ds_name, split=split)

    queries, contexts, relevant = [], [], []

    for ex in ds:
        q = ex["question"].strip()
        c = ex["context"].strip()

        # descartar preguntas sin respuesta si es v2
        if version == "v2":
            has_answer = bool(ex["answers"]["answer_start"])
            if not has_answer:
                continue

        queries.append(q)
        contexts.append(c)
        relevant.append([c])  # relevante = ese contexto

        if max_samples and len(queries) >= max_samples:
            break

    corpus = list(set(contexts)) if dedup else contexts
    return queries, corpus, relevant


def load_evaluation_data(dataset: str, max_samples: int = 200):
    if dataset == "squad":
        return load_eval_queries_from_squad(
            version="v1", split="validation", max_samples=max_samples
        )
    elif dataset == "uda":
        raise NotImplementedError("TODO: soporte UDA")
    else:
        raise ValueError(f"Dataset desconocido: {dataset}")
"""

utils/load_config.py
"""
import numpy as np
import yaml
import os, sys
from pathlib import Path
from types import SimpleNamespace   
import logging

def load_config(path: str) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def init_logger(cfg_logging: dict) -> SimpleNamespace:

    if cfg_logging.get("log_to_file", False):
        Path(cfg_logging["log_file"]).parent.mkdir(parents=True, exist_ok=True)

    handlers = [logging.StreamHandler(sys.stdout)]
    if cfg_logging.get("log_to_file", False):
        handlers.append(logging.FileHandler(cfg_logging["log_file"], encoding="utf-8"))

    logging.basicConfig(
        level=getattr(logging, cfg_logging.get("level", "INFO")),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        handlers=handlers,
        force=True,                        
    )

    return SimpleNamespace(
        main = logging.getLogger("main"),
        train = logging.getLogger("train"),
        utils = logging.getLogger("utils"),
    )
"""

utils/training_utils.py
"""
# utils/training_utils.py
import os, random, logging
import numpy as np
import torch

def set_seed( seed: int, deterministic: bool = False, logger: logging.Logger | None = None ) -> None:
    """
    Fija todas las semillas y el modo determinista de cuDNN.

    Args:
        seed (int): valor de la semilla.
        deterministic (bool): True → reproducibilidad completa
                              (más lento en GPU).
        logger (logging.Logger | None): instancia de logger principal;
                                        si es None se usa el del módulo.
    """
    logger = logger or logging.getLogger(__name__)

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.deterministic = deterministic
    torch.backends.cudnn.benchmark     = not deterministic
    torch.use_deterministic_algorithms(deterministic)

    if deterministic:
        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
        logger.info("cuDNN deterministic mode  ACTIVE (desactivated benchmark mode)")
    else:
        logger.info("cuDNN benchmark mode ACTIVE (desactivated deterministic mode)")


def resolve_device(device_str: str | None = None) -> str:
    if device_str is not None:
        return device_str
    return "cuda" if torch.cuda.is_available() else "cpu"

"""

utils/visualization_exp.py
"""
"""CLI helper — visualise compressed vs. original SBERT embeddings.

Supports both t‑SNE and PCA as low‑dimensional projections and 2‑D or 3‑D
scatter plots.  Latent embeddings are **recomputed in memory** (no cache for
compressed codes).  Works with the three AE checkpoints in the repo
(`contrastive`, `dae`, `vae`).

Example

python -m utils.visualization_exp \
  --sbert-cache data/SQUAD/sbert_cache/sbert_2254a38d6b_all-MiniLM-L6-v2.pt \
  --checkpoint  models/checkpoints/contrastive_ae.pth \
  --projection  tsne \
  --components  2 \
  --sample-size 1200 \
  --k-near 10 

  
"""
from __future__ import annotations

import argparse
from pathlib import Path
from typing import Tuple

import torch

from evaluation.embedding_visualization import visualize_compressed_vs_original

# ---------------------------------------------------------------------------
#  Auto‑encoder loader                                                        
# ---------------------------------------------------------------------------

def _load_autoencoder(ckpt: str, *, device: torch.device | str = "cpu") -> torch.nn.Module:
    """Instantiate the proper AE subclass and load weights from *ckpt*."""
    name = Path(ckpt).name.lower()
    if "contrastive" in name or "cae" in name:
        from models.contrastive_autoencoder import ContrastiveAutoencoder as AE
    elif "dae" in name:
        from models.denoising_autoencoder import DenoisingAutoencoder as AE
    elif "vae" in name:
        from models.variational_autoencoder import VariationalAutoencoder as AE
    else:
        raise ValueError(f"Cannot infer AE type from checkpoint name: {ckpt}")

    model = AE(input_dim=384, latent_dim=64, hidden_dim=512)
    state = torch.load(ckpt, map_location="cpu")
    model.load_state_dict(state)
    return model.to(device).eval()

# ---------------------------------------------------------------------------
#  SBERT pairs loader                                                         
# ---------------------------------------------------------------------------

def _load_sbert_pairs(path: str | Path, n: int, seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor]:
    """Return `(queries, positives)` tensors of shape [n, 384] (CPU)."""
    emb = torch.load(path, map_location="cpu")
    if emb.dim() != 2 or emb.size(0) % 2 != 0:
        raise ValueError("SBERT cache must have shape [2N, D]")
    n_pairs = emb.size(0) // 2
    n = min(n, n_pairs)
    idx = torch.randperm(n_pairs, generator=torch.Generator().manual_seed(seed))[:n]
    q_idx = 2 * idx
    d_idx = q_idx + 1
    return emb[q_idx], emb[d_idx]

# ---------------------------------------------------------------------------
#  Helpers
# ---------------------------------------------------------------------------


def _infer_ae_type(ckpt_name: str) -> str:
    """Return a short tag identifying the AE flavour."""
    lower = ckpt_name.lower()
    if "contrastive" in lower or "cae" in lower:
        return "cae"
    if "dae" in lower:
        return "dae"
    if "vae" in lower:
        return "vae"
    return "ae"


def _build_default_path(
    ckpt: str,
    *,
    projection: str,
    n_components: int,
    sample_size: int,
    k_near: int,
    perplexity: float,
) -> Path:
    """Compose a descriptive file name and ensure ``fig/`` exists.

    The directory is created with parents if missing.
    """
    ae_tag = _infer_ae_type(Path(ckpt).stem)
    fname = f"{ae_tag}_{projection}_{n_components}d_{sample_size}s_{k_near}k"
    if projection == "tsne":
        fname += f"_perp{int(perplexity)}"
    fname += ".png"

    fig_dir = Path("fig")  # ← default relative folder
    fig_dir.mkdir(parents=True, exist_ok=True)
    return fig_dir / fname


# ---------------------------------------------------------------------------
#  Main
# ---------------------------------------------------------------------------


def main() -> None:
    parser = argparse.ArgumentParser(
        "Visualise AE-compressed vs. original embeddings"
    )

    # required paths
    parser.add_argument(
        "--sbert-cache",
        required=True,
        help=".pt file with SBERT cache (queries/ctx interleaved)",
    )
    parser.add_argument(
        "--checkpoint",
        required=True,
        help="Path to trained AE checkpoint (.pth)",
    )

    # visual options
    parser.add_argument(
        "--projection",
        choices=["tsne", "pca"],
        default="tsne",
        help="Low-D projection method",
    )
    parser.add_argument(
        "--components",
        type=int,
        choices=[2, 3],
        default=2,
        help="Number of projection dimensions",
    )
    parser.add_argument(
        "--perplexity",
        type=float,
        default=30.0,
        help="t-SNE perplexity (ignored for PCA)",
    )
    parser.add_argument(
        "--k-near",
        type=int,
        default=5,
        help="Nearest-neighbour threshold for hits",
    )
    parser.add_argument(
        "--bins",
        type=int,
        default=30,
        help="Histogram bins for distance plot",
    )

    # sampling & io
    parser.add_argument(
        "--sample-size",
        type=int,
        default=1000,
        help="Number of query–doc pairs to sample",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed",
    )
    parser.add_argument(
        "--out",
        default=None,  # ← NEW: None triggers automatic path generation
        help=(
            "Output figure path (PNG/PDF).  If omitted, the file is "
            "saved to ./fig/<params>.png"
        ),
    )

    args = parser.parse_args()

    # ------------------------------------------------------------------ #
    # 1. Load SBERT originals
    # ------------------------------------------------------------------ #
    q_orig, d_orig = _load_sbert_pairs(args.sbert_cache, args.sample_size, seed=args.seed)

    # ------------------------------------------------------------------ #
    # 2. Compress with AE (on-the-fly)
    # ------------------------------------------------------------------ #
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    ae = _load_autoencoder(args.checkpoint, device=device)

    with torch.no_grad():
        q_comp = ae.encode(q_orig.to(device))
        if isinstance(q_comp, tuple):
            q_comp = q_comp[0]
        d_comp = ae.encode(d_orig.to(device))
        if isinstance(d_comp, tuple):
            d_comp = d_comp[0]
        q_comp, d_comp = q_comp.cpu(), d_comp.cpu()

    # ------------------------------------------------------------------ #
    # 3. Determine output path                                           #
    # ------------------------------------------------------------------ #
    if args.out is None:  # ← NEW
        args.out = _build_default_path(
            args.checkpoint,
            projection=args.projection,
            n_components=args.components,
            sample_size=args.sample_size,
            k_near=args.k_near,
            perplexity=args.perplexity,
        )

    # ------------------------------------------------------------------ #
    # 4. Visualise                                                       #
    # ------------------------------------------------------------------ #
    metrics = visualize_compressed_vs_original(
        q_orig,
        d_orig,
        q_comp,
        d_comp,
        projection=args.projection,
        n_components=args.components,
        sample_size=args.sample_size,
        k_near=args.k_near,
        perplexity=args.perplexity,
        bins=args.bins,
        random_state=args.seed,
        save_path=str(args.out),  # ensure Path → str
        save_negatives_path=str(args.out).replace(".png", "_negatives_distribution.png")  # Save negatives plot
    )



    print(f"Figure saved  {args.out}\n")


if __name__ == "__main__":
    main()
"""

