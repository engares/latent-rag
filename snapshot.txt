### Directory tree for: .

./
├── AUTORAG.code-workspace
├── README.md
├── config/
│   ├── config.yaml
│   └── prompts/
│       └── system_prompt.txt
├── data/
│   ├── SQUAD/
│   │   ├── squad_contrastive_embeddings.pt
│   │   ├── squad_dae_embeddings.pt
│   │   └── squad_vae_embeddings.pt
│   ├── UDA/
│   │   ├── uda_contrastive_embeddings.pt
│   │   ├── uda_dae_embeddings.pt
│   │   └── uda_vae_embeddings.pt
│   ├── data_processing.py
│   └── torch_datasets.py
├── evaluation/
│   ├── autoencoder_metrics.py
│   ├── benchmark.py
│   ├── generation_metrics.py
│   └── retrieval_metrics.py
├── generation/
│   └── generator.py
├── logs/
│   └── run.log
├── main.py
├── models/
│   ├── base_autoencoder.py
│   ├── checkpoints/
│   │   ├── contrastive_ae.pth
│   │   ├── dae_text.pth
│   │   └── vae_text.pth
│   ├── contrastive_autoencoder.py
│   ├── denoising_autoencoder.py
│   └── variational_autoencoder.py
├── requeriments.txt
├── retrieval/
│   ├── base.py
│   ├── bm25.py
│   ├── dpr.py
│   ├── embedder.py
│   └── retriever.py
├── save_snapshot.sh*
├── snapshot.txt
├── style_guide.md
├── test/
│   ├── test_data_processing.py
│   ├── test_evaluation.py
│   ├── test_loss_functions.py
│   ├── test_models.py
│   ├── test_retrieval.py
│   └── test_train_scripts.py
├── tests.ipynb
├── training/
│   ├── loss_functions.py
│   ├── train_cae.py
│   ├── train_dae.py
│   └── train_vae.py
└── utils/
    ├── data_utils.py
    ├── load_config.py
    └── training_utils.py

14 directories, 49 files


### File contents

AUTORAG.code-workspace
"""
{
	"folders": [
		{
			"path": "."
		}
	],
	"settings": {}
}
"""

README.md
"""

# rag\_autoencoder\_tfm

A repository for training and evaluating retrieval-augmented generation (RAG) pipelines enhanced by various autoencoder compression methods. Supported variants include:

* Variational Autoencoder (VAE)
* Denoising Autoencoder (DAE)
* Contrastive Autoencoder (CAE)

This framework covers from data preparation, model training, retrieval to optional generation with LLMs, and comprehensive evaluation metrics.

---

## Table of Contents

1. [Features](#features)
2. [Prerequisites](#prerequisites)
3. [Setup](#setup)
4. [Configuration](#configuration)
5. [Data Preparation](#data-preparation)
6. [Training](#training)

   * [VAE](#vae)
   * [DAE](#dae)
   * [CAE](#cae)
7. [Pipeline Execution](#pipeline-execution)
8. [Evaluation](#evaluation)
9. [Code Style](#code-style)
10. [Project Structure](#project-structure)
11. [Testing](#testing)
12. [Scripts](#scripts)

---

## Features

* Encode text embeddings using SBERT and compress with VAE/DAE/CAE.
* Retrieve top‑k relevant documents using cosine, Euclidean or Mahalanobis similarity.
* Generate answers via RAG using OpenAI GPT-4o-mini with custom system prompts.
* Evaluate retrieval (Recall\@k, MRR, nDCG) and generation (BLEU, ROUGE-L, METEOR) with bootstrap CIs.
* Configurable via YAML; extensible for other datasets or LLM providers.

## Prerequisites

* Python ≥ 3.10
* GPU recommended for large-scale embedding and training.
* OpenAI API key (set in `.env` as `OPENAI_API_KEY`).

Install dependencies:

```bash
pip install -r requirements.txt
```

## Setup

1. Clone the repository:

```bash

git clone https://github.com/engares/latent-rag.git
cd latent-rag
```

2. Create `.env` with your OpenAI key:
```ini
OPENAI_API_KEY=your_api_key_here
````

3. Adjust paths and hyperparameters in **`config/config.yaml`** as needed.

## Configuration

**`config/config.yaml`** contains:

* Project metadata (name, version)
* Directory paths (`data_dir`, `checkpoints_dir`, `logs_dir`)
* Embedding model settings
* Autoencoder parameters and checkpoints
* Training hyperparameters (batch size, epochs, LR)
* Retrieval & generation options
* Evaluation metrics
* Logging level and file

System prompt for generation is located in **`config/prompts/system_prompt.txt`**.

## Data Preparation

Data tensors for SQuAD are generated automatically when running training or pipeline. To prepare manually:

```bash
python -c "from utils.data_utils import ensure_squad_data; ensure_squad_data(output_dir='./data/SQUAD')"
```

This creates:

* `data/SQUAD/squad_vae_embeddings.pt`
* `data/SQUAD/squad_dae_embeddings.pt`
* `data/SQUAD/squad_contrastive_embeddings.pt`

## Training

### VAE

```bash
python training/train_vae.py \
  --dataset squad \
  --epochs 50 \
  --batch_size 128 \
  --lr 1e-3 \
  --save_path models/checkpoints/vae_text.pth
```

### DAE

```bash
python training/train_dae.py \
  --dataset squad \
  --epochs 50 \
  --batch_size 128 \
  --lr 1e-3 \
  --save_path models/checkpoints/dae_text.pth
```

### CAE

```bash
python training/train_cae.py \
  --dataset squad \
  --epochs 50 \
  --batch_size 128 \
  --lr 1e-3 \
  --save_path models/checkpoints/contrastive_ae.pth
```

All training scripts support early stopping, checkpointing and configurable device.

## Pipeline Execution

Run the end-to-end RAG pipeline:

```bash
python main.py --config config/config.yaml --ae_type vae
```

Replace `--ae_type` with `dae`, `contrastive`, `all` or `none`. The pipeline will:

1. Encode corpus and queries
2. Retrieve top‑k documents
3. Generate answers via GPT-4o-mini
4. Evaluate retrieval and generation metrics

## Evaluation

* Retrieval metrics: per-query and aggregated Recall\@k, MRR, nDCG.
* Generation metrics: BLEU, ROUGE-L, METEOR with 95% bootstrap CIs.
* Visualise embeddings via **`evaluation/autoencoder_metrics.py`** (t-SNE plots).

Commands:

```python
from evaluation.retrieval_metrics import evaluate_retrieval
from evaluation.generation_metrics import evaluate_generation_bootstrap
```


## Project Structure

```text
src/
├── config/           # YAML and prompts
├── data/             # Embedding tensors and loaders
├── evaluation/       # Metrics and visualisations
├── generation/       # RAG generator
├── models/           # AE implementations
├── retrieval/        # Embeddings & retriever
├── training/         # Training scripts and loss functions
├── utils/            # Helpers (config, data, logging)
├── test/             # Unit tests (pytest)
├── main.py           # CLI orchestration
├── requirements.txt
└── style_guide.md
```

## Testing

Run all tests via pytest:

```bash
pytest -q
```

Coverage threshold: 80% (unit tests for data processing, models, retrieval, evaluation, training scripts).


"""

config/config.yaml
"""
project:
  name: rag_autoencoder_tfm
  version: "0.1"

paths:
  data_dir: "./data"
  checkpoints_dir: "./models/checkpoints"
  logs_dir: "./logs"

embedding_model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  max_length: 256

models:
  vae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/SQUAD/squad_vae_embeddings.pt"          
    checkpoint: "./models/checkpoints/vae_text.pth"

  dae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/SQUAD/squad_dae_embeddings.pt"
    checkpoint: "./models/checkpoints/dae_text.pth"

  contrastive:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/SQUAD/squad_contrastive_embeddings.pt"
    checkpoint: "./models/checkpoints/contrastive_ae.pth"

data:
  dataset: "squad"            #  "uda"  |  "squad"
  version: "v1"          #  v1, v2  ONly for squad
  max_samples:         #  optional, blankk to use all samples
  include_unanswerable: false   #  Only Squad

training:
  batch_size: 128
  epochs: 50
  learning_rate: 1e-3
  seed: 42
  device: "cuda"  # "cpu"
  deterministic: false      # (true = modo debug)

retrieval:
  similarity_metric: "cosine"   # cosine, mahalanobis
  top_k: 20
  compress_embeddings: true 

generation:
  provider: "openai"       # Just OpenAI by now
  model: "gpt-4o-mini"
  temperature: 0.3
  max_tokens: 256
  system_prompt_path: "./config/prompts/system_prompt.txt"


evaluation:
  retrieval_metrics: ["Recall@5", "MRR@10", "nDCG@10"] # You can change the k top  simply by changing the @k e.j Recall@10, "MRR@20"
  generation_metrics: ["ROUGE-L", "BLEU", "METEOR"]

logging:
  level: "INFO"
  log_to_file: true
  log_file: "./logs/run.log"

"""

config/prompts/system_prompt.txt
"""
Here is the user query and relevant text chunks. Step 1: Summarize user question in simpler words. Step 2: Decide which retrieved text chunks directly apply. Step 3: Combine those chunks into an outline. Step 4: Draft a single, coherent answer. Show all steps, then provide a final refined answer.
"""

data/data_processing.py
"""
#/data_processing.py

import random
import json
import re
from typing import List, Dict
from datasets import load_dataset

# ---------------------------------------------------------
# UDA Dataset Preprocessing for Autoencoder Training
# ---------------------------------------------------------
# Supports: Denoising AE (with artificial noise), VAE, Contrastive AE
# ---------------------------------------------------------

def clean_text(text: str) -> str:
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def add_noise(text: str, removal_prob=0.1, swap_prob=0.05) -> str:
    words = text.split()
    # Remove tokens
    words = [w for w in words if random.random() > removal_prob]
    # Swap nearby tokens
    for i in range(len(words)-1):
        if random.random() < swap_prob:
            words[i], words[i+1] = words[i+1], words[i]
    return " ".join(words)

def build_dae_dataset(samples: List[str]) -> List[Dict[str, str]]:
    dataset = []
    for original in samples:
        noisy = add_noise(original)
        dataset.append({"input": noisy, "target": original})
    return dataset

def build_contrastive_pairs(dataset, max_negatives=1) -> List[Dict]:
    pairs = []
    for example in dataset:
        q = example["query"]
        pos = example["positive_passages"][0]["text"]
        negs = [n["text"] for n in example["negative_passages"][:max_negatives]]
        for neg in negs:
            pairs.append({"query": q, "positive": pos, "negative": neg})
    return pairs

"""

data/torch_datasets.py
"""
# /data/torch_datasets.py
import torch
from torch.utils.data import Dataset
from typing import Dict, List, Tuple


# ---------- UTILIDADES COMUNES ------------------------------------------------
def _load_pt(path: str) -> Dict[str, torch.Tensor]:
    """
    Carga un fichero .pt con tensores y asegura dtype = float32 en CPU.
    El fichero se espera como un dict { name: Tensor }.
    """
    data = torch.load(path, map_location="cpu")
    return {k: v.float() for k, v in data.items()}


# ---------- DATASETS ---------------------------------------------------------


class EmbeddingVAEDataset(Dataset):
    """
    Carga el fichero .pt generado por `ensure_uda_data`.
    Estructura esperada:
        {"input": <tensor [N×D]>, "target": <tensor [N×D]>}
    """
    def __init__(self, path: str):
        data = torch.load(path, map_location="cpu")
        self.input  = data["input"].float()
        self.target = data["target"].float()
        assert self.input.shape == self.target.shape, "input/target tamaño desigual"

    def __len__(self):
        return self.input.size(0)

    def __getitem__(self, idx):
        return {
            "input":  self.input[idx],
            "target": self.target[idx],
        }


class EmbeddingDAEDataset(Dataset):
    """
    Carga 'uda_dae_embeddings.pt' producido por `ensure_uda_data`.

    Estructura:
        {
            "input":  Tensor [N × D]  (embeddings con ruido)
            "target": Tensor [N × D]  (embeddings limpios)
        }
    """
    def __init__(self, path: str):
        d = torch.load(path, map_location="cpu")
        self.x  = d["input" ].float()
        self.y  = d["target"].float()
        assert self.x.shape == self.y.shape, "Input / target mismatch"

    def __len__(self):          return self.x.size(0)
    def __getitem__(self, idx): return {"x": self.x[idx], "y": self.y[idx]}

    

class EmbeddingTripletDataset(Dataset):
    """
    Carga 'uda_contrastive_embeddings.pt' generado por `ensure_uda_data`.

    Estructura esperada:
        {
            "query":     Tensor [N × D],
            "positive":  Tensor [N × D],
            "negative":  Tensor [N × D]
        }
    """
    def __init__(self, path: str):
        data = torch.load(path, map_location="cpu")
        self.q  = data["query"].float()
        self.p  = data["positive"].float()
        self.n  = data["negative"].float()
        assert self.q.shape == self.p.shape == self.n.shape, "Dimensiones incompatibles"

    def __len__(self) -> int:          return self.q.size(0)

    def __getitem__(self, idx):        # devuelvo tensores individuales
        return {"q": self.q[idx],
                "p": self.p[idx],
                "n": self.n[idx]}


# ---------- PRUEBA RÁPIDA -----------------------------------------------------
if __name__ == "__main__":
    dae_ds = EmbeddingDAEDataset("./data/squad_dae_embeddings.pt")
    vae_ds = EmbeddingDAEDataset("./data/squad_vae_embeddings.pt")
    con_ds = EmbeddingTripletDataset("./data/squad_contrastive_embeddings.pt")

    print("DAE sample ⇒", {k: v.shape for k, v in dae_ds[0].items()})
    print("Contrastive sample ⇒", {k: v.shape for k, v in con_ds[0].items()})
    print("VAE sample ⇒", {k: v.shape for k, v in vae_ds[0].items()})

"""

evaluation/autoencoder_metrics.py
"""
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from typing import Tuple


def evaluate_reconstruction_loss(x: torch.Tensor, x_reconstructed: torch.Tensor, reduction: str = "mean") -> float:
    """Calcula el error de reconstrucción (MSE)."""
    loss_fn = torch.nn.MSELoss(reduction=reduction)
    return loss_fn(x_reconstructed, x).item()

def visualize_embeddings(embeddings: torch.Tensor, labels: torch.Tensor = None, title: str = "Embeddings Visualization"):
    """Proyección 2D de los embeddings usando t-SNE."""
    tsne = TSNE(n_components=2, random_state=42)
    emb_2d = tsne.fit_transform(embeddings.cpu().numpy())

    plt.figure(figsize=(8, 6))
    if labels is not None:
        sns.scatterplot(x=emb_2d[:, 0], y=emb_2d[:, 1], hue=labels.cpu().numpy(), palette="tab10")
    else:
        sns.scatterplot(x=emb_2d[:, 0], y=emb_2d[:, 1])

    plt.title(title)
    plt.show()
"""

evaluation/benchmark.py
"""
# /evalaution/benchmark.py
from typing import Dict, Sequence
from retrieval.bm25 import BM25Retriever
from retrieval.dpr  import DPRRetriever
from retrieval.embedder import EmbeddingCompressor
from utils.load_config import load_config
from evaluation.retrieval_metrics import evaluate_retrieval, paired_bootstrap_test

Retrievers = {
    "bm25":  BM25Retriever(),
    "dpr":   DPRRetriever(),
    "sbert": EmbeddingCompressor(),          # no AE
    "vae":   EmbeddingCompressor(ae_type="vae"),
    "dae":   EmbeddingCompressor(ae_type="dae"),
    "cae":   EmbeddingCompressor(ae_type="contrastive"),
}

def run_benchmark(queries: Sequence[str],
                  corpus:  Sequence[str],
                  gold:    Sequence[Sequence[str]],
                  cfg_path="./config/config.yaml") -> None:
    cfg = load_config(cfg_path)
    metrics = cfg["evaluation"]["retrieval_metrics"]
    results: Dict[str, Dict[str, float]] = {}

    for name, retr in Retrievers.items():
        retr.build_index(corpus)
        retrieved = [retr.retrieve(q, k=cfg["retrieval"]["top_k"]) for q in queries]
        summary   = evaluate_retrieval(retrieved, gold, metrics)   # media + sd
        results[name] = {m: v["mean"] for m, v in summary.items()}
        print(f"{name.upper():<6}", summary)

    # Significancia (ejemplos)
    _print_sig(results, queries, gold, metrics[0])

def _print_sig(res, queries, gold, metric):
    from itertools import combinations
    for a, b in combinations(res.keys(), 2):
        pa, pb = res[a][metric], res[b][metric]
        # bootstrap paired significance (aproveche evaluate_generation_bootstrap si lo desea)
        print(f"{a} vs {b}: Δ={pa-pb:+.4f}")

"""

evaluation/generation_metrics.py
"""
from __future__ import annotations

"""Métricas de generación con *bootstrap* y prueba de significancia emparejada.

Uso principal:
--------------
>>> mean_ci = evaluate_generation_bootstrap(refs, cands, metrics=["BLEU", "ROUGE-L"])
>>> pval = paired_bootstrap_test(refs, sys_a, sys_b, metric="BLEU")
"""

from collections.abc import Callable
from typing import List, Dict, Tuple
import numpy as np
from sacrebleu.metrics import BLEU as _BLEUMetric
from rouge_score import rouge_scorer
import random

###############################################################################
#  MÉTRICAS BÁSICAS                                                           #
###############################################################################

_bleu = _BLEUMetric()
_scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)


def compute_bleu(candidates: List[str], references: List[str]) -> float:
    """BLEU corpus‐level sacreBLEU (0‑100)."""
    return _bleu.corpus_score(candidates, [references]).score


def compute_rouge_l(candidates: List[str], references: List[str]) -> float:
    """Promedio de ROUGE‑L (F1) ×100."""
    scores = [
        _scorer.score(ref, cand)["rougeL"].fmeasure * 100
        for ref, cand in zip(references, candidates)
    ]
    return float(np.mean(scores))


_metric_fn: Dict[str, Callable[[List[str], List[str]], float]] = {
    "BLEU": compute_bleu,
    "ROUGE-L": compute_rouge_l,
}

###############################################################################
#  BOOTSTRAP                                                                  #
###############################################################################

def _bootstrap_ci(
    func: Callable[[List[str], List[str]], float],
    refs: List[str],
    cands: List[str],
    n_samples: int = 2000,
    alpha: float = 0.05,
    seed: int | None = None,
) -> Tuple[float, float, float]:
    """Devuelve media, límite inferior y superior del IC al (1‑alpha)."""
    if seed is not None:
        random.seed(seed)
    N = len(refs)
    stats = []
    for _ in range(n_samples):
        idx = [random.randint(0, N - 1) for _ in range(N)]
        stats.append(func([cands[i] for i in idx], [refs[i] for i in idx]))
    stats_np = np.array(stats)
    mean = stats_np.mean()
    lower = np.percentile(stats_np, 100 * alpha / 2)
    upper = np.percentile(stats_np, 100 * (1 - alpha / 2))
    return mean, lower, upper


def evaluate_generation_bootstrap(
    references: List[str],
    candidates: List[str],
    metrics: List[str] | None = None,
    n_samples: int = 2000,
    alpha: float = 0.05,
    seed: int | None = None,
) -> Dict[str, Dict[str, float]]:
    """Calcula métricas + IC al 95 % mediante bootstrap.

    Retorna: {metric: {"mean": m, "ci_lower": l, "ci_upper": u}}
    """
    if metrics is None:
        metrics = ["BLEU", "ROUGE-L"]

    assert len(references) == len(candidates) >= 30, (
        "Se requieren al menos 30 pares ref‑cand para un IC mínimo; se recomienda ≥1000."
    )

    results: Dict[str, Dict[str, float]] = {}
    for m in metrics:
        if m not in _metric_fn:
            raise ValueError(f"Métrica '{m}' no soportada.")
        mean, lo, hi = _bootstrap_ci(_metric_fn[m], references, candidates, n_samples, alpha, seed)
        results[m] = {"mean": mean, "ci_lower": lo, "ci_upper": hi}
    return results

###############################################################################
#  PAIRED BOOTSTRAP SIGNIFICANCE TEST                                         #
###############################################################################

def paired_bootstrap_test(
    references: List[str],
    sys_a: List[str],
    sys_b: List[str],
    metric: str = "BLEU",
    n_samples: int = 10000,
    seed: int | None = None,
) -> Dict[str, float]:
    """Prueba de significancia emparejada (bootstrap) entre dos sistemas.

    Devuelve un dict: {"diff_mean": d, "ci_lower": lo, "ci_upper": hi, "p_value": p}
    p‑value ≈ proporción de muestras con diferencia ≤0 (o ≥0, según el signo).
    """
    assert len(references) == len(sys_a) == len(sys_b)
    if seed is not None:
        random.seed(seed)

    if metric not in _metric_fn:
        raise ValueError(f"Métrica '{metric}' no soportada.")
    fn = _metric_fn[metric]

    N = len(references)
    diffs = []
    for _ in range(n_samples):
        idx = [random.randint(0, N - 1) for _ in range(N)]
        a_score = fn([sys_a[i] for i in idx], [references[i] for i in idx])
        b_score = fn([sys_b[i] for i in idx], [references[i] for i in idx])
        diffs.append(a_score - b_score)

    diffs_np = np.array(diffs)
    diff_mean = diffs_np.mean()
    ci_lower = np.percentile(diffs_np, 2.5)
    ci_upper = np.percentile(diffs_np, 97.5)
    # p‑value: H0: diff <= 0  (si diff_mean>0) ó diff >=0 (si diff_mean<0)
    if diff_mean >= 0:
        p_val = (diffs_np <= 0).mean()
    else:
        p_val = (diffs_np >= 0).mean()

    return {
        "diff_mean": diff_mean,
        "ci_lower": ci_lower,
        "ci_upper": ci_upper,
        "p_value": p_val,
    }

"""

evaluation/retrieval_metrics.py
"""
# evaluation/retrieval_metrics.py

from __future__ import annotations

import numpy as np
from typing import List, Sequence, Dict, Tuple, Union
              
ID = Union[int, str]

###############################################################################
#  Métricas elementales (1 consulta)
###############################################################################

def recall_at_k(retrieved: Sequence[ID], relevant: Sequence[ID], k: int) -> float:
    if not relevant:
        return 0.0
    return len(set(retrieved[:k]) & set(relevant)) / len(relevant)

def mrr(retrieved: Sequence[ID], relevant: Sequence[ID]) -> float:
    for i, d in enumerate(retrieved, 1):
        if d in relevant:
            return 1.0 / i
    return 0.0

def ndcg_at_k(retrieved: Sequence[ID], relevant: Sequence[ID], k: int) -> float:
    dcg = sum(
        1.0 / np.log2(i + 2) if d in relevant else 0.0
        for i, d in enumerate(retrieved[:k])
    )
    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), k)))
    return dcg / idcg if idcg else 0.0

###############################################################################
#  Vectorización sobre lotes
###############################################################################

def _parse_metric(m: str) -> Tuple[str, int | None]:
    return (m.split("@")[0], int(m.split("@")[1])) if "@" in m else (m, None)

def _score_single(
    retrieved: Sequence[ID],
    relevant: Sequence[ID],
    name: str,
    k: int | None,
) -> float:
    name = name.lower()
    if name == "recall" and k is not None:
        return recall_at_k(retrieved, relevant, k)
    if name == "mrr":
        return mrr(retrieved[: (k or len(retrieved))], relevant)
    if name == "ndcg" and k is not None:
        return ndcg_at_k(retrieved, relevant, k)
    raise ValueError(f"Métrica '{name}' mal especificada.")

def evaluate_retrieval(retrieved_batch: List[Sequence[ID]] | Sequence[ID],
    relevant_batch: List[Sequence[ID]] | Sequence[ID],
    metrics: List[str] | None = None,
    *,
    return_per_query: bool = False,
) -> Dict[str, Dict[str, float]] | Tuple[Dict[str, Dict[str, float]],
                                         List[Dict[str, float]]]:
    
    # ── Normalizar entrada a lote ──────────────────────────────────────────
    single = isinstance(retrieved_batch[0], (str, int))  # type: ignore[index]
    if single:
        retrieved_batch = [retrieved_batch]              # type: ignore[assignment]
        relevant_batch  = [relevant_batch]               # type: ignore[assignment]

    assert len(retrieved_batch) == len(relevant_batch), \
        "retrieved_batch and relevant_batch must have the same length."

    if not metrics:
        raise ValueError("No metrics specified.")

    Q = len(retrieved_batch)
    per_query: List[Dict[str, float]] = [{} for _ in range(Q)]
    summary: Dict[str, Dict[str, float]] = {}

    for m in metrics:
        name, k = _parse_metric(m)
        vals = [
            _score_single(r, rel, name, k)
            for r, rel in zip(retrieved_batch, relevant_batch)
        ]
        summary[m] = {
            "mean": float(np.mean(vals)),
            "std":  float(np.std(vals, ddof=1)) if Q > 1 else 0.0,
        }
        for d, v in zip(per_query, vals):
            d[m] = v

    if return_per_query:
        return summary, per_query
    if single:
        return {k: v["mean"] for k, v in summary.items()}      # compat.
    return summary

"""

generation/generator.py
"""
# generation/generator.py  – RAG (Refactor estilo train_vae)

from __future__ import annotations
import os, textwrap, logging
from typing import List, Dict, Any
from dataclasses import dataclass, field
from openai import OpenAI, AsyncOpenAI

def _load_prompt(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        logging.getLogger(__name__).warning("Prompt no encontrado: %s", path)
        return ""

@dataclass
class LLMSettings:
    model: str = "gpt-4o-mini"
    temperature: float = 0.3
    top_p: float = 1.0
    max_tokens: int = 512
    system_prompt_path: str = "./config/prompts/system_prompt.txt"
    system_prompt: str = field(init=False)

    def __post_init__(self):
        self.system_prompt = _load_prompt(self.system_prompt_path)

@dataclass
class GeneratorConfig:
    llm: LLMSettings = field(default_factory=LLMSettings)
    max_context_tokens: int = 4096
    provider: str = "openai"          
    extras: Dict[str, Any] = field(default_factory=dict)


###############################################################################
#  RAG GENERATOR                                                               #
###############################################################################

class RAGGenerator:
    """Generador basado en LLM + documentos recuperados."""

    def __init__(self, cfg: Dict[str, Any], **overrides):
        # fusinon YAML + overrides CLI
        gen_cfg_dict = {**cfg.get("generation", {}), **overrides}

        llm_cfg_dict = gen_cfg_dict.pop("llm", {})
        self.cfg = GeneratorConfig(
            llm=LLMSettings(**llm_cfg_dict),
            **{k: v for k, v in gen_cfg_dict.items() if k in {"max_context_tokens", "provider"}},
            extras={k: v for k, v in gen_cfg_dict.items() if k not in {"max_context_tokens", "provider"}}
        )

        self.logger = logging.getLogger(self.__class__.__name__)
        self._init_openai()

    # ---------------- PUBLIC API -------------------------------------------
    def generate(self, query: str, retrieved_docs: List[str]) -> str:
        prompt = self._build_prompt(query, retrieved_docs)
        self.logger.debug("Prompt (%d chars) construido.", len(prompt))

        response = self.client.chat.completions.create(
            model=self.cfg.llm.model,
            temperature=self.cfg.llm.temperature,
            top_p=self.cfg.llm.top_p,
            max_tokens=self.cfg.llm.max_tokens,
            messages=[
                {"role": "system", "content": self.cfg.llm.system_prompt},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()


    async def generate_async(self, query: str, retrieved_docs: List[str]) -> str:
        prompt = self._build_prompt(query, retrieved_docs)
        self.logger.debug("Prompt async (%d chars).", len(prompt))

        client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = await client.chat.completions.create(
            model=self.cfg.llm.model,
            temperature=self.cfg.llm.temperature,
            top_p=self.cfg.llm.top_p,
            max_tokens=self.cfg.llm.max_tokens,
            messages=[
                {"role": "system", "content": self.cfg.llm.system_prompt},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()

    # ---------------- INTERNALS -------------------------------------------
    def _init_openai(self) -> None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise EnvironmentError(
                "Variable OPENAI_API_KEY no definida. Añádela a tu .env o al entorno."
            )
        self.client = OpenAI(api_key=api_key)
        self.logger.info("API Key OpenAI cargada correctamente.")

    def _build_prompt(self, query: str, docs: List[str]) -> str:
        context = self._truncate_docs(docs)
        joined = "\n\n".join(f"Doc {i+1}: {d}" for i, d in enumerate(context))
        return textwrap.dedent(
            f"""\
            Utiliza exclusivamente la siguiente información para responder.\n\n{joined}\n\n
            Pregunta: {query}\n\nRespuesta:"""
        )

    def _truncate_docs(self, docs: List[str]) -> List[str]:
        max_chars = self.cfg.max_context_tokens * 4   # heuristic ≈ tokens*4
        out, acc = [], 0
        for d in docs:
            if acc + len(d) > max_chars:
                break
            out.append(d)
            acc += len(d)
        return out

"""

main.py
"""
from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple

import torch
from dotenv import load_dotenv
from utils.data_utils import load_evaluation_data

# Third‑party
from rich import print as rprint
from sentence_transformers import SentenceTransformer  # lazy‑loaded by embedder

# First‑party (repository) -----------------------------------------------------
from utils.load_config import init_logger, load_config
from utils.training_utils import resolve_device, set_seed
from retrieval.embedder import EmbeddingCompressor
from retrieval.retriever import retrieve_top_k
from evaluation.retrieval_metrics import evaluate_retrieval
from evaluation.generation_metrics import (
    evaluate_generation_bootstrap as eval_generation,
)
from generation.generator import RAGGenerator
from models.variational_autoencoder import VariationalAutoencoder
from models.denoising_autoencoder import DenoisingAutoencoder
from models.contrastive_autoencoder import ContrastiveAutoencoder

# ---------------------------------------------------------------------------
# Helper factories
# ---------------------------------------------------------------------------

def _load_autoencoder(
    cfg_models: Dict[str, Dict[str, Any]],
    ae_type: str,
    device: str,
) -> Optional[torch.nn.Module]:
    """Instantiate and load the requested auto‑encoder.

    Args:
        cfg_models: Dict extracted from YAML under `models`.
        ae_type:    "vae", "dae", "contrastive" or "none".
        device:     "cpu" | "cuda".

    Returns:
        A `torch.nn.Module` in eval mode, or *None* if `ae_type == "none"`.
    """

    if ae_type == "none":
        return None

    if ae_type not in cfg_models:
        raise ValueError(
            f"[CONFIG] Auto‑encoder '{ae_type}' not found under 'models' in config."
        )

    mcfg = cfg_models[ae_type]
    input_dim = mcfg.get("input_dim", 384)
    latent_dim = mcfg.get("latent_dim", 64)
    hidden_dim = mcfg.get("hidden_dim", 512)
    checkpoint = mcfg.get("checkpoint")

    if ae_type == "vae":
        model: torch.nn.Module = VariationalAutoencoder(  # type: ignore[assignment]
            input_dim, latent_dim, hidden_dim
        )
    elif ae_type == "dae":
        model = DenoisingAutoencoder(input_dim, latent_dim, hidden_dim)
    elif ae_type == "contrastive":
        model = ContrastiveAutoencoder(input_dim, latent_dim, hidden_dim)
    else:
        raise RuntimeError("Unreachable branch – ae_type already validated.")

    if checkpoint and Path(checkpoint).exists():
        model.load_state_dict(torch.load(checkpoint, map_location=device))
    else:
        raise FileNotFoundError(f"Checkpoint for '{ae_type}' not found: {checkpoint}")

    return model.to(device).eval()


# ---------------------------------------------------------------------------
# Pipeline steps
# ---------------------------------------------------------------------------

def _encode_corpus(
    compressor: EmbeddingCompressor,
    texts: Sequence[str],
    compress: bool = True,
) -> torch.Tensor:
    """Return document embeddings as `[N × D]` float32 CPU tensor."""

    return compressor.encode_text(list(texts), compress=compress)


def _retrieve_documents(
    query_emb: torch.Tensor,
    doc_emb: torch.Tensor,
    corpus: Sequence[str],
    retr_cfg: Dict[str, Any],
) -> Tuple[List[str], List[float]]:
    """Retrieve top‑k docs and similarity scores for a *single* query."""

    top_k = retr_cfg.get("top_k", 10)
    metric = retr_cfg.get("similarity_metric", "cosine")
    results = retrieve_top_k(query_emb, doc_emb, list(corpus), k=top_k, metric=metric)
    docs, scores = zip(*results)
    return list(docs), list(scores)


def _evaluate_retrieval(
    retrieved: Sequence[Sequence[str]],
    relevant: Sequence[Sequence[str]] | Sequence[str],
    metrics: List[str],
) -> Dict[str, Dict[str, float]]:
    """Wrapper around `evaluate_retrieval` with sensible defaults."""

    return evaluate_retrieval(retrieved, relevant, metrics=metrics)


# ---------------------------------------------------------------------------
# Core runner
# ---------------------------------------------------------------------------

class PipelineRunner:  # noqa: D101 – simple orchestrator
    def __init__(self, cfg: Dict[str, Any], ae_type: str, logger):
        self.cfg = cfg
        self.ae_type = ae_type
        self.logger = logger

        device = resolve_device(cfg.get("training", {}).get("device"))
        self.device = device
        self.logger.main.info("Device resolved → %s", device)

        # Auto‑encoder & compressor ------------------------------------------------
        ae_model = _load_autoencoder(cfg["models"], ae_type, device)
        self.compressor = EmbeddingCompressor(
            base_model_name=cfg["embedding_model"]["name"],
            autoencoder=ae_model,
            device=device,
        )
        self.logger.main.info("Compressor ready (AE = %s)", ae_type)

        # Retriever ---------------------------------------------------------------
        self.retr_cfg = cfg.get("retrieval", {})

        # Generator ---------------------------------------------------------------
        self.generator = RAGGenerator(cfg)

    # ---------------------------------------------------------------------
    def process(
        self,
        queries: Sequence[str],
        corpus: Sequence[str],
        relevant_docs: Optional[Sequence[str]] = None,
    ) -> None:
        """Run encode → retrieve → generate → evaluate for *all* queries."""

        self.logger.main.info("Running pipeline: |queries|=%d |docs|=%d", len(queries), len(corpus))
        doc_embeddings = _encode_corpus(self.compressor, corpus, compress=True)
        query_embeddings = _encode_corpus(self.compressor, queries, compress=True)

        all_retrieved: List[Sequence[str]] = []
        answers: List[str] = []

        for idx, (q, q_emb) in enumerate(zip(queries, query_embeddings)):
            docs_k, _ = _retrieve_documents(q_emb, doc_embeddings, corpus, self.retr_cfg)
            all_retrieved.append(docs_k)
            ans = self.generator.generate(q, docs_k)
            answers.append(ans)
            self.logger.main.debug("[%d] Q: %s | A: %s", idx, q, ans[:60] + "…")

        # ----------------------------------------------------------------- EVAL
        eval_cfg = self.cfg.get("evaluation", {})
        if relevant_docs:
            ret_metrics = _evaluate_retrieval(
                all_retrieved,
                relevant_docs,
                metrics=eval_cfg.get("retrieval_metrics", ["Recall@5"]),
            )
            rprint("[bold magenta]\n[Retrieval evaluation]\n[/]")
            for k, v in ret_metrics.items():
                rprint(f"{k}: {v['mean']:.4f} ± {v['std']:.4f}")

        if relevant_docs and len(queries) >= 30:
            # If the user provided refs with ≥30 samples we can bootstrap.
            gen_metrics = eval_generation(
                references=list(relevant_docs),
                candidates=answers,
                metrics=eval_cfg.get("generation_metrics", ["ROUGE-L"]),
            )
            rprint("[bold magenta]\n[Generation evaluation]\n[/]")
            for m, d in gen_metrics.items():
                rprint(f"{m}: {d['mean']:.2f} (CI 95%: {d['ci_lower']:.2f}–{d['ci_upper']:.2f})")


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def _parse_args() -> argparse.Namespace:  # noqa: D401
    """Return command‑line arguments."""

    pre_parser = argparse.ArgumentParser(add_help=False)
    pre_parser.add_argument("--config", default="./config/config.yaml")
    known, _ = pre_parser.parse_known_args(sys.argv[1:])

    cfg = load_config(known.config)
    valid_ae = list(cfg.get("models", {}).keys()) + ["none", "all"]

    parser = argparse.ArgumentParser(description="Run RAG‑AE experimental pipeline")
    parser.add_argument("--config", default="./config/config.yaml", help="Path to YAML config")
    parser.add_argument("--ae_type", default="vae", choices=valid_ae, help="Select auto‑encoder variant")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")

    parser.add_argument("--dataset", choices=["squad", "uda"], default="squad",
                    help="Dataset for evaluation (SQuAD or UDA)")
    parser.add_argument("--max_samples", type=int, default=200,
                        help="Maximum number of queries to use")
    parser.add_argument("--benchmark", action="store_true",
                        help="Compare against BM25, DPR, SBERT, AE...")


    return parser.parse_args()


# ---------------------------------------------------------------------------
# Entry‑point
# ---------------------------------------------------------------------------

def main() -> None:  # noqa: D401 – standard script
    args = _parse_args()

    cfg = load_config(args.config)
    log = init_logger(cfg.get("logging", {}))
    set_seed(args.seed, cfg.get("training", {}).get("deterministic", False), logger=log.train)
    load_dotenv()

    ae_variants = (
        [args.ae_type]
        if args.ae_type != "all"
        else [k for k in cfg.get("models", {}).keys() if k in {"vae", "dae", "contrastive"}]
    )

    # --------------------------------------------------------------------- Toy corpus (replace with real dataset) --
    queries, corpus, relevant = load_evaluation_data(args.dataset, max_samples=2) # args.max_samples

    # --------------------------------------------------------------------- Run each variant
    for ae in ae_variants:
        rprint(f"\n[bold cyan]==== PIPELINE ({ae.upper()}) ====\n[/]")
        runner = PipelineRunner(cfg, ae, log)
        runner.process(queries, corpus, relevant_docs=relevant)


if __name__ == "__main__":
    main()

"""

models/base_autoencoder.py
"""
import torch
import torch.nn as nn
from abc import ABC, abstractmethod

class BaseAutoencoder(nn.Module, ABC):
    def __init__(self, input_dim: int, latent_dim: int):
        super(BaseAutoencoder, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim

    @abstractmethod
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        pass

    @abstractmethod
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        pass

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encode(x)
        return self.decode(z)

"""

models/contrastive_autoencoder.py
"""
# /models/contrastive_autoencoder.py
import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder

class ContrastiveAutoencoder(BaseAutoencoder):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super(ContrastiveAutoencoder, self).__init__(input_dim, latent_dim)

        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # Useful if input vectors are normalized
        )

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encoder(x)
        return torch.nn.functional.normalize(z, p=2, dim=-1)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)
      
    def forward(self, x: torch.Tensor):
        z = self.encode(x)
        x_recon = self.decode(z)
        return x_recon, z

"""

models/denoising_autoencoder.py
"""
# /models/denoising_autoencoder.py

import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder
import torch.nn.functional as F

class DenoisingAutoencoder(BaseAutoencoder):
    """Feed‑forward Denoising Autoencoder.

    The dataset must supply *noisy* inputs; the model learns to reconstruct the
    clean version. Use `dae_loss` (MSE) during training.
    """

    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super().__init__(input_dim, latent_dim)

        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # assume inputs ∈ [0,1]; change if different scale
        )

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        return self.encoder(x)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encode(x)
        return self.decode(z)

"""

models/variational_autoencoder.py
"""
# / models/variational_autoencoder.py
import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder

class VariationalAutoencoder(BaseAutoencoder):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super(VariationalAutoencoder, self).__init__(input_dim, latent_dim)
        
        # Encoder: proyecciones a la media y desviación estándar
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.mu_layer = nn.Linear(hidden_dim, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # Asumimos entrada normalizada (0-1)
        )

    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        h = self.encoder(x)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        return mu, logvar

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_reconstructed = self.decode(z)
        return x_reconstructed, mu, logvar

"""

requeriments.txt
"""
# Core ML & Data Processing
torch>=2.0.0
transformers>=4.38.0
sentence-transformers>=2.2.2
scikit-learn>=1.3.0
numpy>=1.24.0
pandas>=2.0.0

datasets>=2.19.0
scipy>=1.10.0
openai>=1.14.0
pyserini>=0.21


# Visualization & Analysis
matplotlib>=3.7.0
seaborn>=0.12.0

# Evaluation Metrics (Efficient and Torch-Compatible)
rouge-score >=0.1.2
sacrebleu>=2.5.1

# Configuration & Utilities
pyyaml>=6.0
python-dotenv>=1.0.0

# Optional CLI Enhancements
rich>=13.0.0

pytest 
"""

retrieval/base.py
"""
from __future__ import annotations
from typing import Protocol, Sequence, Tuple, List

class BaseRetriever(Protocol):
    """Interface for all first-stage retrievers."""
    def build_index(self, corpus: Sequence[str]) -> None: ...
    def retrieve(self, query: str, k: int) -> List[Tuple[str, float]]: ...

"""

retrieval/bm25.py
"""
from pyserini.search import SimpleSearcher            # pip install pyserini>=0.21
from retrieval.base import BaseRetriever
import tempfile, os, json

class BM25Retriever(BaseRetriever):
    def __init__(self, bm25_k1: float = 0.9, b: float = 0.4):
        self.k1, self.b = bm25_k1, b
        self._searcher = None
        self._tmpdir   = tempfile.mkdtemp()

    def build_index(self, corpus):
        # 1) escribir cada doc en un fichero JSONL (id + text)
        tmp_jsonl = os.path.join(self._tmpdir, "docs.jsonl")
        with open(tmp_jsonl, "w", encoding="utf-8") as f:
            for i, doc in enumerate(corpus):
                f.write(json.dumps({"id": str(i), "contents": doc}) + "\n")

        # 2) invocar indexador Lucene
        from pyserini.index import build_index
        build_index(tmp_jsonl, self._tmpdir, overwrite=True)

        # 3) abrir buscador Lucene
        self._searcher = SimpleSearcher(self._tmpdir)
        self._searcher.set_bm25(self.k1, self.b)

    def retrieve(self, query, k):
        hits = self._searcher.search(query, k)
        return [(h.raw, float(h.score)) for h in hits]

"""

retrieval/dpr.py
"""
from retrieval.base import BaseRetriever
from sentence_transformers import SentenceTransformer
import faiss, numpy as np

class DPRRetriever(BaseRetriever):
    """Dense Passage Retrieval (dual-encoder)."""
    def __init__(self,
                 q_model: str = "facebook-dpr-question_encoder-single-nq-base",
                 p_model: str = "facebook-dpr-ctx_encoder-single-nq-base",
                 device: str | None = None):
        self.q_encoder = SentenceTransformer(q_model, device=device)
        self.p_encoder = SentenceTransformer(p_model, device=device)
        self.index = None
        self.docs  = []

    def build_index(self, corpus):
        self.docs = list(corpus)
        emb = self.p_encoder.encode(self.docs,
                                    batch_size=64,
                                    convert_to_numpy=True,
                                    normalize_embeddings=True)
        d = emb.shape[1]
        self.index = faiss.IndexHNSWFlat(d, 32)
        self.index.hnsw.efConstruction = 200
        self.index.add(emb.astype("float32"))

    def retrieve(self, query, k):
        q_emb = self.q_encoder.encode([query],
                                      convert_to_numpy=True,
                                      normalize_embeddings=True)
        dist, idx = self.index.search(q_emb.astype("float32"), k)
        return [(self.docs[i], float(-dist[0][j])) for j, i in enumerate(idx[0])]

"""

retrieval/embedder.py
"""
# retrieval/embedder.py

from sentence_transformers import SentenceTransformer
import torch


class EmbeddingCompressor:
    def __init__(
        self,
        base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        autoencoder: torch.nn.Module = None,
        device: str = None
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # Load pretrained SBERT model (includes pooling + normalization)
        self.model = SentenceTransformer(base_model_name, device=self.device)

        # Optional autoencoder for compression (VAE, DAE, etc.)
        self.autoencoder = autoencoder.to(self.device) if autoencoder else None
        if self.autoencoder:
            self.autoencoder.eval()

    def encode_text(self, texts: list[str], compress: bool = True) -> torch.Tensor:
        """Returns SBERT embeddings, optionally compressed with an autoencoder.

        Args:
            texts: List of input strings to encode.
            compress: Whether to apply the autoencoder (if available).

        Returns:
            A float32 tensor [N × D] on CPU.
        """
        with torch.no_grad():
            embeddings = self.model.encode(
                texts,
                batch_size=64,
                convert_to_tensor=True,
                normalize_embeddings=True
            ).to(self.device)

            if self.autoencoder and compress:
                encoded = self.autoencoder.encode(embeddings)
                if isinstance(encoded, tuple):  # VAE returns (mu, logvar)
                    encoded = encoded[0]        # use mean as latent code
                return encoded.cpu()

            return embeddings.cpu()

"""

retrieval/retriever.py
"""
# /retrieval/retriever.py

from __future__ import annotations

import torch
import torch.nn.functional as F
import numpy as np
from sklearn.covariance import EmpiricalCovariance
from typing import List, Tuple, Literal

SimilarityMetric = Literal["cosine", "euclidean", "mahalanobis"]

###############################################################################
#  MATRICES DE SIMILITUD                                                      #
###############################################################################

def cosine_similarity_matrix(
    query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor
) -> np.ndarray:
    """Matriz [Q × N] con similitud coseno."""
    q_norm = F.normalize(query_embeddings, p=2, dim=1)
    d_norm = F.normalize(doc_embeddings, p=2, dim=1)
    sim = torch.mm(q_norm, d_norm.T)  # [Q, N]
    return sim.cpu().numpy()


def euclidean_similarity_matrix(
    query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor
) -> np.ndarray:
    """Matriz [Q × N] con "+similitud" inversa a la distancia euclídea."""
    q = query_embeddings.unsqueeze(1)  # [Q, 1, D]
    d = doc_embeddings.unsqueeze(0)  # [1, N, D]
    dist = torch.norm(q - d, dim=2)  # [Q, N]
    return (-dist).cpu().numpy()  # valores altos = más similares


def mahalanobis_similarity_matrix(
    query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor, eps: float = 1e-6
) -> np.ndarray:
    """Matriz [Q × N] con similitud inversa de la distancia de Mahalanobis.

    *Se ajusta la matriz de covarianza exclusivamente sobre los documentos* para
    preservar la simetría deseada en el espacio de recuperación.
    """
    # -- Datos a NumPy -------------------------------------------------------
    d_np: np.ndarray = doc_embeddings.cpu().numpy()
    q_np: np.ndarray = query_embeddings.cpu().numpy()

    # -- Precisión (inversa de la covarianza) -------------------------------
    # EmpiricalCovariance añade regularización de Ledoit‑Wolf si el sistema lo
    # necesita, pero incluimos un término eps para garantizar invertibilidad.
    emp = EmpiricalCovariance(assume_centered=False).fit(d_np)
    VI: np.ndarray = emp.precision_ + eps * np.eye(d_np.shape[1], dtype=np.float64)

    # -- Distancias ----------------------------------------------------------
    diff = q_np[:, None, :] - d_np[None, :, :]  # [Q, N, D]
    # einsum: (q n d, d d, q n d) → (q n)
    dist = np.einsum("qnd,dd,qnd->qn", diff, VI, diff, optimize=True)

    # -- Convertir a "similitud" (negativo de la distancia) -----------------
    sim = -dist  # altos valores ⇒ mayor similitud
    return sim.astype(np.float32)


###############################################################################
#  FRONT‑END DE RECUPERACIÓN                                                  #
###############################################################################

def compute_similarity(
    query_embeddings: torch.Tensor,
    doc_embeddings: torch.Tensor,
    metric: SimilarityMetric = "cosine",
) -> np.ndarray:
    """Devuelve matriz [Q × N] según la métrica solicitada y valida formas."""

    if query_embeddings.dim() == 1:
        query_embeddings = query_embeddings.unsqueeze(0)
    if doc_embeddings.dim() == 1:
        doc_embeddings = doc_embeddings.unsqueeze(0)

    funcs = {
        "cosine": cosine_similarity_matrix,
        "euclidean": euclidean_similarity_matrix,
        "mahalanobis": mahalanobis_similarity_matrix,
    }

    if metric not in funcs:
        raise ValueError(f"Métrica de similitud '{metric}' no soportada.")

    sim = funcs[metric](query_embeddings, doc_embeddings)

    # -------- Validación de forma -----------------------------------------
    q, d = query_embeddings.shape[0], doc_embeddings.shape[0]
    if sim.shape != (q, d):
        raise RuntimeError(
            f"Shape mismatch: expected ({q}, {d}) got {sim.shape} for metric '{metric}'."
        )
    return sim


def retrieve_top_k(
    query_embedding: torch.Tensor,
    doc_embeddings: torch.Tensor,
    doc_texts: List[str],
    k: int = 5,
    metric: SimilarityMetric = "cosine",
) -> List[Tuple[str, float]]:
    """Recupera los *k* documentos con mayor similitud."""

    sim_scores = compute_similarity(query_embedding, doc_embeddings, metric)  # [Q, N]
    # Suponemos única consulta (Q = 1) para esta utilidad.
    if sim_scores.shape[0] != 1:
        raise ValueError("Esta función está pensada para una única consulta.")

    top_idx = sim_scores[0].argsort()[::-1][:k]
    return [(doc_texts[i], float(sim_scores[0, i])) for i in top_idx]

"""


training/loss_functions.py
"""
# /training/loss_functions.py

import torch
import torch.nn.functional as F

###############################################################################
#  VAE                                                                        #
###############################################################################

import torch
import torch.nn.functional as F

def vae_loss(
    x_reconstructed: torch.Tensor,
    x_target: torch.Tensor,
    mu: torch.Tensor,
    logvar: torch.Tensor,
    *,
    mse_reduction: str = "mean",   # "mean" or "sum"
    beta: float = 1.0,             # β-VAE (β=1 → classic VAE)
) -> torch.Tensor:
    """VAE loss = reconstruction + β·KL  (KL normalized by batch).

    Args:
        x_reconstructed: output from the decoder  ― shape [B, D]
        x_target:        original embeddings ― shape [B, D]
        mu, logvar:      parameters of the latent distribution ― shape [B, Z]
        mse_reduction:   "mean" (recommended) or "sum"
        beta:            weight of the KL term (β-VAE)
    """
    # ── 1. reconstruction error ─────────────────────────────────────────
    recon = F.mse_loss(x_reconstructed, x_target, reduction=mse_reduction)

    # ── 2. KL (normalized) ───────────────────────────────────────────────
    #   KL(q(z|x) || N(0,1))  =  -½ Σ_i (1 + logσ²_i − μ²_i − σ²_i)
    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()  # ← .mean() ≈ /B/Z

    return recon + beta * kl

###############################################################################
#  DAE                                                                        #
###############################################################################

def dae_loss(
    x_reconstructed: torch.Tensor,
    x_clean: torch.Tensor,
    reduction: str = "mean",
) -> torch.Tensor:
    """Mean‑squared error for Denoising Auto‑Encoders."""
    return F.mse_loss(x_reconstructed, x_clean, reduction=reduction)

###############################################################################
#  CONTRASTIVE                                                                #
###############################################################################

def contrastive_loss(
    z_q: torch.Tensor,
    z_pos: torch.Tensor,
    *,
    margin: float = 0.2,
    hard_negatives: bool = True,
) -> torch.Tensor:
    """Triplet loss with negative selection within the batch.

    If `hard_negatives` is True, uses the closest negative; otherwise,
    permutes `z_pos` to obtain a random negative.
    """
    z_q = F.normalize(z_q, p=2, dim=1)
    z_pos = F.normalize(z_pos, p=2, dim=1)

    if hard_negatives:
        dist_mat = torch.cdist(z_q, z_pos, p=2)
        mask = torch.eye(dist_mat.size(0), dtype=torch.bool, device=z_q.device)
        dist_mat = dist_mat.masked_fill(mask, float("inf"))  # ← corrected
        neg_dist, _ = dist_mat.min(dim=1)

    else:
        idx = torch.randperm(z_pos.size(0), device=z_pos.device)
        neg_dist = torch.norm(z_q - z_pos[idx], dim=1)

    pos_dist = torch.norm(z_q - z_pos, dim=1)
    return F.relu(pos_dist - neg_dist + margin).mean()

"""

training/train_cae.py
"""
# training/train_cae.py ― Contrastive Auto-Encoder with negative mining and validation

from __future__ import annotations
import argparse, os, math
from typing import Optional

import torch
from torch.utils.data import DataLoader
from torch.nn.utils import clip_grad_norm_

from data.torch_datasets import EmbeddingTripletDataset
from models.contrastive_autoencoder import ContrastiveAutoencoder
from training.loss_functions import contrastive_loss        # in-batch mining
from utils.load_config import load_config, init_logger
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import prepare_datasets, split_dataset
from dotenv import load_dotenv

# --------------------------------------------------------------------------- #
#  AUX                                                                       #
# --------------------------------------------------------------------------- #

def _build_optimizer(model: torch.nn.Module, lr: float, weight_decay: float) -> torch.optim.Optimizer:
    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

def _build_scheduler(optim: torch.optim.Optimizer, patience: int, factor: float = 0.5):
    # Reduce LR if val_loss does not improve for `patience` consecutive epochs
    return torch.optim.lr_scheduler.ReduceLROnPlateau(
        optim, mode="min", factor=factor, patience=max(1, patience // 2)
    )

# --------------------------------------------------------------------------- #
#  TRAINING LOOP                                                             #
# --------------------------------------------------------------------------- #

def train_cae(
    *,
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    logger,
    hard_negatives: bool = True,
    margin: float = 0.2,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    min_delta: float = 0.003,                   # 0.3% relative improvement
    weight_decay: float = 1e-4,
    clip_grad_norm: float = 1.0,                # 0 = disable
    device: Optional[str] = None,
) -> None:

    device = device or resolve_device()
    log = logger.train if hasattr(logger, "train") else logger

    log.info("CAE | device=%s | hard_negatives=%s | margin=%.3f", device, hard_negatives, margin)

    # ---------------- Dataset ---------------------------
    full_ds = EmbeddingTripletDataset(dataset_path)
    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=True)
    dl_val   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)

    # ---------------- Model & Opt -----------------------
    model = ContrastiveAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = _build_optimizer(model, lr, weight_decay)
    scheduler = _build_scheduler(optim, patience or 4)

    best_val, epochs_no_improve = math.inf, 0

    # Triplet loss native
    triplet_fn = torch.nn.TripletMarginLoss(margin=margin, p=2)

    for epoch in range(1, epochs + 1):
        # ---------------- Train -------------------------
        model.train(); running = 0.0
        for batch in dl_train:
            z_q  = model.encode(batch["q"].to(device))
            z_p  = model.encode(batch["p"].to(device))
            z_n  = model.encode(batch["n"].to(device))

            if hard_negatives:
                loss = contrastive_loss(z_q, z_p, margin=margin, hard_negatives=True)
            else:
                loss = triplet_fn(z_q, z_p, z_n)

            optim.zero_grad()
            loss.backward()
            if clip_grad_norm > 0:
                clip_grad_norm_(model.parameters(), clip_grad_norm)
            optim.step()
            running += loss.item() * z_q.size(0)

        train_loss = running / len(train_ds)

        # ---------------- Validation --------------------
        model.eval(); val_running = 0.0
        with torch.no_grad():
            for batch in dl_val:
                z_q  = model.encode(batch["q"].to(device))
                z_p  = model.encode(batch["p"].to(device))
                z_n  = model.encode(batch["n"].to(device))

                if hard_negatives:
                    vloss = contrastive_loss(z_q, z_p, margin=margin, hard_negatives=True)
                else:
                    vloss = triplet_fn(z_q, z_p, z_n)

                val_running += vloss.item() * z_q.size(0)
        val_loss = val_running / len(val_ds)

        log.info("[Epoch %02d/%d] train=%.6f | val=%.6f", epoch, epochs, train_loss, val_loss)
        scheduler.step(val_loss)

        # ---------------- Early stop --------------------
        rel_improve = (best_val - val_loss) / best_val if best_val < math.inf else 1.0
        if rel_improve > min_delta:
            best_val, epochs_no_improve = val_loss, 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  -> New best val_loss. Checkpoint saved at {model_save_path}")
            logger.train.info("New best val_loss: %.6f → checkpoint %s", best_val, model_save_path)
        else:
            epochs_no_improve += 1
            if patience and epochs_no_improve >= patience:
                print("[EARLY STOP] No improvement in validation.")
                logger.train.info("[EARLY STOP] No improvement in validation.")
                break

    print(f"[DONE] Best val_loss = {best_val:.6f}")
    logger.main.info("[DONE] Best val_loss = %.6f", best_val)
    logger.main.info("")

# --------------------------------------------------------------------------- #
#  CLI                                                                       #
# --------------------------------------------------------------------------- #

if __name__ == "__main__":
    load_dotenv()

    p = argparse.ArgumentParser(description="Train Contrastive Auto-Encoder (CAE)")
    p.add_argument("--config", default="./config/config.yaml")
    p.add_argument("--dataset", choices=["uda", "squad"], help="Override YAML dataset")
    p.add_argument("--epochs",  type=int)
    p.add_argument("--batch_size", type=int)
    p.add_argument("--lr",      type=float)
    p.add_argument("--weight_decay", type=float, default=1e-4)
    p.add_argument("--clip_grad", type=float, default=1.0)
    p.add_argument("--margin",  type=float, default=0.2)
    p.add_argument("--val_split", type=float, default=0.1)
    p.add_argument("--patience", type=int, default=5)
    p.add_argument("--no-hard-negatives", action="store_true")
    p.add_argument("--save_path")
    args = p.parse_args()

    # ---------- Config & logging -----------------------------------------
    cfg       = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg["models"]["contrastive"]
    log       = init_logger(cfg["logging"])

    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False), logger=log.main)

    # ---------- Dataset ---------------------------------------------------
    ds_path = prepare_datasets(cfg, variant="cae", dataset_override=args.dataset)

    # ---------- Hparams final ---------------------------------------------
    hparams = dict(
        dataset_path = ds_path,
        input_dim    = model_cfg.get("input_dim", 384),
        latent_dim   = model_cfg.get("latent_dim", 64),
        hidden_dim   = model_cfg.get("hidden_dim", 512),
        batch_size   = args.batch_size or train_cfg.get("batch_size", 256),
        epochs       = args.epochs or train_cfg.get("epochs", 20),
        lr           = args.lr or float(train_cfg.get("learning_rate", 1e-3)),
        weight_decay = args.weight_decay,
        clip_grad_norm = args.clip_grad,
        margin       = args.margin,
        hard_negatives = not args.no_hard_negatives,
        val_split    = args.val_split,
        patience     = None if args.patience == 0 else args.patience,
        model_save_path = args.save_path or model_cfg.get("checkpoint", "./models/checkpoints/contrastive_ae.pth"),
        logger       = log,
    )

    train_cae(**hparams)

"""

training/train_dae.py
"""
# training/train_dae.py – Denoising Auto‑Encoder con validación y early‑stopping

from __future__ import annotations

import argparse
import os
from typing import Optional

import torch
from torch.utils.data import DataLoader

from data.torch_datasets import EmbeddingDAEDataset
from models.denoising_autoencoder import DenoisingAutoencoder
from training.loss_functions import dae_loss
from utils.load_config import load_config
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import split_dataset, prepare_datasets
from utils.load_config import init_logger
from dotenv import load_dotenv

###############################################################################
#  TRAINING FUNCTION                                                          #
###############################################################################

def train_dae(
    *,
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    logger,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    device: Optional[str] = None,
):
    """Run DAE training/validation loop."""
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Training DAE on {device} | val_split={val_split}")
    logger.main.info("")
    logger.main.info("Training DAE | device=%s", device)

    # ---------------- Dataset --------------------------
    full_ds = EmbeddingDAEDataset(dataset_path)
    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    dl_val = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)

    # ---------------- Model & Optimizer ----------------
    model = DenoisingAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = torch.optim.Adam(model.parameters(), lr=lr)

    best_val = float("inf")
    no_improve = 0

    # ---------------- Training Loop -------------------
    for epoch in range(1, epochs + 1):
        model.train()
        running = 0.0
        for batch in dl_train:
            x_noisy = batch["x"].to(device)
            x_clean = batch["y"].to(device)

            optim.zero_grad()
            x_rec = model(x_noisy)
            loss = dae_loss(x_rec, x_clean, reduction="mean")
            loss.backward()
            optim.step()
            running += loss.item() * x_noisy.size(0)

        train_loss = running / len(train_ds)

        # ---------------- Validation ------------------
        model.eval()
        with torch.no_grad():
            val_running = 0.0
            for batch in dl_val:
                x_noisy = batch["x"].to(device)
                x_clean = batch["y"].to(device)
                x_rec = model(x_noisy)
                vloss = dae_loss(x_rec, x_clean, reduction="mean")
                val_running += vloss.item() * x_noisy.size(0)
            val_loss = val_running / len(val_ds)

        print(
            f"[Epoch {epoch:02d}/{epochs}] train_loss={train_loss:.6f} | val_loss={val_loss:.6f}"
        )
        logger.train.info(
            "[Epoch %02d/%d] train=%.6f | val=%.6f", epoch, epochs, train_loss, val_loss
        )

        # ---------------- Early Stopping --------------
        if val_loss < best_val - 1e-4:
            best_val = val_loss
            no_improve = 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  -> New best val_loss. Checkpoint saved at {model_save_path}")
            logger.train.info("New best val_loss: %.6f → checkpoint %s", best_val, model_save_path)
        else:
            no_improve += 1
            if patience and no_improve >= patience:
                print("[EARLY STOP] No improvement in validation.")
                logger.train.info("[EARLY STOP] No improvement in validation.")
                break

    print(f"[DONE] Best val_loss = {best_val:.6f}")
    logger.main.info("[DONE] Best val_loss = %.6f", best_val)
    logger.main.info("")

###############################################################################
#  CLI                                                                        #
###############################################################################

if __name__ == "__main__":
    load_dotenv()

    parser = argparse.ArgumentParser(description="Train Denoising Auto‑Encoder (DAE)")
    parser.add_argument("--config", default="./config/config.yaml")
    parser.add_argument("--dataset", choices=["uda", "squad"], help="Override YAML dataset")
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--save_path")
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--val_split", type=float, default=0.1)
    parser.add_argument("--patience", type=int, default=5)
    args = parser.parse_args()

    # ---------------- Config & logging ----------------
    cfg = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg.get("models", {}).get("dae", {})
    log = init_logger(cfg["logging"])

    # ---------------- Reproducibility ----------------
    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False), logger=log.train)
    device = resolve_device(train_cfg.get("device"))

    # ---------------- Dataset prep -------------------
    dataset_path = prepare_datasets(cfg, variant="dae", dataset_override=args.dataset)

    # ---------------- Training -----------------------
    train_dae(
        dataset_path=dataset_path,
        input_dim=model_cfg.get("input_dim", 384),
        latent_dim=model_cfg.get("latent_dim", 64),
        hidden_dim=model_cfg.get("hidden_dim", 512),
        batch_size=args.batch_size or train_cfg.get("batch_size", 256),
        epochs=args.epochs or train_cfg.get("epochs", 20),
        lr=args.lr if args.lr is not None else float(train_cfg.get("learning_rate", 1e-3)),
        model_save_path=args.save_path or model_cfg.get("checkpoint", "./models/checkpoints/dae_text.pth"),
        val_split=args.val_split,
        patience=None if args.patience == 0 else args.patience,
        device=device,
        logger=log,
    )

"""

training/train_vae.py
"""
# training/train_vae.py – Variational Auto‑Encoder con validación y early‑stopping

import argparse
import os
from typing import Optional

import torch
from torch.utils.data import DataLoader

from data.torch_datasets import EmbeddingVAEDataset
from models.variational_autoencoder import VariationalAutoencoder
from training.loss_functions import vae_loss
from utils.load_config import load_config, init_logger
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import prepare_datasets
from dotenv import load_dotenv

###############################################################################
#  TRAINING LOOP                                                             #
###############################################################################

def train_vae(
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    device: Optional[str] = None,
):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Training VAE on {device} | val_split={val_split}")

    full_ds = EmbeddingVAEDataset(dataset_path)
    from utils.data_utils import split_dataset  # local import to avoid circular

    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    dl_val   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)

    model = VariationalAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = torch.optim.Adam(model.parameters(), lr=lr)

    best_val, no_improve = float("inf"), 0
    for epoch in range(1, epochs + 1):
        # ---------------- train ------------------
        model.train(); running = 0.0
        for batch in dl_train:
            x_in  = batch["input"].to(device)
            x_tar = batch["target"].to(device)
            optim.zero_grad()
            x_rec, mu, logvar = model(x_in)
            loss = vae_loss(x_rec, x_tar, mu, logvar, mse_reduction="mean")
            loss.backward(); optim.step()
            running += loss.item() * x_in.size(0)
        train_loss = running / len(train_ds)

        # ---------------- validation -------------
        model.eval(); val_running = 0.0
        with torch.no_grad():
            for batch in dl_val:
                x_in  = batch["input"].to(device)
                x_tar = batch["target"].to(device)
                x_rec, mu, logvar = model(x_in)
                vloss = vae_loss(x_rec, x_tar, mu, logvar, mse_reduction="mean")
                val_running += vloss.item() * x_in.size(0)
        val_loss = val_running / len(val_ds)

        print(f"[Epoch {epoch:02d}/{epochs}] train={train_loss:.6f} | val={val_loss:.6f}")

        if val_loss < best_val - 1e-4:
            best_val, no_improve = val_loss, 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
        else:
            no_improve += 1
            if patience and no_improve >= patience:
                print("[EARLY STOP] No improvement in validation."); break

    print(f"[DONE] best_val_loss = {best_val:.6f}")

###############################################################################
#  CLI                                                                       #
###############################################################################

if __name__ == "__main__":
    load_dotenv()

    parser = argparse.ArgumentParser(description="Train Variational Auto‑Encoder (VAE)")
    parser.add_argument("--config", default="./config/config.yaml")
    parser.add_argument("--dataset", choices=["uda", "squad"], help="Override dataset in config.yaml")
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--save_path")
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--val_split", type=float, default=0.1)
    parser.add_argument("--patience", type=int, default=5)
    args = parser.parse_args()

    # ------------- config & logging -------------
    cfg       = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg.get("models", {}).get("vae", {})
    log       = init_logger(cfg["logging"])

    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False))
    device = resolve_device(train_cfg.get("device"))

    # ------------- dataset paths -----------------
    dataset_path = prepare_datasets(cfg, variant="vae", dataset_override=args.dataset)

    # ------------- training ----------------------
    train_vae(
        dataset_path=dataset_path,
        input_dim=model_cfg.get("input_dim", 384),
        latent_dim=model_cfg.get("latent_dim", 64),
        hidden_dim=model_cfg.get("hidden_dim", 512),
        batch_size=args.batch_size or train_cfg.get("batch_size", 256),
        epochs=args.epochs or train_cfg.get("epochs", 20),
        lr=float(args.lr) if args.lr is not None else float(train_cfg.get("learning_rate", 1e-3)),
        model_save_path=args.save_path or model_cfg.get("checkpoint", "./models/checkpoints/vae_text.pth"),
        val_split=args.val_split,
        patience=None if args.patience == 0 else args.patience,
        device=device,
    )

"""

utils/data_utils.py
"""
 # /utils/data_utils.py
from __future__ import annotations
import os
from typing import List, Tuple, Optional

import torch
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm
import random
from torch.utils.data import Subset

from pathlib import Path
from typing import Dict, Optional

def _compute_embeddings(
    texts: List[str],
    model: SentenceTransformer,
    batch_size: int = 64,
) -> torch.Tensor:
    """Devuelve un tensor CPU float32 [N × D] con los CLS-embeddings."""
    chunks: List[torch.Tensor] = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding"):
        batch = texts[i : i + batch_size]
        with torch.no_grad():
            emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
            chunks.append(torch.from_numpy(emb))
    return torch.cat(chunks, dim=0).float()

def _jaccard_sim(a: str, b: str) -> float:
    a_set = set(a.lower().split())
    b_set = set(b.lower().split())
    inter = a_set & b_set
    union = a_set | b_set
    return len(inter) / len(union) if union else 0.0

def ensure_uda_data(
    *,
    output_dir: str = "./data/",
    max_samples: Optional[int] = None,
    base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    noise_std: float = 0.05,
    force: bool = False,
) -> None:
    """Genera (o reutiliza) los ficheros de embeddings para VAE, DAE y contraste."""
    os.makedirs(output_dir, exist_ok=True)

    vae_path = os.path.join(output_dir, "uda_vae_embeddings.pt")
    dae_path = os.path.join(output_dir, "uda_dae_embeddings.pt")
    contrastive_path = os.path.join(output_dir, "uda_contrastive_embeddings.pt")

    if (
        not force
        and os.path.exists(vae_path)
        and os.path.exists(dae_path)
        and os.path.exists(contrastive_path)
    ):
        print("[INFO] UDA embeddings ya preparados — nada que hacer.")
        return

    print("[INFO] Descargando / cargando UDA…")
    uda = load_dataset("qinchuanhui/UDA-QA", "nq")
    if max_samples is not None:
        uda = uda.select(range(min(max_samples, len(uda))))
    print(f"[INFO] UDA listo con {len(uda):,} ejemplos.")

    clean_texts: List[str] = []
    contrastive_triples: List[Tuple[str, str, str]] = []

    for i, ex in enumerate(uda["test"]): # TEMPORAL ******************************************************* CAMBIAR URGENTEMENTE
        q = ex.get("question", "").strip()
        pos = ex.get("long_answer", "").strip()
        if not q or not pos:
            continue

        neg = None
        for _ in range(10):
            j = random.randint(0, len(uda["test"]) - 1)
            if j == i:
                continue
            neg_cand = uda["test"][j].get("long_answer", "").strip()
            if not neg_cand:
                continue
            if _jaccard_sim(q, neg_cand) < 0.1:
                neg = neg_cand
                break

        if neg is None:
            continue

        clean_texts.extend((q, pos))             # query + positive answer
        contrastive_triples.append((q, pos, neg))


    print(f"[INFO] Tripletas contrastivas generadas: {len(contrastive_triples):,}")

    print(f"[INFO] Cargando SentenceTransformer '{base_model_name}' …")
    st_model = SentenceTransformer(base_model_name)

    print("[INFO] Generando embeddings VAE/DAE (positivos)…")
    target_emb = _compute_embeddings(clean_texts, st_model)

    if force or not os.path.exists(vae_path):
        torch.save({"input": target_emb, "target": target_emb.clone()}, vae_path)
        print(f"[OK]  VAE embeddings guardados → {vae_path}")

    if force or not os.path.exists(dae_path):
        input_emb = target_emb + torch.randn_like(target_emb) * noise_std
        torch.save({"input": input_emb, "target": target_emb}, dae_path)
        print(f"[OK]  DAE embeddings guardados → {dae_path}")

    if force or not os.path.exists(contrastive_path):
        print("[INFO] Generando embeddings de triples (query/pos/neg)…")
        qs, ps, ns = zip(*contrastive_triples)
        q_emb = _compute_embeddings(list(qs), st_model)
        p_emb = _compute_embeddings(list(ps), st_model)
        n_emb = _compute_embeddings(list(ns), st_model)
        torch.save({"query": q_emb, "positive": p_emb, "negative": n_emb}, contrastive_path)
        print(f"[OK]  Contrastive embeddings guardados → {contrastive_path}")

    print("[DONE] Preprocesado de UDA completo.")

def split_dataset(dataset: torch.utils.data.Dataset, val_split: float = 0.1, seed: int = 42) -> Tuple[Subset, Subset]:
    n_total = len(dataset)
    idx = list(range(n_total))
    random.Random(seed).shuffle(idx)
    n_val = int(n_total * val_split)
    val_idx = idx[:n_val]
    train_idx = idx[n_val:]
    return Subset(dataset, train_idx), Subset(dataset, val_idx)


def ensure_squad_data(
    *,
    output_dir: str = "./data",
    max_samples: Optional[int] = None,
    base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    noise_std: float = 0.05,
    include_unanswerable: bool = False,
    force: bool = False,
) -> None:
    """Generate VAE / DAE / CAE embedding tensors from **SQuAD v1/v2**.

    The tensors are stored on disk using the same structure as the UDA helper,
    so training scripts remain unchanged.  Filenames:

        squad_vae_embeddings.pt
        squad_dae_embeddings.pt
        squad_contrastive_embeddings.pt
    """
    os.makedirs(output_dir, exist_ok=True)

    vae_path         = os.path.join(output_dir, "squad_vae_embeddings.pt")
    dae_path         = os.path.join(output_dir, "squad_dae_embeddings.pt")
    contrastive_path = os.path.join(output_dir, "squad_contrastive_embeddings.pt")

    # --------------------------------------------------------------------- #
    #  Early exit if everything is already cached                           #
    # --------------------------------------------------------------------- #
    if (
        not force
        and all(os.path.exists(p) for p in (vae_path, dae_path, contrastive_path))
    ):
        print("[INFO] SQuAD embeddings already prepared nothing to do.")
        return

    # --------------------------------------------------------------------- #
    #  1. Load SQuAD                                                        #
    # --------------------------------------------------------------------- #
    ds_name = "squad_v2" if include_unanswerable else "squad"
    print(f"[INFO] Loading {ds_name} …")
    squad = load_dataset(ds_name, split="train")
    if max_samples is not None:
        squad = squad.select(range(min(max_samples, len(squad))))
    print(f"[INFO] SQuAD loaded with {len(squad):,} examples.")

    # --------------------------------------------------------------------- #
    #  2. Build positive contexts and contrastive triples                   #
    # --------------------------------------------------------------------- #
    clean_texts: List[str] = []
    contrastive_triples: List[Tuple[str, str, str]] = []

    for i, ex in enumerate(squad):
        q   = ex["question"].strip()
        ctx = ex["context"].strip()
        if not q or not ctx:
            continue

        # ----------------- simple negative mining ------------------------ #
        neg = None
        for _ in range(10):
            j = random.randint(0, len(squad) - 1)
            if j == i:
                continue
            neg_ctx = squad[j]["context"].strip()
            if neg_ctx and _jaccard_sim(q, neg_ctx) < 0.1:
                neg = neg_ctx
                break
        if neg is None:
            continue

        clean_texts.extend([q, ctx])              # query + positive context
        contrastive_triples.append((q, ctx, neg))

    print(f"[INFO] Contrastive triples generated: {len(contrastive_triples):,}")

    # --------------------------------------------------------------------- #
    #  3. Encode with SBERT                                                 #
    # --------------------------------------------------------------------- #
    print(f"[INFO] Loading SBERT '{base_model_name}' …")
    st_model = SentenceTransformer(base_model_name)

    print("[INFO] Encoding queries + positive contexts …")
    target_emb = _compute_embeddings(clean_texts, st_model)   # shape [N × D]

    # --------------------------------------------------------------------- #
    #  4. Save VAE and DAE variants                                         #
    # --------------------------------------------------------------------- #
    if force or not os.path.exists(vae_path):
        torch.save({"input": target_emb, "target": target_emb.clone()}, vae_path)
        print(f"[OK]  VAE embeddings   → {vae_path}")

    if force or not os.path.exists(dae_path):
        input_emb = target_emb + torch.randn_like(target_emb) * noise_std
        torch.save({"input": input_emb, "target": target_emb}, dae_path)
        print(f"[OK]  DAE embeddings   → {dae_path}")

    # --------------------------------------------------------------------- #
    #  5. Save contrastive triplets                                         #
    # --------------------------------------------------------------------- #
    if force or not os.path.exists(contrastive_path):
        print("[INFO] Encoding triplets …")
        qs, ps, ns = zip(*contrastive_triples)
        q_emb = _compute_embeddings(list(qs), st_model)
        p_emb = _compute_embeddings(list(ps), st_model)
        n_emb = _compute_embeddings(list(ns), st_model)
        torch.save(
            {"query": q_emb, "positive": p_emb, "negative": n_emb},
            contrastive_path,
        )
        print(f"[OK]  Contrastive embeddings → {contrastive_path}")

    print("[DONE] SQuAD preprocessing finished.")



def _prepare_uda(cfg: dict) -> Dict[str, str]:
    common = dict(
        output_dir="./data/UDA",
        max_samples=cfg["data"].get("max_samples"),
        base_model_name=cfg["embedding_model"]["name"],
        force=False,
    )
    ensure_uda_data(**common)
    return {
        "vae": "./data/UDA/uda_vae_embeddings.pt",
        "dae": "./data/UDA/uda_dae_embeddings.pt",
        "cae": "./data/UDA/uda_contrastive_embeddings.pt",
    }


def _prepare_squad(cfg: dict) -> Dict[str, str]:
    data_cfg = cfg["data"]
    common = dict(
        output_dir="./data/SQUAD/",
        max_samples=data_cfg.get("max_samples"),
        base_model_name=cfg["embedding_model"]["name"],
        noise_std=0.05,
        include_unanswerable=data_cfg.get("include_unanswerable", False),
        force=False,
    )
    ensure_squad_data(**common)
    return {
        "vae": "./data/SQUAD/squad_vae_embeddings.pt",
        "dae": "./data/SQUAD/squad_dae_embeddings.pt",
        "cae": "./data/SQUAD/squad_contrastive_embeddings.pt",
    }


def prepare_datasets(
    cfg: dict,
    *,
    variant: str,
    dataset_override: Optional[str] = None,
) -> str:
    """Ensure dataset tensors exist and return path for requested variant.

    Args:
        cfg: Parsed YAML config dict (must include data and embedding_model).
        variant: One of `{"vae", "dae", "cae"}.
        dataset_override: If provided, forces `"uda" or "squad"

    Returns:
        The filesystem path to the tensor file corresponding to the *variant*.
    """
    variant = variant.lower()
    assert variant in {"vae", "dae", "cae"}, "variant must be vae, dae or cae"

    ds_name = (dataset_override or cfg.get("data", {}).get("dataset", "squad")).lower()
    if ds_name == "squad":
        paths = _prepare_squad(cfg)
    elif ds_name == "uda":
        paths = _prepare_uda(cfg)
    else:
        raise ValueError(f"Unknown dataset: {ds_name}")

    path = paths[variant]
    if not Path(path).exists():
        raise FileNotFoundError(f"Expected dataset file not found: {path}")
    return path




def load_eval_queries_from_squad(
    version: str = "v1",
    split: str = "validation",
    max_samples: Optional[int] = None,
    dedup: bool = True,
) -> Tuple[List[str], List[str], List[List[str]]]:
    """
    Prepara triples (queries, corpus, relevantes) para evaluación de retrieval.

    Args:
        version: "v1" o "v2"
        split: "train" o "validation"
        max_samples: límite de queries
        dedup: si True, elimina contextos repetidos del corpus

    Returns:
        queries, corpus, relevant_docs (1 a 1 con queries)
    """
    ds_name = "squad_v2" if version == "v2" else "squad"
    ds = load_dataset(ds_name, split=split)

    queries, contexts, relevant = [], [], []

    for ex in ds:
        q = ex["question"].strip()
        c = ex["context"].strip()

        # descartar preguntas sin respuesta si es v2
        if version == "v2":
            has_answer = bool(ex["answers"]["answer_start"])
            if not has_answer:
                continue

        queries.append(q)
        contexts.append(c)
        relevant.append([c])  # relevante = ese contexto

        if max_samples and len(queries) >= max_samples:
            break

    corpus = list(set(contexts)) if dedup else contexts
    return queries, corpus, relevant


def load_evaluation_data(dataset: str, max_samples: int = 200):
    if dataset == "squad":
        return load_eval_queries_from_squad(
            version="v1", split="validation", max_samples=max_samples
        )
    elif dataset == "uda":
        raise NotImplementedError("TODO: soporte UDA")
    else:
        raise ValueError(f"Dataset desconocido: {dataset}")
"""

utils/load_config.py
"""
import numpy as np
import yaml
import os, sys
from pathlib import Path
from types import SimpleNamespace   
import logging

def load_config(path: str) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def init_logger(cfg_logging: dict) -> SimpleNamespace:

    if cfg_logging.get("log_to_file", False):
        Path(cfg_logging["log_file"]).parent.mkdir(parents=True, exist_ok=True)

    handlers = [logging.StreamHandler(sys.stdout)]
    if cfg_logging.get("log_to_file", False):
        handlers.append(logging.FileHandler(cfg_logging["log_file"], encoding="utf-8"))

    logging.basicConfig(
        level=getattr(logging, cfg_logging.get("level", "INFO")),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        handlers=handlers,
        force=True,                        
    )

    return SimpleNamespace(
        main = logging.getLogger("main"),
        train = logging.getLogger("train"),
        utils = logging.getLogger("utils"),
    )
"""

utils/training_utils.py
"""
# utils/training_utils.py
import os, random, logging
import numpy as np
import torch

def set_seed( seed: int, deterministic: bool = False, logger: logging.Logger | None = None ) -> None:
    """
    Fija todas las semillas y el modo determinista de cuDNN.

    Args:
        seed (int): valor de la semilla.
        deterministic (bool): True → reproducibilidad completa
                              (más lento en GPU).
        logger (logging.Logger | None): instancia de logger principal;
                                        si es None se usa el del módulo.
    """
    logger = logger or logging.getLogger(__name__)

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.deterministic = deterministic
    torch.backends.cudnn.benchmark     = not deterministic
    torch.use_deterministic_algorithms(deterministic)

    if deterministic:
        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
        logger.info("cuDNN deterministic mode  ACTIVE (desactivated benchmark mode)")
    else:
        logger.info("cuDNN benchmark mode ACTIVE (desactivated deterministic mode)")


def resolve_device(device_str: str | None = None) -> str:
    if device_str is not None:
        return device_str
    return "cuda" if torch.cuda.is_available() else "cpu"

"""

