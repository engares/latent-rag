### Directory tree for: .

./
├── AUTORAG.code-workspace
├── README.md
├── config/
│   ├── config.yaml
│   └── prompts/
│       └── system_prompt.txt
├── data/
│   ├── data_processing.py
│   ├── squad_contrastive_embeddings.pt
│   ├── squad_dae_embeddings.pt
│   ├── squad_loader.py
│   ├── squad_vae_embeddings.pt
│   ├── torch_datasets.py
│   ├── uda_contrastive_embeddings.pt
│   ├── uda_dae_embeddings.pt
│   └── uda_vae_embeddings.pt
├── evaluation/
│   ├── autoencoder_metrics.py
│   ├── generation_metrics.py
│   └── retrieval_metrics.py
├── generation/
│   └── generator.py
├── logs/
│   └── run.log
├── main.py
├── models/
│   ├── base_autoencoder.py
│   ├── checkpoints/
│   │   ├── contrastive_ae.pth
│   │   ├── dae_text.pth
│   │   └── vae_text.pth
│   ├── contrastive_autoencoder.py
│   ├── denoising_autoencoder.py
│   ├── tests.ipynb
│   └── variational_autoencoder.py
├── requeriments.txt
├── retrieval/
│   ├── embedder.py
│   └── retriever.py
├── save_snapshot.sh*
├── snapshot.txt
├── style_guide.md
├── test/
│   ├── test_evaluation.py
│   ├── test_loss_functions.py
│   ├── test_models.py
│   ├── test_retrieval.py
│   └── test_train_scripts.py
├── training/
│   ├── loss_functions.py
│   ├── train_cae.py
│   ├── train_dae.py
│   └── train_vae.py
└── utils/
    ├── data_utils.py
    ├── load_config.py
    └── training_utils.py

12 directories, 45 files


### File contents

config/config.yaml
"""
project:
  name: rag_autoencoder_tfm
  version: "0.1"

paths:
  data_dir: "./data"
  checkpoints_dir: "./models/checkpoints"
  logs_dir: "./logs"

embedding_model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  max_length: 256

models:
  vae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/uda_vae_embeddings.pt"          
    checkpoint: "./models/checkpoints/vae_text.pth"

  dae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/uda_dae_embeddings.pt"          
    checkpoint: "./models/checkpoints/dae_text.pth"

  contrastive:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/uda_contrastive_embeddings.pt"  
    checkpoint: "./models/checkpoints/contrastive_ae.pth"

data:
  dataset: "squad"            #  "uda"  |  "squad"
  version: "v1"          #  v1, v2  ONly for squad
  max_samples:         #  optional, blankk to use all samples
  include_unanswerable: false   #  Only Squad

training:
  batch_size: 128
  epochs: 50
  learning_rate: 1e-3
  seed: 42
  device: "cuda"  # "cpu"
  deterministic: false      # (true = modo debug)

retrieval:
  similarity_metric: "cosine"   # cosine, mahalanobis
  top_k: 20
  compress_embeddings: true 

generation:
  provider: "openai"       # Just OpenAI by now
  model: "gpt-4o-mini"
  temperature: 0.3
  max_tokens: 256
  system_prompt_path: "./config/prompts/system_prompts.txt"


evaluation:
  retrieval_metrics: ["Recall@5", "MRR@10", "nDCG@10"] # You can change the k top  simply by changing the @k e.j Recall@10, "MRR@20"
  generation_metrics: ["ROUGE-L", "BLEU", "METEOR"]

logging:
  level: "INFO"
  log_to_file: true
  log_file: "./logs/run.log"

"""

config/prompts/system_prompt.txt
"""
Here is the user query and relevant text chunks. Step 1: Summarize user question in simpler words. Step 2: Decide which retrieved text chunks directly apply. Step 3: Combine those chunks into an outline. Step 4: Draft a single, coherent answer. Show all steps, then provide a final refined answer.
"""

data/data_processing.py
"""
#/data_processing.py

import random
import json
import re
from typing import List, Dict
from datasets import load_dataset

# ---------------------------------------------------------
# UDA Dataset Preprocessing for Autoencoder Training
# ---------------------------------------------------------
# Supports: Denoising AE (with artificial noise), VAE, Contrastive AE
# ---------------------------------------------------------

def clean_text(text: str) -> str:
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def add_noise(text: str, removal_prob=0.1, swap_prob=0.05) -> str:
    words = text.split()
    # Remove tokens
    words = [w for w in words if random.random() > removal_prob]
    # Swap nearby tokens
    for i in range(len(words)-1):
        if random.random() < swap_prob:
            words[i], words[i+1] = words[i+1], words[i]
    return " ".join(words)

def build_dae_dataset(samples: List[str]) -> List[Dict[str, str]]:
    dataset = []
    for original in samples:
        noisy = add_noise(original)
        dataset.append({"input": noisy, "target": original})
    return dataset

def build_contrastive_pairs(dataset, max_negatives=1) -> List[Dict]:
    pairs = []
    for example in dataset:
        q = example["query"]
        pos = example["positive_passages"][0]["text"]
        negs = [n["text"] for n in example["negative_passages"][:max_negatives]]
        for neg in negs:
            pairs.append({"query": q, "positive": pos, "negative": neg})
    return pairs

def load_uda(split="train", max_samples=5000):
    print("[INFO] Loading UDA benchmark dataset...")
    uda = load_dataset("osunlp/uda", split=split)
    return uda.select(range(min(max_samples, len(uda))))

if __name__ == "__main__":
    # Load base data
    data = load_uda(split="train", max_samples=1000)
    texts = [clean_text(p["text"]) for row in data for p in row["positive_passages"][:1]]

    # Generate DAE data
    dae_data = build_dae_dataset(texts)
    with open("./data/uda_dae_train.jsonl", "w", encoding="utf-8") as f:
        for item in dae_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    # Generate contrastive pairs
    contrastive = build_contrastive_pairs(data, max_negatives=1)
    with open("./data/uda_contrastive_train.jsonl", "w", encoding="utf-8") as f:
        for item in contrastive:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print("[INFO] Dataset files written to ./data/")

"""

data/squad_loader.py
"""
from __future__ import annotations
from typing import Dict, List, Optional
from datasets import load_dataset

__all__ = [
    "SquadExample",
    "load_squad",
]

SquadExample = Dict[str, str]  # keys: question, context, answer


# ---------------------------------------------------------------------------
# Loader
# ---------------------------------------------------------------------------

def load_squad(
    *,
    version: str = "v1",  # "v1" | "v2"
    split: str = "train",  # "train" | "validation"
    max_samples: Optional[int] = None,
    keep_unanswerable: bool = False,
) -> List[SquadExample]:
    """Return a list of canonical SQuAD examples.

    Args:
        version: Which dataset variant to use.  "v2" contains unanswerable
            questions.  "v1" provides only answerable ones.
        split: Dataset split to load.  Must be either "train" or "validation".
        max_samples: Optional hard cap on the number of examples returned.  Use
            this for quick prototyping.
        keep_unanswerable: If *False* (default) and *version == 'v2'*, examples
            without an answer are filtered out.

    Returns:
        A list of dicts with keys ``question``, ``context`` and ``answer`` (the
        first answer string provided by the annotation set).
    """

    if version not in {"v1", "v2"}:
        raise ValueError("version must be 'v1' or 'v2'")
    if split not in {"train", "validation"}:
        raise ValueError("split must be 'train' or 'validation'")

    hf_name = "squad_v2" if version == "v2" else "squad"
    ds = load_dataset(hf_name, split=split)

    examples: List[SquadExample] = []
    for ex in ds:
        question = ex["question"].strip()
        context = ex["context"].strip()

        # v2 contains lists of possible answers + a boolean flag
        if version == "v2":
            if not ex["answers"]["answer_start"] and not keep_unanswerable:
                # skip no annotated answer 
                continue
            answer = ex["answers"]["text"][0] if ex["answers"]["text"] else ""
        else:  # v1
            answer = ex["answers"][0]["text"]

        examples.append({"question": question, "context": context, "answer": answer})

    if max_samples is not None:
        examples = examples[: max_samples]

    return examples

"""

data/torch_datasets.py
"""
# /data/torch_datasets.py
import torch
from torch.utils.data import Dataset
from typing import Dict, List, Tuple


# ---------- UTILIDADES COMUNES ------------------------------------------------
def _load_pt(path: str) -> Dict[str, torch.Tensor]:
    """
    Carga un fichero .pt con tensores y asegura dtype = float32 en CPU.
    El fichero se espera como un dict { name: Tensor }.
    """
    data = torch.load(path, map_location="cpu")
    return {k: v.float() for k, v in data.items()}


# ---------- DATASETS ---------------------------------------------------------


class EmbeddingVAEDataset(Dataset):
    """
    Carga el fichero .pt generado por `ensure_uda_data`.
    Estructura esperada:
        {"input": <tensor [N×D]>, "target": <tensor [N×D]>}
    """
    def __init__(self, path: str):
        data = torch.load(path, map_location="cpu")
        self.input  = data["input"].float()
        self.target = data["target"].float()
        assert self.input.shape == self.target.shape, "input/target tamaño desigual"

    def __len__(self):
        return self.input.size(0)

    def __getitem__(self, idx):
        return {
            "input":  self.input[idx],
            "target": self.target[idx],
        }


class EmbeddingDAEDataset(Dataset):
    """
    Carga 'uda_dae_embeddings.pt' producido por `ensure_uda_data`.

    Estructura:
        {
            "input":  Tensor [N × D]  (embeddings con ruido)
            "target": Tensor [N × D]  (embeddings limpios)
        }
    """
    def __init__(self, path: str):
        d = torch.load(path, map_location="cpu")
        self.x  = d["input" ].float()
        self.y  = d["target"].float()
        assert self.x.shape == self.y.shape, "Input / target mismatch"

    def __len__(self):          return self.x.size(0)
    def __getitem__(self, idx): return {"x": self.x[idx], "y": self.y[idx]}

    

class EmbeddingTripletDataset(Dataset):
    """
    Carga 'uda_contrastive_embeddings.pt' generado por `ensure_uda_data`.

    Estructura esperada:
        {
            "query":     Tensor [N × D],
            "positive":  Tensor [N × D],
            "negative":  Tensor [N × D]
        }
    """
    def __init__(self, path: str):
        data = torch.load(path, map_location="cpu")
        self.q  = data["query"].float()
        self.p  = data["positive"].float()
        self.n  = data["negative"].float()
        assert self.q.shape == self.p.shape == self.n.shape, "Dimensiones incompatibles"

    def __len__(self) -> int:          return self.q.size(0)

    def __getitem__(self, idx):        # devuelvo tensores individuales
        return {"q": self.q[idx],
                "p": self.p[idx],
                "n": self.n[idx]}


# ---------- PRUEBA RÁPIDA -----------------------------------------------------
if __name__ == "__main__":
    dae_ds = EmbeddingDAEDataset("./data/squad_dae_embeddings.pt")
    vae_ds = EmbeddingDAEDataset("./data/squad_vae_embeddings.pt")
    con_ds = EmbeddingTripletDataset("./data/squad_contrastive_embeddings.pt")

    print("DAE sample ⇒", {k: v.shape for k, v in dae_ds[0].items()})
    print("Contrastive sample ⇒", {k: v.shape for k, v in con_ds[0].items()})
    print("VAE sample ⇒", {k: v.shape for k, v in vae_ds[0].items()})

"""

evaluation/autoencoder_metrics.py
"""
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from typing import Tuple


def evaluate_reconstruction_loss(x: torch.Tensor, x_reconstructed: torch.Tensor, reduction: str = "mean") -> float:
    """Calcula el error de reconstrucción (MSE)."""
    loss_fn = torch.nn.MSELoss(reduction=reduction)
    return loss_fn(x_reconstructed, x).item()

def visualize_embeddings(embeddings: torch.Tensor, labels: torch.Tensor = None, title: str = "Embeddings Visualization"):
    """Proyección 2D de los embeddings usando t-SNE."""
    tsne = TSNE(n_components=2, random_state=42)
    emb_2d = tsne.fit_transform(embeddings.cpu().numpy())

    plt.figure(figsize=(8, 6))
    if labels is not None:
        sns.scatterplot(x=emb_2d[:, 0], y=emb_2d[:, 1], hue=labels.cpu().numpy(), palette="tab10")
    else:
        sns.scatterplot(x=emb_2d[:, 0], y=emb_2d[:, 1])

    plt.title(title)
    plt.show()
"""

evaluation/generation_metrics.py
"""
from __future__ import annotations

"""Métricas de generación con *bootstrap* y prueba de significancia emparejada.

Uso principal:
--------------
>>> mean_ci = evaluate_generation_bootstrap(refs, cands, metrics=["BLEU", "ROUGE-L"])
>>> pval = paired_bootstrap_test(refs, sys_a, sys_b, metric="BLEU")
"""

from collections.abc import Callable
from typing import List, Dict, Tuple
import numpy as np
from sacrebleu.metrics import BLEU as _BLEUMetric
from rouge_score import rouge_scorer
import random

###############################################################################
#  MÉTRICAS BÁSICAS                                                           #
###############################################################################

_bleu = _BLEUMetric()
_scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)


def compute_bleu(candidates: List[str], references: List[str]) -> float:
    """BLEU corpus‐level sacreBLEU (0‑100)."""
    return _bleu.corpus_score(candidates, [references]).score


def compute_rouge_l(candidates: List[str], references: List[str]) -> float:
    """Promedio de ROUGE‑L (F1) ×100."""
    scores = [
        _scorer.score(ref, cand)["rougeL"].fmeasure * 100
        for ref, cand in zip(references, candidates)
    ]
    return float(np.mean(scores))


_metric_fn: Dict[str, Callable[[List[str], List[str]], float]] = {
    "BLEU": compute_bleu,
    "ROUGE-L": compute_rouge_l,
}

###############################################################################
#  BOOTSTRAP                                                                  #
###############################################################################

def _bootstrap_ci(
    func: Callable[[List[str], List[str]], float],
    refs: List[str],
    cands: List[str],
    n_samples: int = 2000,
    alpha: float = 0.05,
    seed: int | None = None,
) -> Tuple[float, float, float]:
    """Devuelve media, límite inferior y superior del IC al (1‑alpha)."""
    if seed is not None:
        random.seed(seed)
    N = len(refs)
    stats = []
    for _ in range(n_samples):
        idx = [random.randint(0, N - 1) for _ in range(N)]
        stats.append(func([cands[i] for i in idx], [refs[i] for i in idx]))
    stats_np = np.array(stats)
    mean = stats_np.mean()
    lower = np.percentile(stats_np, 100 * alpha / 2)
    upper = np.percentile(stats_np, 100 * (1 - alpha / 2))
    return mean, lower, upper


def evaluate_generation_bootstrap(
    references: List[str],
    candidates: List[str],
    metrics: List[str] | None = None,
    n_samples: int = 2000,
    alpha: float = 0.05,
    seed: int | None = None,
) -> Dict[str, Dict[str, float]]:
    """Calcula métricas + IC al 95 % mediante bootstrap.

    Retorna: {metric: {"mean": m, "ci_lower": l, "ci_upper": u}}
    """
    if metrics is None:
        metrics = ["BLEU", "ROUGE-L"]

    assert len(references) == len(candidates) >= 30, (
        "Se requieren al menos 30 pares ref‑cand para un IC mínimo; se recomienda ≥1000."
    )

    results: Dict[str, Dict[str, float]] = {}
    for m in metrics:
        if m not in _metric_fn:
            raise ValueError(f"Métrica '{m}' no soportada.")
        mean, lo, hi = _bootstrap_ci(_metric_fn[m], references, candidates, n_samples, alpha, seed)
        results[m] = {"mean": mean, "ci_lower": lo, "ci_upper": hi}
    return results

###############################################################################
#  PAIRED BOOTSTRAP SIGNIFICANCE TEST                                         #
###############################################################################

def paired_bootstrap_test(
    references: List[str],
    sys_a: List[str],
    sys_b: List[str],
    metric: str = "BLEU",
    n_samples: int = 10000,
    seed: int | None = None,
) -> Dict[str, float]:
    """Prueba de significancia emparejada (bootstrap) entre dos sistemas.

    Devuelve un dict: {"diff_mean": d, "ci_lower": lo, "ci_upper": hi, "p_value": p}
    p‑value ≈ proporción de muestras con diferencia ≤0 (o ≥0, según el signo).
    """
    assert len(references) == len(sys_a) == len(sys_b)
    if seed is not None:
        random.seed(seed)

    if metric not in _metric_fn:
        raise ValueError(f"Métrica '{metric}' no soportada.")
    fn = _metric_fn[metric]

    N = len(references)
    diffs = []
    for _ in range(n_samples):
        idx = [random.randint(0, N - 1) for _ in range(N)]
        a_score = fn([sys_a[i] for i in idx], [references[i] for i in idx])
        b_score = fn([sys_b[i] for i in idx], [references[i] for i in idx])
        diffs.append(a_score - b_score)

    diffs_np = np.array(diffs)
    diff_mean = diffs_np.mean()
    ci_lower = np.percentile(diffs_np, 2.5)
    ci_upper = np.percentile(diffs_np, 97.5)
    # p‑value: H0: diff <= 0  (si diff_mean>0) ó diff >=0 (si diff_mean<0)
    if diff_mean >= 0:
        p_val = (diffs_np <= 0).mean()
    else:
        p_val = (diffs_np >= 0).mean()

    return {
        "diff_mean": diff_mean,
        "ci_lower": ci_lower,
        "ci_upper": ci_upper,
        "p_value": p_val,
    }

"""

evaluation/retrieval_metrics.py
"""
# evaluation/retrieval_metrics.py

from __future__ import annotations

import numpy as np
from typing import List, Sequence, Dict, Tuple, Union
              
ID = Union[int, str]

###############################################################################
#  Métricas elementales (1 consulta)
###############################################################################

def recall_at_k(retrieved: Sequence[ID], relevant: Sequence[ID], k: int) -> float:
    if not relevant:
        return 0.0
    return len(set(retrieved[:k]) & set(relevant)) / len(relevant)

def mrr(retrieved: Sequence[ID], relevant: Sequence[ID]) -> float:
    for i, d in enumerate(retrieved, 1):
        if d in relevant:
            return 1.0 / i
    return 0.0

def ndcg_at_k(retrieved: Sequence[ID], relevant: Sequence[ID], k: int) -> float:
    dcg = sum(
        1.0 / np.log2(i + 2) if d in relevant else 0.0
        for i, d in enumerate(retrieved[:k])
    )
    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), k)))
    return dcg / idcg if idcg else 0.0

###############################################################################
#  Vectorización sobre lotes
###############################################################################

def _parse_metric(m: str) -> Tuple[str, int | None]:
    return (m.split("@")[0], int(m.split("@")[1])) if "@" in m else (m, None)

def _score_single(
    retrieved: Sequence[ID],
    relevant: Sequence[ID],
    name: str,
    k: int | None,
) -> float:
    name = name.lower()
    if name == "recall" and k is not None:
        return recall_at_k(retrieved, relevant, k)
    if name == "mrr":
        return mrr(retrieved[: (k or len(retrieved))], relevant)
    if name == "ndcg" and k is not None:
        return ndcg_at_k(retrieved, relevant, k)
    raise ValueError(f"Métrica '{name}' mal especificada.")

def evaluate_retrieval(retrieved_batch: List[Sequence[ID]] | Sequence[ID],
    relevant_batch: List[Sequence[ID]] | Sequence[ID],
    metrics: List[str] | None = None,
    *,
    return_per_query: bool = False,
) -> Dict[str, Dict[str, float]] | Tuple[Dict[str, Dict[str, float]],
                                         List[Dict[str, float]]]:
    
    # ── Normalizar entrada a lote ──────────────────────────────────────────
    single = isinstance(retrieved_batch[0], (str, int))  # type: ignore[index]
    if single:
        retrieved_batch = [retrieved_batch]              # type: ignore[assignment]
        relevant_batch  = [relevant_batch]               # type: ignore[assignment]

    assert len(retrieved_batch) == len(relevant_batch), \
        "retrieved_batch y relevant_batch deben tener la misma longitud."

    if not metrics:
        raise ValueError("No se especificaron métricas de evaluación.")

    Q = len(retrieved_batch)
    per_query: List[Dict[str, float]] = [{} for _ in range(Q)]
    summary: Dict[str, Dict[str, float]] = {}

    for m in metrics:
        name, k = _parse_metric(m)
        vals = [
            _score_single(r, rel, name, k)
            for r, rel in zip(retrieved_batch, relevant_batch)
        ]
        summary[m] = {
            "mean": float(np.mean(vals)),
            "std":  float(np.std(vals, ddof=1)) if Q > 1 else 0.0,
        }
        for d, v in zip(per_query, vals):
            d[m] = v

    if return_per_query:
        return summary, per_query
    if single:
        return {k: v["mean"] for k, v in summary.items()}      # compat.
    return summary

"""

generation/generator.py
"""
# generation/generator.py  – RAG (Refactor estilo train_vae)

from __future__ import annotations
import os, textwrap, logging
from typing import List, Dict, Any
from dataclasses import dataclass, field
from openai import OpenAI, AsyncOpenAI

def _load_prompt(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        logging.getLogger(__name__).warning("Prompt no encontrado: %s", path)
        return ""

@dataclass
class LLMSettings:
    model: str = "gpt-4o-mini"
    temperature: float = 0.3
    top_p: float = 1.0
    max_tokens: int = 512
    system_prompt_path: str = "./config/prompts/system_prompt.txt"
    system_prompt: str = field(init=False)

    def __post_init__(self):
        self.system_prompt = _load_prompt(self.system_prompt_path)

@dataclass
class GeneratorConfig:
    llm: LLMSettings = field(default_factory=LLMSettings)
    max_context_tokens: int = 4096
    provider: str = "openai"          
    extras: Dict[str, Any] = field(default_factory=dict)


###############################################################################
#  RAG GENERATOR                                                               #
###############################################################################

class RAGGenerator:
    """Generador basado en LLM + documentos recuperados."""

    def __init__(self, cfg: Dict[str, Any], **overrides):
        # fusinon YAML + overrides CLI
        gen_cfg_dict = {**cfg.get("generation", {}), **overrides}

        llm_cfg_dict = gen_cfg_dict.pop("llm", {})
        self.cfg = GeneratorConfig(
            llm=LLMSettings(**llm_cfg_dict),
            **{k: v for k, v in gen_cfg_dict.items() if k in {"max_context_tokens", "provider"}},
            extras={k: v for k, v in gen_cfg_dict.items() if k not in {"max_context_tokens", "provider"}}
        )

        self.logger = logging.getLogger(self.__class__.__name__)
        self._init_openai()

    # ---------------- PUBLIC API -------------------------------------------
    def generate(self, query: str, retrieved_docs: List[str]) -> str:
        prompt = self._build_prompt(query, retrieved_docs)
        self.logger.debug("Prompt (%d chars) construido.", len(prompt))

        response = self.client.chat.completions.create(
            model=self.cfg.llm.model,
            temperature=self.cfg.llm.temperature,
            top_p=self.cfg.llm.top_p,
            max_tokens=self.cfg.llm.max_tokens,
            messages=[
                {"role": "system", "content": self.cfg.llm.system_prompt},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()


    async def generate_async(self, query: str, retrieved_docs: List[str]) -> str:
        prompt = self._build_prompt(query, retrieved_docs)
        self.logger.debug("Prompt async (%d chars).", len(prompt))

        client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = await client.chat.completions.create(
            model=self.cfg.llm.model,
            temperature=self.cfg.llm.temperature,
            top_p=self.cfg.llm.top_p,
            max_tokens=self.cfg.llm.max_tokens,
            messages=[
                {"role": "system", "content": self.cfg.llm.system_prompt},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()

    # ---------------- INTERNALS -------------------------------------------
    def _init_openai(self) -> None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise EnvironmentError(
                "Variable OPENAI_API_KEY no definida. Añádela a tu .env o al entorno."
            )
        self.client = OpenAI(api_key=api_key)
        self.logger.info("API Key OpenAI cargada correctamente.")

    def _build_prompt(self, query: str, docs: List[str]) -> str:
        context = self._truncate_docs(docs)
        joined = "\n\n".join(f"Doc {i+1}: {d}" for i, d in enumerate(context))
        return textwrap.dedent(
            f"""\
            Utiliza exclusivamente la siguiente información para responder.\n\n{joined}\n\n
            Pregunta: {query}\n\nRespuesta:"""
        )

    def _truncate_docs(self, docs: List[str]) -> List[str]:
        max_chars = self.cfg.max_context_tokens * 4   # heuristic ≈ tokens*4
        out, acc = [], 0
        for d in docs:
            if acc + len(d) > max_chars:
                break
            out.append(d)
            acc += len(d)
        return out

"""

main.py
"""
from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple

import torch
from dotenv import load_dotenv

# Third‑party
from rich import print as rprint
from sentence_transformers import SentenceTransformer  # lazy‑loaded by embedder

# First‑party (repository) -----------------------------------------------------
from utils.load_config import init_logger, load_config
from utils.training_utils import resolve_device, set_seed
from retrieval.embedder import EmbeddingCompressor
from retrieval.retriever import retrieve_top_k
from evaluation.retrieval_metrics import evaluate_retrieval
from evaluation.generation_metrics import (
    evaluate_generation_bootstrap as eval_generation,
)
from generation.generator import RAGGenerator
from models.variational_autoencoder import VariationalAutoencoder
from models.denoising_autoencoder import DenoisingAutoencoder
from models.contrastive_autoencoder import ContrastiveAutoencoder

# ---------------------------------------------------------------------------
# Helper factories
# ---------------------------------------------------------------------------

def _load_autoencoder(
    cfg_models: Dict[str, Dict[str, Any]],
    ae_type: str,
    device: str,
) -> Optional[torch.nn.Module]:
    """Instantiate and load the requested auto‑encoder.

    Args:
        cfg_models: Dict extracted from YAML under `models`.
        ae_type:    "vae", "dae", "contrastive" or "none".
        device:     "cpu" | "cuda".

    Returns:
        A `torch.nn.Module` in eval mode, or *None* if `ae_type == "none"`.
    """

    if ae_type == "none":
        return None

    if ae_type not in cfg_models:
        raise ValueError(
            f"[CONFIG] Auto‑encoder '{ae_type}' not found under 'models' in config."
        )

    mcfg = cfg_models[ae_type]
    input_dim = mcfg.get("input_dim", 384)
    latent_dim = mcfg.get("latent_dim", 64)
    hidden_dim = mcfg.get("hidden_dim", 512)
    checkpoint = mcfg.get("checkpoint")

    if ae_type == "vae":
        model: torch.nn.Module = VariationalAutoencoder(  # type: ignore[assignment]
            input_dim, latent_dim, hidden_dim
        )
    elif ae_type == "dae":
        model = DenoisingAutoencoder(input_dim, latent_dim, hidden_dim)
    elif ae_type == "contrastive":
        model = ContrastiveAutoencoder(input_dim, latent_dim, hidden_dim)
    else:
        raise RuntimeError("Unreachable branch – ae_type already validated.")

    if checkpoint and Path(checkpoint).exists():
        model.load_state_dict(torch.load(checkpoint, map_location=device))
    else:
        raise FileNotFoundError(f"Checkpoint for '{ae_type}' not found: {checkpoint}")

    return model.to(device).eval()


# ---------------------------------------------------------------------------
# Pipeline steps
# ---------------------------------------------------------------------------

def _encode_corpus(
    compressor: EmbeddingCompressor,
    texts: Sequence[str],
    compress: bool = True,
) -> torch.Tensor:
    """Return document embeddings as `[N × D]` float32 CPU tensor."""

    return compressor.encode_text(list(texts), compress=compress)


def _retrieve_documents(
    query_emb: torch.Tensor,
    doc_emb: torch.Tensor,
    corpus: Sequence[str],
    retr_cfg: Dict[str, Any],
) -> Tuple[List[str], List[float]]:
    """Retrieve top‑k docs and similarity scores for a *single* query."""

    top_k = retr_cfg.get("top_k", 10)
    metric = retr_cfg.get("similarity_metric", "cosine")
    results = retrieve_top_k(query_emb, doc_emb, list(corpus), k=top_k, metric=metric)
    docs, scores = zip(*results)
    return list(docs), list(scores)


def _evaluate_retrieval(
    retrieved: Sequence[Sequence[str]],
    relevant: Sequence[Sequence[str]] | Sequence[str],
    metrics: List[str],
) -> Dict[str, Dict[str, float]]:
    """Wrapper around `evaluate_retrieval` with sensible defaults."""

    return evaluate_retrieval(retrieved, relevant, metrics=metrics)


# ---------------------------------------------------------------------------
# Core runner
# ---------------------------------------------------------------------------

class PipelineRunner:  # noqa: D101 – simple orchestrator
    def __init__(self, cfg: Dict[str, Any], ae_type: str, logger):
        self.cfg = cfg
        self.ae_type = ae_type
        self.logger = logger

        device = resolve_device(cfg.get("training", {}).get("device"))
        self.device = device
        self.logger.main.info("Device resolved → %s", device)

        # Auto‑encoder & compressor ------------------------------------------------
        ae_model = _load_autoencoder(cfg["models"], ae_type, device)
        self.compressor = EmbeddingCompressor(
            base_model_name=cfg["embedding_model"]["name"],
            autoencoder=ae_model,
            device=device,
        )
        self.logger.main.info("Compressor ready (AE = %s)", ae_type)

        # Retriever ---------------------------------------------------------------
        self.retr_cfg = cfg.get("retrieval", {})

        # Generator ---------------------------------------------------------------
        self.generator = RAGGenerator(cfg)

    # ---------------------------------------------------------------------
    def process(
        self,
        queries: Sequence[str],
        corpus: Sequence[str],
        relevant_docs: Optional[Sequence[str]] = None,
    ) -> None:
        """Run encode → retrieve → generate → evaluate for *all* queries."""

        self.logger.main.info("Running pipeline: |queries|=%d |docs|=%d", len(queries), len(corpus))
        doc_embeddings = _encode_corpus(self.compressor, corpus, compress=True)
        query_embeddings = _encode_corpus(self.compressor, queries, compress=True)

        all_retrieved: List[Sequence[str]] = []
        answers: List[str] = []

        for idx, (q, q_emb) in enumerate(zip(queries, query_embeddings)):
            docs_k, _ = _retrieve_documents(q_emb, doc_embeddings, corpus, self.retr_cfg)
            all_retrieved.append(docs_k)
            ans = self.generator.generate(q, docs_k)
            answers.append(ans)
            self.logger.main.debug("[%d] Q: %s | A: %s", idx, q, ans[:60] + "…")

        # ----------------------------------------------------------------- EVAL
        eval_cfg = self.cfg.get("evaluation", {})
        if relevant_docs:
            ret_metrics = _evaluate_retrieval(
                all_retrieved,
                relevant_docs,
                metrics=eval_cfg.get("retrieval_metrics", ["Recall@5"]),
            )
            rprint("[bold magenta]\n[Retrieval evaluation]\n[/]")
            for k, v in ret_metrics.items():
                rprint(f"{k}: {v['mean']:.4f} ± {v['std']:.4f}")

        if relevant_docs and len(queries) >= 30:
            # If the user provided refs with ≥30 samples we can bootstrap.
            gen_metrics = eval_generation(
                references=list(relevant_docs),
                candidates=answers,
                metrics=eval_cfg.get("generation_metrics", ["ROUGE-L"]),
            )
            rprint("[bold magenta]\n[Generation evaluation]\n[/]")
            for m, d in gen_metrics.items():
                rprint(f"{m}: {d['mean']:.2f} (CI 95%: {d['ci_lower']:.2f}–{d['ci_upper']:.2f})")


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def _parse_args() -> argparse.Namespace:  # noqa: D401
    """Return command‑line arguments."""

    pre_parser = argparse.ArgumentParser(add_help=False)
    pre_parser.add_argument("--config", default="./config/config.yaml")
    known, _ = pre_parser.parse_known_args(sys.argv[1:])

    cfg = load_config(known.config)
    valid_ae = list(cfg.get("models", {}).keys()) + ["none", "all"]

    parser = argparse.ArgumentParser(description="Run RAG‑AE experimental pipeline")
    parser.add_argument("--config", default="./config/config.yaml", help="Path to YAML config")
    parser.add_argument("--ae_type", default="vae", choices=valid_ae, help="Select auto‑encoder variant")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")

    return parser.parse_args()


# ---------------------------------------------------------------------------
# Entry‑point
# ---------------------------------------------------------------------------

def main() -> None:  # noqa: D401 – standard script
    args = _parse_args()

    cfg = load_config(args.config)
    log = init_logger(cfg.get("logging", {}))
    set_seed(args.seed, cfg.get("training", {}).get("deterministic", False), logger=log.train)
    load_dotenv()

    ae_variants = (
        [args.ae_type]
        if args.ae_type != "all"
        else [k for k in cfg.get("models", {}).keys() if k in {"vae", "dae", "contrastive"}]
    )

    # --------------------------------------------------------------------- Toy corpus (replace with real dataset) --
    corpus = [
        "Paris is the capital of France.",
        "The Pythagorean theorem applies to right‑angled triangles.",
        "The Spanish Civil War began in 1936.",
        "GPT is a language model developed by OpenAI.",
        "Autoencoders allow nonlinear compression.",
    ]
    queries = [
        "Which model does OpenAI use for text generation?",
    ]
    relevant = [
        ["GPT is a language model developed by OpenAI."]
    ]

    # --------------------------------------------------------------------- Run each variant
    for ae in ae_variants:
        rprint(f"\n[bold cyan]==== PIPELINE ({ae.upper()}) ====\n[/]")
        runner = PipelineRunner(cfg, ae, log)
        runner.process(queries, corpus, relevant_docs=relevant)


if __name__ == "__main__":
    main()

"""

models/base_autoencoder.py
"""
import torch
import torch.nn as nn
from abc import ABC, abstractmethod

class BaseAutoencoder(nn.Module, ABC):
    def __init__(self, input_dim: int, latent_dim: int):
        super(BaseAutoencoder, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim

    @abstractmethod
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        pass

    @abstractmethod
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        pass

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encode(x)
        return self.decode(z)

"""

models/contrastive_autoencoder.py
"""
# /models/contrastive_autoencoder.py
import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder

class ContrastiveAutoencoder(BaseAutoencoder):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super(ContrastiveAutoencoder, self).__init__(input_dim, latent_dim)

        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # Useful if input vectors are normalized
        )

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encoder(x)
        return torch.nn.functional.normalize(z, p=2, dim=-1)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)
      
    def forward(self, x: torch.Tensor):
        z = self.encode(x)
        x_recon = self.decode(z)
        return x_recon, z

"""

models/denoising_autoencoder.py
"""
# /models/denoising_autoencoder.py

import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder
import torch.nn.functional as F

class DenoisingAutoencoder(BaseAutoencoder):
    """Feed‑forward Denoising Autoencoder.

    The dataset must supply *noisy* inputs; the model learns to reconstruct the
    clean version. Use `dae_loss` (MSE) during training.
    """

    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super().__init__(input_dim, latent_dim)

        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # assume inputs ∈ [0,1]; change if different scale
        )

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        return self.encoder(x)

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encode(x)
        return self.decode(z)

"""

models/variational_autoencoder.py
"""
# / models/variational_autoencoder.py
import torch
import torch.nn as nn
from models.base_autoencoder import BaseAutoencoder

class VariationalAutoencoder(BaseAutoencoder):
    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super(VariationalAutoencoder, self).__init__(input_dim, latent_dim)
        
        # Encoder: proyecciones a la media y desviación estándar
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.mu_layer = nn.Linear(hidden_dim, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            # nn.Sigmoid()  # Asumimos entrada normalizada (0-1)
        )

    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        h = self.encoder(x)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        return mu, logvar

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        return self.decoder(z)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_reconstructed = self.decode(z)
        return x_reconstructed, mu, logvar

"""

requeriments.txt
"""
# Core ML & Data Processing
torch>=2.0.0
transformers>=4.38.0
sentence-transformers>=2.2.2
scikit-learn>=1.3.0
numpy>=1.24.0
pandas>=2.0.0

datasets>=2.19.0
scipy>=1.10.0
openai>=1.14.0


# Visualization & Analysis
matplotlib>=3.7.0
seaborn>=0.12.0

# Evaluation Metrics (Efficient and Torch-Compatible)
rouge-score >=0.1.2
sacrebleu>=2.5.1

# Configuration & Utilities
pyyaml>=6.0
python-dotenv>=1.0.0

# Optional CLI Enhancements
rich>=13.0.0

pytest 
"""

retrieval/embedder.py
"""
# retrieval/embedder.py

from sentence_transformers import SentenceTransformer
import torch


class EmbeddingCompressor:
    def __init__(
        self,
        base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        autoencoder: torch.nn.Module = None,
        device: str = None
    ):
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # Load pretrained SBERT model (includes pooling + normalization)
        self.model = SentenceTransformer(base_model_name, device=self.device)

        # Optional autoencoder for compression (VAE, DAE, etc.)
        self.autoencoder = autoencoder.to(self.device) if autoencoder else None
        if self.autoencoder:
            self.autoencoder.eval()

    def encode_text(self, texts: list[str], compress: bool = True) -> torch.Tensor:
        """Returns SBERT embeddings, optionally compressed with an autoencoder.

        Args:
            texts: List of input strings to encode.
            compress: Whether to apply the autoencoder (if available).

        Returns:
            A float32 tensor [N × D] on CPU.
        """
        with torch.no_grad():
            embeddings = self.model.encode(
                texts,
                batch_size=64,
                convert_to_tensor=True,
                normalize_embeddings=True
            ).to(self.device)

            if self.autoencoder and compress:
                encoded = self.autoencoder.encode(embeddings)
                if isinstance(encoded, tuple):  # VAE returns (mu, logvar)
                    encoded = encoded[0]        # use mean as latent code
                return encoded.cpu()

            return embeddings.cpu()

"""

retrieval/retriever.py
"""
# /retrieval/retriever.py

from __future__ import annotations

import torch
import torch.nn.functional as F
import numpy as np
from sklearn.covariance import EmpiricalCovariance
from typing import List, Tuple, Literal

SimilarityMetric = Literal["cosine", "euclidean", "mahalanobis"]

###############################################################################
#  MATRICES DE SIMILITUD                                                      #
###############################################################################

def cosine_similarity_matrix(
    query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor
) -> np.ndarray:
    """Matriz [Q × N] con similitud coseno."""
    q_norm = F.normalize(query_embeddings, p=2, dim=1)
    d_norm = F.normalize(doc_embeddings, p=2, dim=1)
    sim = torch.mm(q_norm, d_norm.T)  # [Q, N]
    return sim.cpu().numpy()


def euclidean_similarity_matrix(
    query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor
) -> np.ndarray:
    """Matriz [Q × N] con "+similitud" inversa a la distancia euclídea."""
    q = query_embeddings.unsqueeze(1)  # [Q, 1, D]
    d = doc_embeddings.unsqueeze(0)  # [1, N, D]
    dist = torch.norm(q - d, dim=2)  # [Q, N]
    return (-dist).cpu().numpy()  # valores altos = más similares


def mahalanobis_similarity_matrix(
    query_embeddings: torch.Tensor, doc_embeddings: torch.Tensor, eps: float = 1e-6
) -> np.ndarray:
    """Matriz [Q × N] con similitud inversa de la distancia de Mahalanobis.

    *Se ajusta la matriz de covarianza exclusivamente sobre los documentos* para
    preservar la simetría deseada en el espacio de recuperación.
    """
    # -- Datos a NumPy -------------------------------------------------------
    d_np: np.ndarray = doc_embeddings.cpu().numpy()
    q_np: np.ndarray = query_embeddings.cpu().numpy()

    # -- Precisión (inversa de la covarianza) -------------------------------
    # EmpiricalCovariance añade regularización de Ledoit‑Wolf si el sistema lo
    # necesita, pero incluimos un término eps para garantizar invertibilidad.
    emp = EmpiricalCovariance(assume_centered=False).fit(d_np)
    VI: np.ndarray = emp.precision_ + eps * np.eye(d_np.shape[1], dtype=np.float64)

    # -- Distancias ----------------------------------------------------------
    diff = q_np[:, None, :] - d_np[None, :, :]  # [Q, N, D]
    # einsum: (q n d, d d, q n d) → (q n)
    dist = np.einsum("qnd,dd,qnd->qn", diff, VI, diff, optimize=True)

    # -- Convertir a "similitud" (negativo de la distancia) -----------------
    sim = -dist  # altos valores ⇒ mayor similitud
    return sim.astype(np.float32)


###############################################################################
#  FRONT‑END DE RECUPERACIÓN                                                  #
###############################################################################

def compute_similarity(
    query_embeddings: torch.Tensor,
    doc_embeddings: torch.Tensor,
    metric: SimilarityMetric = "cosine",
) -> np.ndarray:
    """Devuelve matriz [Q × N] según la métrica solicitada y valida formas."""

    if query_embeddings.dim() == 1:
        query_embeddings = query_embeddings.unsqueeze(0)
    if doc_embeddings.dim() == 1:
        doc_embeddings = doc_embeddings.unsqueeze(0)

    funcs = {
        "cosine": cosine_similarity_matrix,
        "euclidean": euclidean_similarity_matrix,
        "mahalanobis": mahalanobis_similarity_matrix,
    }

    if metric not in funcs:
        raise ValueError(f"Métrica de similitud '{metric}' no soportada.")

    sim = funcs[metric](query_embeddings, doc_embeddings)

    # -------- Validación de forma -----------------------------------------
    q, d = query_embeddings.shape[0], doc_embeddings.shape[0]
    if sim.shape != (q, d):
        raise RuntimeError(
            f"Shape mismatch: expected ({q}, {d}) got {sim.shape} for metric '{metric}'."
        )
    return sim


def retrieve_top_k(
    query_embedding: torch.Tensor,
    doc_embeddings: torch.Tensor,
    doc_texts: List[str],
    k: int = 5,
    metric: SimilarityMetric = "cosine",
) -> List[Tuple[str, float]]:
    """Recupera los *k* documentos con mayor similitud."""

    sim_scores = compute_similarity(query_embedding, doc_embeddings, metric)  # [Q, N]
    # Suponemos única consulta (Q = 1) para esta utilidad.
    if sim_scores.shape[0] != 1:
        raise ValueError("Esta función está pensada para una única consulta.")

    top_idx = sim_scores[0].argsort()[::-1][:k]
    return [(doc_texts[i], float(sim_scores[0, i])) for i in top_idx]

"""

save_snapshot.sh
"""
#!/usr/bin/env bash
#
# save_snapshot.sh  –  Dump a folder’s tree plus every file’s contents
# Usage:
#   ./save_snapshot.sh [TARGET_DIR] [OUTPUT_TXT]
# Defaults:
#   TARGET_DIR="."                  (current directory)
#   OUTPUT_TXT="snapshot.txt"       (created/overwritten)

set -euo pipefail

###############################################################################
# 1. Input handling
###############################################################################
TARGET_DIR="${1:-.}"
OUTPUT_TXT="${2:-snapshot.txt}"

# Verify prerequisites
command -v tree >/dev/null 2>&1 || {
  printf 'Error: "tree" command not found. Please install it first.\n' >&2; exit 1; }

# Avoid accidental overwrite of important files
if [[ -e "$OUTPUT_TXT" && ! -w "$OUTPUT_TXT" ]]; then
  printf 'Error: Output file "%s" is not writable.\n' "$OUTPUT_TXT" >&2
  exit 1
fi

###############################################################################
# 2. Capture the directory tree
###############################################################################
# Truncate/overwrite existing output
: > "$OUTPUT_TXT"

printf '### Directory tree for: %s\n\n' "$TARGET_DIR" >> "$OUTPUT_TXT"
tree -F -I '__pycache__' "$TARGET_DIR" >> "$OUTPUT_TXT" || {
  printf 'Warning: Unable to generate tree for "%s".\n' "$TARGET_DIR" >&2; }

printf '\n\n### File contents\n\n' >> "$OUTPUT_TXT"

###############################################################################
# 3. Append each file (relative path + contents)
###############################################################################
# Absolute path to output for comparison
ABS_OUTPUT="$(realpath "$OUTPUT_TXT")"

# Exclude hidden files, __pycache__, and specific extensions
find "$TARGET_DIR" -type f \
  ! -path '*/.*/*' \
  ! -name '.*' \
  ! -path '*/__pycache__/*' \
  -print0 | sort -z |
while IFS= read -r -d '' FILE
do
  # Resolve absolute path of the file
  ABS_FILE="$(realpath "$FILE")"

  # Skip the output file itself
  if [[ "$ABS_FILE" == "$ABS_OUTPUT" ]]; then
    continue
  fi

  # Skip files with undesired extensions
  case "$FILE" in
    *.pt|*.pth|*.ipynb|*.log)
      continue
      ;;
  esac

  # Remove leading base path for relative display
  REL_PATH="${FILE#$TARGET_DIR/}"

  # Header
  printf '%s\n' "$REL_PATH" >> "$OUTPUT_TXT"
  printf '"""\n' >> "$OUTPUT_TXT"

  # File contents
  if ! cat "$FILE" >> "$OUTPUT_TXT" 2>/dev/null; then
    printf '[[[ Error reading file ]]]\n' >> "$OUTPUT_TXT"
    printf 'Warning: Could not read "%s".\n' "$REL_PATH" >&2
  fi

  # Footer
  printf '\n"""\n\n' >> "$OUTPUT_TXT"
done

printf 'Snapshot saved to: %s\n' "$OUTPUT_TXT"

"""

style_guide.md
"""
# Project-wide Code Style Guide (English)

> **Scope**  All Python modules, notebooks, shell scripts and configuration files located under the `tfm` repository.  New code **must** comply immediately.  Legacy code should be progressively refactored.

---

## 1  General Principles

| Principle                         | Rationale                                                        |
| --------------------------------- | ---------------------------------------------------------------- |
| **Consistency over cleverness**   | Readability and maintenance outweigh micro-optimisations.        |
| **Explicit > implicit**           | Follow *The Zen of Python*. Avoid hidden state and side effects. |
| **Fail fast, fail loud**          | Raise specific exceptions early; log context-rich messages.      |
| **Pure functions where possible** | Functions that depend only on arguments are easier to test.      |

---

## 2  File & Folder Layout

* Package modules use **snake\_case** filenames: `contrastive_autoencoder.py`.
* Public executables go under `/scripts` with a short shebang and CLI (`argparse`).
* Unit tests mirror the package tree under `/tests`.
* Keep data or model artefacts out of Git; place under `/data` or `/models/checkpoints` and add to `.gitignore`.

---

## 3  Imports

```
# 1. Standard library
import os
import json

# 2. Third-party
import numpy as np
import torch

# 3. First-party (this repo)
from utils.load_config import load_config
```

* Use **absolute imports** inside the package.
* Never use `from module import *`.
* Group imports and separate blocks with one blank line.

---

## 4  Naming Conventions

* **snake\_case** for variables, functions and methods.
* **PascalCase** for classes and `Enum` members.
* **UPPER\_SNAKE\_CASE** for module-level constants.
* Avoid Spanish identifiers; prefer descriptive English (`embedding_dim`, not `dim_emb`).

---

## 5  Docstrings & Comments

* **Every** public module, function, class and method **must** have a docstring in **English** using the **Google style**.
* Keep inline comments short; they explain *why*, not *what*.
* TODO/FIXME tags must include assignee or ticket reference.

```python
def recall_at_k(retrieved: Sequence[str], relevant: Sequence[str], k: int) -> float:
    """Return Recall@k.

    Args:
        retrieved: Ordered list of retrieved IDs.
        relevant: Set or list of relevant IDs.
        k: Cut-off rank.

    Returns:
        Fraction of relevant items found in the top-k.
    """
```

---

## 6  Type Annotations & Runtime Checks

* Use **PEP 484** type hints everywhere (functions, class attributes).
* **All functions must declare input and output types explicitly.**
* Validate external inputs with `assert` or explicit `if ... raise ValueError`.
* Run **mypy** in *strict* mode as a CI step.

---

## 7  Logging

* Initialise loggers via `utils.load_config.init_logger`.
* Use module-level loggers: `logger = logging.getLogger(__name__)`.
* Logging levels: `debug` (dev insights), `info` (milestones), `warning` (recoverable), `error` (cannot proceed), `critical` (program abort).
* Never hide exceptions; use `logger.exception` to preserve traceback.

---

## 8  Error Handling

* Prefer built-in exceptions (`ValueError`, `TypeError`) unless a custom domain error clarifies intent.
* Enrich messages with variable values.
* Do **not** swallow exceptions silently.

---

## 9  Configuration Management

* All hyper-parameters live in YAML under `/config`.
* Load configs **only** through `utils.load_config.load_config`.
* Functions accept an explicit `config: dict` argument instead of reading files internally, unless the function’s **sole purpose** is configuration loading.

---

## 10  CLI & Scripts

* Use `argparse` with long option names (`--batch_size`).
* Provide `--config` flag pointing to a YAML; CLI flags override YAML.
* Scripts must be import-safe (`if __name__ == "__main__":`).

---

## 11  Testing & Quality Gates

* Write **pytest** unit tests covering critical paths.
* Minimum coverage threshold: **80 %**.
* Run `black`, `ruff`, `isort`, `mypy` and tests in CI before merge.

---

## 12  Formatting Tools

| Tool      | Version | Role                                 |
| --------- | ------- | ------------------------------------ |
| **black** | 24.3+   | Code formatting (line length 88).    |
| **ruff**  | 0.3+    | Linter (select = "ALL", ignore = …). |
| **isort** | 5+      | Import ordering (profile = "black"). |

---

## 13  Dependencies & Virtual Envs

* Pin versions in `requirements.txt` (prod) and `requirements-dev.txt` (lint/test).
* Use **conda** or **venv**; never rely on system Python.
* Document GPU/CPU requirements in `README.md`.

---

## 14  Internationalisation

* **All code, comments and docstrings must be in English.**  Spanish is reserved for external documentation or academic writing outside the repository.

---

## 15  Example Module Skeleton

```python
"""Contrastive Autoencoder model.

Implements an encoder–decoder architecture trained with triplet loss.
"""
from __future__ import annotations

import torch
from torch import nn, Tensor

class ContrastiveAutoencoder(nn.Module):
    """Linear contrastive autoencoder with L2-normalised latent vectors."""

    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 512):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim),
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
        )

    def encode(self, x: Tensor) -> Tensor:  # noqa: D401
        """Return L2-normalised latent representation."""
        return torch.nn.functional.normalize(self.encoder(x), p=2, dim=-1)

    def decode(self, z: Tensor) -> Tensor:
        return self.decoder(z)

    def forward(self, x: Tensor) -> Tensor:    # type: ignore[override]
        return self.decode(self.encode(x))
```

---

## 16  Migration Plan for Legacy Code

1. **Phase 1**  — New code follows this guide immediately.
2. **Phase 2**  — Touch legacy modules only when editing; refactor headers, identifiers, comments to English.
3. **Phase 3**  — Run automated formatters; fix mypy errors; localised refactors.

---

*End of style guide.*

"""

test/test_evaluation.py
"""
import pytest
import torch
import numpy as np
from evaluation.retrieval_metrics import recall_at_k, mrr, ndcg_at_k, evaluate_retrieval
from evaluation.generation_metrics import compute_bleu, compute_rouge_l, evaluate_generation_bootstrap
from evaluation.autoencoder_metrics import evaluate_reconstruction_loss

# ------------------- Retrieval Metrics -------------------
def test_recall_at_k():
    retrieved = [1, 2, 3, 4, 5]
    relevant = [3, 4, 6]
    assert recall_at_k(retrieved, relevant, k=3) == 1 / 3, "Recall@k mismatch"

def test_mrr():
    retrieved = [1, 2, 3, 4, 5]
    relevant = [3, 4, 6]
    assert mrr(retrieved, relevant) == 1 / 3, "MRR mismatch"

def test_ndcg_at_k():
    retrieved = [1, 2, 3, 4, 5]
    relevant = [3, 4, 6]
    assert np.isclose(ndcg_at_k(retrieved, relevant, k=3), 0.23463936301137822, atol=1e-6), "NDCG@k mismatch"

def test_evaluate_retrieval():
    retrieved_batch = [[1, 2, 3], [4, 5, 6]]
    relevant_batch = [[3, 4], [5, 6]]
    metrics = ["recall@2", "mrr"]
    results = evaluate_retrieval(retrieved_batch, relevant_batch, metrics)
    assert "recall@2" in results and "mrr" in results, "Evaluation metrics missing"

# ------------------- Generation Metrics -------------------
def test_compute_bleu():
    refs = ["this is a test"]
    cands = ["this is a test"]
    assert np.isclose(compute_bleu(cands, refs), 100.0, atol=1e-6), "BLEU mismatch"

def test_compute_rouge_l():
    refs = ["this is a test"]
    cands = ["this is a test"]
    assert compute_rouge_l(cands, refs) == 100.0, "ROUGE-L mismatch"

def test_evaluate_generation_bootstrap():
    refs = ["this is a test"] * 30
    cands = ["this is a test"] * 30
    results = evaluate_generation_bootstrap(refs, cands, metrics=["BLEU", "ROUGE-L"], n_samples=100)
    assert "BLEU" in results and "ROUGE-L" in results, "Bootstrap metrics missing"

# ------------------- Autoencoder Metrics -------------------
def test_evaluate_reconstruction_loss():
    x = torch.randn(10, 16)
    x_reconstructed = x + torch.randn(10, 16) * 0.1
    loss = evaluate_reconstruction_loss(x, x_reconstructed, reduction="mean")
    assert loss > 0, "Reconstruction loss should be positive"

"""

test/test_loss_functions.py
"""
import torch
import pytest
from training.loss_functions import vae_loss, dae_loss, contrastive_loss

# ------------------- VAE Loss -------------------
def test_vae_loss_basic():
    x_reconstructed = torch.randn(8, 16)
    x_target = torch.randn(8, 16)
    mu = torch.zeros(8, 4)
    logvar = torch.zeros(8, 4)
    loss = vae_loss(x_reconstructed, x_target, mu, logvar)
    assert loss.shape == (), "VAE loss should be a scalar"
    assert loss.item() >= 0

def test_vae_loss_beta():
    x_reconstructed = torch.randn(4, 10)
    x_target = torch.randn(4, 10)
    mu = torch.randn(4, 3)
    logvar = torch.randn(4, 3)
    loss1 = vae_loss(x_reconstructed, x_target, mu, logvar, beta=0.5)
    loss2 = vae_loss(x_reconstructed, x_target, mu, logvar, beta=2.0)
    assert loss1 != loss2

# ------------------- DAE Loss -------------------
def test_dae_loss_basic():
    x_reconstructed = torch.randn(5, 7)
    x_clean = torch.randn(5, 7)
    loss = dae_loss(x_reconstructed, x_clean)
    assert loss.shape == (), "DAE loss should be a scalar"
    assert loss.item() >= 0

# ------------------- Contrastive Loss -------------------
def test_contrastive_loss_hard_negatives():
    z_q = torch.randn(6, 8)
    z_pos = torch.randn(6, 8)
    loss = contrastive_loss(z_q, z_pos, hard_negatives=True)
    assert loss.shape == (), "Contrastive loss should be a scalar"
    assert loss.item() >= 0

def test_contrastive_loss_random_negatives():
    z_q = torch.randn(6, 8)
    z_pos = torch.randn(6, 8)
    loss = contrastive_loss(z_q, z_pos, hard_negatives=False)
    assert loss.shape == (), "Contrastive loss should be a scalar"
    assert loss.item() >= 0

def test_contrastive_loss_margin_effect():
    z_q = torch.randn(4, 5)
    z_pos = torch.randn(4, 5)
    loss1 = contrastive_loss(z_q, z_pos, margin=0.1)
    loss2 = contrastive_loss(z_q, z_pos, margin=1.0)
    assert loss1 != loss2

"""

test/test_models.py
"""
import pytest
import torch
from models.variational_autoencoder import VariationalAutoencoder
from models.denoising_autoencoder import DenoisingAutoencoder
from models.contrastive_autoencoder import ContrastiveAutoencoder

# ------------------- Variational Autoencoder -------------------
def test_variational_autoencoder():
    model = VariationalAutoencoder(input_dim=16, latent_dim=4, hidden_dim=8)
    x = torch.randn(2, 16)
    x_reconstructed, mu, logvar = model(x)

    assert x_reconstructed.shape == x.shape, "Reconstructed output shape mismatch"
    assert mu.shape == (2, 4), "Latent mean shape mismatch"
    assert logvar.shape == (2, 4), "Latent logvar shape mismatch"

# ------------------- Denoising Autoencoder -------------------
def test_denoising_autoencoder():
    model = DenoisingAutoencoder(input_dim=16, latent_dim=4, hidden_dim=8)
    x_noisy = torch.randn(2, 16)
    x_reconstructed = model(x_noisy)

    assert x_reconstructed.shape == x_noisy.shape, "Reconstructed output shape mismatch"

# ------------------- Contrastive Autoencoder -------------------
def test_contrastive_autoencoder():
    model = ContrastiveAutoencoder(input_dim=16, latent_dim=4, hidden_dim=8)
    x = torch.randn(2, 16)
    x_reconstructed, z = model(x)

    assert x_reconstructed.shape == x.shape, "Reconstructed output shape mismatch"
    assert z.shape == (2, 4), "Latent embedding shape mismatch"

    # Test normalized embeddings
    z_norm = torch.norm(z, p=2, dim=-1)
    assert torch.allclose(z_norm, torch.ones_like(z_norm), atol=1e-6), "Latent embeddings are not normalized"

"""

test/test_retrieval.py
"""
import pytest
import torch
from retrieval.embedder import EmbeddingCompressor
from retrieval.retriever import compute_similarity, retrieve_top_k

# ------------------- EmbeddingCompressor -------------------
def test_embedding_compressor_without_autoencoder():
    embedder = EmbeddingCompressor()
    texts = ["This is a test.", "Another test."]
    embeddings = embedder.encode_text(texts, compress=False)

    assert embeddings.shape == (2, 384), "Embedding shape mismatch"

def test_embedding_compressor_with_autoencoder():
    class DummyAutoencoder(torch.nn.Module):
        def __init__(self):
            super().__init__()
        def encode(self, x):
            return x * 0.5

    autoencoder = DummyAutoencoder()
    embedder = EmbeddingCompressor(autoencoder=autoencoder)
    texts = ["This is a test.", "Another test."]
    embeddings = embedder.encode_text(texts, compress=True)

    assert embeddings.shape == (2, 384), "Compressed embedding shape mismatch"

# ------------------- Retriever -------------------
def test_compute_similarity():
    queries = torch.randn(2, 384)
    docs = torch.randn(5, 384)

    cosine_sim = compute_similarity(queries, docs, metric="cosine")
    assert cosine_sim.shape == (2, 5), "Cosine similarity shape mismatch"

    euclidean_sim = compute_similarity(queries, docs, metric="euclidean")
    assert euclidean_sim.shape == (2, 5), "Euclidean similarity shape mismatch"

    mahalanobis_sim = compute_similarity(queries, docs, metric="mahalanobis")
    assert mahalanobis_sim.shape == (2, 5), "Mahalanobis similarity shape mismatch"

def test_retrieve_top_k():
    query = torch.randn(384)
    docs = torch.randn(10, 384)
    doc_texts = [f"Document {i}" for i in range(10)]

    top_k = retrieve_top_k(query, docs, doc_texts, k=3, metric="cosine")
    assert len(top_k) == 3, "Top-k retrieval length mismatch"
    assert all(isinstance(item, tuple) and len(item) == 2 for item in top_k), "Top-k format mismatch"

"""

test/test_train_scripts.py
"""
import pytest
import torch
import types
from training import train_vae, train_dae, train_cae

class DummyLogger:
    def __init__(self):
        self.main = self
        self.train = self
    def info(self, *args, **kwargs):
        pass

class DummyDataset(torch.utils.data.Dataset):
    def __init__(self, n=10, d=8):
        self.n = n; self.d = d
    def __len__(self): return self.n
    def __getitem__(self, idx):
        return {"input": torch.randn(self.d), "target": torch.randn(self.d), "x": torch.randn(self.d), "y": torch.randn(self.d), "q": torch.randn(self.d), "p": torch.randn(self.d), "n": torch.randn(self.d)}

class DummyVAE(torch.nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dim):
        super().__init__()
        self.dummy_param = torch.nn.Parameter(torch.zeros(1))
    def forward(self, x):
        out = x + self.dummy_param
        return out, torch.zeros(x.size(0), 2), torch.zeros(x.size(0), 2)

class DummyDAE(torch.nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dim):
        super().__init__()
        self.dummy_param = torch.nn.Parameter(torch.zeros(1))
    def forward(self, x):
        return x + self.dummy_param

class DummyCAE(torch.nn.Module):
    def __init__(self, input_dim, latent_dim, hidden_dim):
        super().__init__()
        self.dummy_param = torch.nn.Parameter(torch.zeros(1))
    def encode(self, x):
        return x + self.dummy_param
    def forward(self, x):
        return x + self.dummy_param

def patch_train_vae(monkeypatch):
    monkeypatch.setattr("training.train_vae.EmbeddingVAEDataset", lambda path: DummyDataset())
    monkeypatch.setattr("training.train_vae.VariationalAutoencoder", DummyVAE)
    monkeypatch.setattr("training.train_vae.torch.save", lambda *a, **k: None)
    monkeypatch.setattr("training.train_vae.os.makedirs", lambda *a, **k: None)
    monkeypatch.setattr("utils.data_utils.split_dataset", lambda ds, val_split: (ds, ds))

def patch_train_dae(monkeypatch):
    monkeypatch.setattr("training.train_dae.EmbeddingDAEDataset", lambda path: DummyDataset())
    monkeypatch.setattr("training.train_dae.DenoisingAutoencoder", DummyDAE)
    monkeypatch.setattr("training.train_dae.torch.save", lambda *a, **k: None)
    monkeypatch.setattr("training.train_dae.os.makedirs", lambda *a, **k: None)
    monkeypatch.setattr("utils.data_utils.split_dataset", lambda ds, val_split: (ds, ds))

def patch_train_cae(monkeypatch):
    monkeypatch.setattr("training.train_cae.EmbeddingTripletDataset", lambda path: DummyDataset())
    monkeypatch.setattr("training.train_cae.ContrastiveAutoencoder", DummyCAE)
    monkeypatch.setattr("training.train_cae.torch.save", lambda *a, **k: None)
    monkeypatch.setattr("training.train_cae.os.makedirs", lambda *a, **k: None)
    monkeypatch.setattr("utils.data_utils.split_dataset", lambda ds, val_split: (ds, ds))

def test_train_vae_runs(monkeypatch):
    patch_train_vae(monkeypatch)
    train_vae.train_vae(
        dataset_path="dummy",
        input_dim=8,
        latent_dim=2,
        hidden_dim=4,
        batch_size=2,
        epochs=1,
        lr=1e-3,
        model_save_path="/tmp/vae.pth",
        val_split=0.2,
        patience=1,
        device="cpu",
    )

def test_train_dae_runs(monkeypatch):
    patch_train_dae(monkeypatch)
    train_dae.train_dae(
        dataset_path="dummy",
        input_dim=8,
        latent_dim=2,
        hidden_dim=4,
        batch_size=2,
        epochs=1,
        lr=1e-3,
        model_save_path="/tmp/dae.pth",
        val_split=0.2,
        patience=1,
        device="cpu",
        logger=DummyLogger(),
    )

def test_train_cae_runs(monkeypatch):
    patch_train_cae(monkeypatch)
    train_cae.train_cae(
        dataset_path="dummy",
        input_dim=8,
        latent_dim=2,
        hidden_dim=4,
        batch_size=2,
        epochs=1,
        lr=1e-3,
        model_save_path="/tmp/cae.pth",
        val_split=0.2,
        patience=1,
        device="cpu",
        logger=DummyLogger(),
    )

"""

training/loss_functions.py
"""
# /training/loss_functions.py

import torch
import torch.nn.functional as F

###############################################################################
#  VAE                                                                        #
###############################################################################

import torch
import torch.nn.functional as F

def vae_loss(
    x_reconstructed: torch.Tensor,
    x_target: torch.Tensor,
    mu: torch.Tensor,
    logvar: torch.Tensor,
    *,
    mse_reduction: str = "mean",   # "mean" or "sum"
    beta: float = 1.0,             # β-VAE (β=1 → classic VAE)
) -> torch.Tensor:
    """VAE loss = reconstruction + β·KL  (KL normalized by batch).

    Args:
        x_reconstructed: output from the decoder  ― shape [B, D]
        x_target:        original embeddings ― shape [B, D]
        mu, logvar:      parameters of the latent distribution ― shape [B, Z]
        mse_reduction:   "mean" (recommended) or "sum"
        beta:            weight of the KL term (β-VAE)
    """
    # ── 1. reconstruction error ─────────────────────────────────────────
    recon = F.mse_loss(x_reconstructed, x_target, reduction=mse_reduction)

    # ── 2. KL (normalized) ───────────────────────────────────────────────
    #   KL(q(z|x) || N(0,1))  =  -½ Σ_i (1 + logσ²_i − μ²_i − σ²_i)
    kl = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).mean()  # ← .mean() ≈ /B/Z

    return recon + beta * kl

###############################################################################
#  DAE                                                                        #
###############################################################################

def dae_loss(
    x_reconstructed: torch.Tensor,
    x_clean: torch.Tensor,
    reduction: str = "mean",
) -> torch.Tensor:
    """Mean‑squared error for Denoising Auto‑Encoders."""
    return F.mse_loss(x_reconstructed, x_clean, reduction=reduction)

###############################################################################
#  CONTRASTIVE                                                                #
###############################################################################

def contrastive_loss(
    z_q: torch.Tensor,
    z_pos: torch.Tensor,
    *,
    margin: float = 0.2,
    hard_negatives: bool = True,
) -> torch.Tensor:
    """Triplet loss with negative selection within the batch.

    If `hard_negatives` is True, uses the closest negative; otherwise,
    permutes `z_pos` to obtain a random negative.
    """
    z_q = F.normalize(z_q, p=2, dim=1)
    z_pos = F.normalize(z_pos, p=2, dim=1)

    if hard_negatives:
        dist_mat = torch.cdist(z_q, z_pos, p=2)
        mask = torch.eye(dist_mat.size(0), dtype=torch.bool, device=z_q.device)
        dist_mat = dist_mat.masked_fill(mask, float("inf"))  # ← corrected
        neg_dist, _ = dist_mat.min(dim=1)

    else:
        idx = torch.randperm(z_pos.size(0), device=z_pos.device)
        neg_dist = torch.norm(z_q - z_pos[idx], dim=1)

    pos_dist = torch.norm(z_q - z_pos, dim=1)
    return F.relu(pos_dist - neg_dist + margin).mean()

"""

training/train_cae.py
"""
# training/train_cae.py ― Contrastive Auto-Encoder with negative mining and validation

from __future__ import annotations
import argparse, os, math
from typing import Optional

import torch
from torch.utils.data import DataLoader
from torch.nn.utils import clip_grad_norm_

from data.torch_datasets import EmbeddingTripletDataset
from models.contrastive_autoencoder import ContrastiveAutoencoder
from training.loss_functions import contrastive_loss        # in-batch mining
from utils.load_config import load_config, init_logger
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import prepare_datasets, split_dataset
from dotenv import load_dotenv

# --------------------------------------------------------------------------- #
#  AUX                                                                       #
# --------------------------------------------------------------------------- #

def _build_optimizer(model: torch.nn.Module, lr: float, weight_decay: float) -> torch.optim.Optimizer:
    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

def _build_scheduler(optim: torch.optim.Optimizer, patience: int, factor: float = 0.5):
    # Reduce LR if val_loss does not improve for `patience` consecutive epochs
    return torch.optim.lr_scheduler.ReduceLROnPlateau(
        optim, mode="min", factor=factor, patience=max(1, patience // 2)
    )

# --------------------------------------------------------------------------- #
#  TRAINING LOOP                                                             #
# --------------------------------------------------------------------------- #

def train_cae(
    *,
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    logger,
    hard_negatives: bool = True,
    margin: float = 0.2,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    min_delta: float = 0.003,                   # 0.3% relative improvement
    weight_decay: float = 1e-4,
    clip_grad_norm: float = 1.0,                # 0 = disable
    device: Optional[str] = None,
) -> None:

    device = device or resolve_device()
    log = logger.train if hasattr(logger, "train") else logger

    log.info("CAE | device=%s | hard_negatives=%s | margin=%.3f", device, hard_negatives, margin)

    # ---------------- Dataset ---------------------------
    full_ds = EmbeddingTripletDataset(dataset_path)
    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=True)
    dl_val   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)

    # ---------------- Model & Opt -----------------------
    model = ContrastiveAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = _build_optimizer(model, lr, weight_decay)
    scheduler = _build_scheduler(optim, patience or 4)

    best_val, epochs_no_improve = math.inf, 0

    # Triplet loss native
    triplet_fn = torch.nn.TripletMarginLoss(margin=margin, p=2)

    for epoch in range(1, epochs + 1):
        # ---------------- Train -------------------------
        model.train(); running = 0.0
        for batch in dl_train:
            z_q  = model.encode(batch["q"].to(device))
            z_p  = model.encode(batch["p"].to(device))
            z_n  = model.encode(batch["n"].to(device))

            if hard_negatives:
                loss = contrastive_loss(z_q, z_p, margin=margin, hard_negatives=True)
            else:
                loss = triplet_fn(z_q, z_p, z_n)

            optim.zero_grad()
            loss.backward()
            if clip_grad_norm > 0:
                clip_grad_norm_(model.parameters(), clip_grad_norm)
            optim.step()
            running += loss.item() * z_q.size(0)

        train_loss = running / len(train_ds)

        # ---------------- Validation --------------------
        model.eval(); val_running = 0.0
        with torch.no_grad():
            for batch in dl_val:
                z_q  = model.encode(batch["q"].to(device))
                z_p  = model.encode(batch["p"].to(device))
                z_n  = model.encode(batch["n"].to(device))

                if hard_negatives:
                    vloss = contrastive_loss(z_q, z_p, margin=margin, hard_negatives=True)
                else:
                    vloss = triplet_fn(z_q, z_p, z_n)

                val_running += vloss.item() * z_q.size(0)
        val_loss = val_running / len(val_ds)

        log.info("[Epoch %02d/%d] train=%.6f | val=%.6f", epoch, epochs, train_loss, val_loss)
        scheduler.step(val_loss)

        # ---------------- Early stop --------------------
        rel_improve = (best_val - val_loss) / best_val if best_val < math.inf else 1.0
        if rel_improve > min_delta:
            best_val, epochs_no_improve = val_loss, 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  -> New best val_loss. Checkpoint saved at {model_save_path}")
            logger.train.info("New best val_loss: %.6f → checkpoint %s", best_val, model_save_path)
        else:
            epochs_no_improve += 1
            if patience and epochs_no_improve >= patience:
                print("[EARLY STOP] No improvement in validation.")
                logger.train.info("[EARLY STOP] No improvement in validation.")
                break

    print(f"[DONE] Best val_loss = {best_val:.6f}")
    logger.main.info("[DONE] Best val_loss = %.6f", best_val)
    logger.main.info("")

# --------------------------------------------------------------------------- #
#  CLI                                                                       #
# --------------------------------------------------------------------------- #

if __name__ == "__main__":
    load_dotenv()

    p = argparse.ArgumentParser(description="Train Contrastive Auto-Encoder (CAE)")
    p.add_argument("--config", default="./config/config.yaml")
    p.add_argument("--dataset", choices=["uda", "squad"], help="Override YAML dataset")
    p.add_argument("--epochs",  type=int)
    p.add_argument("--batch_size", type=int)
    p.add_argument("--lr",      type=float)
    p.add_argument("--weight_decay", type=float, default=1e-4)
    p.add_argument("--clip_grad", type=float, default=1.0)
    p.add_argument("--margin",  type=float, default=0.2)
    p.add_argument("--val_split", type=float, default=0.1)
    p.add_argument("--patience", type=int, default=5)
    p.add_argument("--no-hard-negatives", action="store_true")
    p.add_argument("--save_path")
    args = p.parse_args()

    # ---------- Config & logging -----------------------------------------
    cfg       = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg["models"]["contrastive"]
    log       = init_logger(cfg["logging"])

    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False), logger=log.main)

    # ---------- Dataset ---------------------------------------------------
    ds_path = prepare_datasets(cfg, variant="cae", dataset_override=args.dataset)

    # ---------- Hparams final ---------------------------------------------
    hparams = dict(
        dataset_path = ds_path,
        input_dim    = model_cfg.get("input_dim", 384),
        latent_dim   = model_cfg.get("latent_dim", 64),
        hidden_dim   = model_cfg.get("hidden_dim", 512),
        batch_size   = args.batch_size or train_cfg.get("batch_size", 256),
        epochs       = args.epochs or train_cfg.get("epochs", 20),
        lr           = args.lr or float(train_cfg.get("learning_rate", 1e-3)),
        weight_decay = args.weight_decay,
        clip_grad_norm = args.clip_grad,
        margin       = args.margin,
        hard_negatives = not args.no_hard_negatives,
        val_split    = args.val_split,
        patience     = None if args.patience == 0 else args.patience,
        model_save_path = args.save_path or model_cfg.get("checkpoint", "./models/checkpoints/contrastive_ae.pth"),
        logger       = log,
    )

    train_cae(**hparams)

"""

training/train_dae.py
"""
# training/train_dae.py – Denoising Auto‑Encoder con validación y early‑stopping

from __future__ import annotations

import argparse
import os
from typing import Optional

import torch
from torch.utils.data import DataLoader

from data.torch_datasets import EmbeddingDAEDataset
from models.denoising_autoencoder import DenoisingAutoencoder
from training.loss_functions import dae_loss
from utils.load_config import load_config
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import split_dataset, prepare_datasets
from utils.load_config import init_logger
from dotenv import load_dotenv

###############################################################################
#  TRAINING FUNCTION                                                          #
###############################################################################

def train_dae(
    *,
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    logger,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    device: Optional[str] = None,
):
    """Run DAE training/validation loop."""
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Training DAE on {device} | val_split={val_split}")
    logger.main.info("")
    logger.main.info("Training DAE | device=%s", device)

    # ---------------- Dataset --------------------------
    full_ds = EmbeddingDAEDataset(dataset_path)
    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    dl_val = DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False)

    # ---------------- Model & Optimizer ----------------
    model = DenoisingAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = torch.optim.Adam(model.parameters(), lr=lr)

    best_val = float("inf")
    no_improve = 0

    # ---------------- Training Loop -------------------
    for epoch in range(1, epochs + 1):
        model.train()
        running = 0.0
        for batch in dl_train:
            x_noisy = batch["x"].to(device)
            x_clean = batch["y"].to(device)

            optim.zero_grad()
            x_rec = model(x_noisy)
            loss = dae_loss(x_rec, x_clean, reduction="mean")
            loss.backward()
            optim.step()
            running += loss.item() * x_noisy.size(0)

        train_loss = running / len(train_ds)

        # ---------------- Validation ------------------
        model.eval()
        with torch.no_grad():
            val_running = 0.0
            for batch in dl_val:
                x_noisy = batch["x"].to(device)
                x_clean = batch["y"].to(device)
                x_rec = model(x_noisy)
                vloss = dae_loss(x_rec, x_clean, reduction="mean")
                val_running += vloss.item() * x_noisy.size(0)
            val_loss = val_running / len(val_ds)

        print(
            f"[Epoch {epoch:02d}/{epochs}] train_loss={train_loss:.6f} | val_loss={val_loss:.6f}"
        )
        logger.train.info(
            "[Epoch %02d/%d] train=%.6f | val=%.6f", epoch, epochs, train_loss, val_loss
        )

        # ---------------- Early Stopping --------------
        if val_loss < best_val - 1e-4:
            best_val = val_loss
            no_improve = 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  -> New best val_loss. Checkpoint saved at {model_save_path}")
            logger.train.info("New best val_loss: %.6f → checkpoint %s", best_val, model_save_path)
        else:
            no_improve += 1
            if patience and no_improve >= patience:
                print("[EARLY STOP] No improvement in validation.")
                logger.train.info("[EARLY STOP] No improvement in validation.")
                break

    print(f"[DONE] Best val_loss = {best_val:.6f}")
    logger.main.info("[DONE] Best val_loss = %.6f", best_val)
    logger.main.info("")

###############################################################################
#  CLI                                                                        #
###############################################################################

if __name__ == "__main__":
    load_dotenv()

    parser = argparse.ArgumentParser(description="Train Denoising Auto‑Encoder (DAE)")
    parser.add_argument("--config", default="./config/config.yaml")
    parser.add_argument("--dataset", choices=["uda", "squad"], help="Override YAML dataset")
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--save_path")
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--val_split", type=float, default=0.1)
    parser.add_argument("--patience", type=int, default=5)
    args = parser.parse_args()

    # ---------------- Config & logging ----------------
    cfg = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg.get("models", {}).get("dae", {})
    log = init_logger(cfg["logging"])

    # ---------------- Reproducibility ----------------
    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False), logger=log.train)
    device = resolve_device(train_cfg.get("device"))

    # ---------------- Dataset prep -------------------
    dataset_path = prepare_datasets(cfg, variant="dae", dataset_override=args.dataset)

    # ---------------- Training -----------------------
    train_dae(
        dataset_path=dataset_path,
        input_dim=model_cfg.get("input_dim", 384),
        latent_dim=model_cfg.get("latent_dim", 64),
        hidden_dim=model_cfg.get("hidden_dim", 512),
        batch_size=args.batch_size or train_cfg.get("batch_size", 256),
        epochs=args.epochs or train_cfg.get("epochs", 20),
        lr=args.lr if args.lr is not None else float(train_cfg.get("learning_rate", 1e-3)),
        model_save_path=args.save_path or model_cfg.get("checkpoint", "./models/checkpoints/dae_text.pth"),
        val_split=args.val_split,
        patience=None if args.patience == 0 else args.patience,
        device=device,
        logger=log,
    )

"""

training/train_vae.py
"""
# training/train_vae.py – Variational Auto‑Encoder con validación y early‑stopping

import argparse
import os
from typing import Optional

import torch
from torch.utils.data import DataLoader

from data.torch_datasets import EmbeddingVAEDataset
from models.variational_autoencoder import VariationalAutoencoder
from training.loss_functions import vae_loss
from utils.load_config import load_config, init_logger
from utils.training_utils import set_seed, resolve_device
from utils.data_utils import prepare_datasets
from dotenv import load_dotenv

###############################################################################
#  TRAINING LOOP                                                             #
###############################################################################

def train_vae(
    dataset_path: str,
    input_dim: int,
    latent_dim: int,
    hidden_dim: int,
    batch_size: int,
    epochs: int,
    lr: float,
    model_save_path: str,
    val_split: float = 0.1,
    patience: Optional[int] = 5,
    device: Optional[str] = None,
):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Training VAE on {device} | val_split={val_split}")

    full_ds = EmbeddingVAEDataset(dataset_path)
    from utils.data_utils import split_dataset  # local import to avoid circular

    train_ds, val_ds = split_dataset(full_ds, val_split=val_split)
    dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    dl_val   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)

    model = VariationalAutoencoder(input_dim, latent_dim, hidden_dim).to(device)
    optim = torch.optim.Adam(model.parameters(), lr=lr)

    best_val, no_improve = float("inf"), 0
    for epoch in range(1, epochs + 1):
        # ---------------- train ------------------
        model.train(); running = 0.0
        for batch in dl_train:
            x_in  = batch["input"].to(device)
            x_tar = batch["target"].to(device)
            optim.zero_grad()
            x_rec, mu, logvar = model(x_in)
            loss = vae_loss(x_rec, x_tar, mu, logvar, mse_reduction="mean")
            loss.backward(); optim.step()
            running += loss.item() * x_in.size(0)
        train_loss = running / len(train_ds)

        # ---------------- validation -------------
        model.eval(); val_running = 0.0
        with torch.no_grad():
            for batch in dl_val:
                x_in  = batch["input"].to(device)
                x_tar = batch["target"].to(device)
                x_rec, mu, logvar = model(x_in)
                vloss = vae_loss(x_rec, x_tar, mu, logvar, mse_reduction="mean")
                val_running += vloss.item() * x_in.size(0)
        val_loss = val_running / len(val_ds)

        print(f"[Epoch {epoch:02d}/{epochs}] train={train_loss:.6f} | val={val_loss:.6f}")

        if val_loss < best_val - 1e-4:
            best_val, no_improve = val_loss, 0
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
        else:
            no_improve += 1
            if patience and no_improve >= patience:
                print("[EARLY STOP] No improvement in validation."); break

    print(f"[DONE] best_val_loss = {best_val:.6f}")

###############################################################################
#  CLI                                                                       #
###############################################################################

if __name__ == "__main__":
    load_dotenv()

    parser = argparse.ArgumentParser(description="Train Variational Auto‑Encoder (VAE)")
    parser.add_argument("--config", default="./config/config.yaml")
    parser.add_argument("--dataset", choices=["uda", "squad"], help="Override dataset in config.yaml")
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--lr", type=float)
    parser.add_argument("--save_path")
    parser.add_argument("--batch_size", type=int)
    parser.add_argument("--val_split", type=float, default=0.1)
    parser.add_argument("--patience", type=int, default=5)
    args = parser.parse_args()

    # ------------- config & logging -------------
    cfg       = load_config(args.config)
    train_cfg = cfg.get("training", {})
    model_cfg = cfg.get("models", {}).get("vae", {})
    log       = init_logger(cfg["logging"])

    set_seed(train_cfg.get("seed", 42), train_cfg.get("deterministic", False))
    device = resolve_device(train_cfg.get("device"))

    # ------------- dataset paths -----------------
    dataset_path = prepare_datasets(cfg, variant="vae", dataset_override=args.dataset)

    # ------------- training ----------------------
    train_vae(
        dataset_path=dataset_path,
        input_dim=model_cfg.get("input_dim", 384),
        latent_dim=model_cfg.get("latent_dim", 64),
        hidden_dim=model_cfg.get("hidden_dim", 512),
        batch_size=args.batch_size or train_cfg.get("batch_size", 256),
        epochs=args.epochs or train_cfg.get("epochs", 20),
        lr=float(args.lr) if args.lr is not None else float(train_cfg.get("learning_rate", 1e-3)),
        model_save_path=args.save_path or model_cfg.get("checkpoint", "./models/checkpoints/vae_text.pth"),
        val_split=args.val_split,
        patience=None if args.patience == 0 else args.patience,
        device=device,
    )

"""

utils/data_utils.py
"""
 # /utils/data_utils.py
from __future__ import annotations
import os
from typing import List, Tuple, Optional

import torch
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm
import random
from torch.utils.data import Subset

from pathlib import Path
from typing import Dict, Optional

def _compute_embeddings(
    texts: List[str],
    model: SentenceTransformer,
    batch_size: int = 64,
) -> torch.Tensor:
    """Devuelve un tensor CPU float32 [N × D] con los CLS-embeddings."""
    chunks: List[torch.Tensor] = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding"):
        batch = texts[i : i + batch_size]
        with torch.no_grad():
            emb = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
            chunks.append(torch.from_numpy(emb))
    return torch.cat(chunks, dim=0).float()

def _jaccard_sim(a: str, b: str) -> float:
    a_set = set(a.lower().split())
    b_set = set(b.lower().split())
    inter = a_set & b_set
    union = a_set | b_set
    return len(inter) / len(union) if union else 0.0

def ensure_uda_data(
    *,
    output_dir: str = "./data",
    max_samples: Optional[int] = None,
    base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    noise_std: float = 0.05,
    force: bool = False,
) -> None:
    """Genera (o reutiliza) los ficheros de embeddings para VAE, DAE y contraste."""
    os.makedirs(output_dir, exist_ok=True)

    vae_path = os.path.join(output_dir, "uda_vae_embeddings.pt")
    dae_path = os.path.join(output_dir, "uda_dae_embeddings.pt")
    contrastive_path = os.path.join(output_dir, "uda_contrastive_embeddings.pt")

    if (
        not force
        and os.path.exists(vae_path)
        and os.path.exists(dae_path)
        and os.path.exists(contrastive_path)
    ):
        print("[INFO] UDA embeddings ya preparados — nada que hacer.")
        return

    print("[INFO] Descargando / cargando UDA…")
    uda = load_dataset("qinchuanhui/UDA-QA", "nq")
    if max_samples is not None:
        uda = uda.select(range(min(max_samples, len(uda))))
    print(f"[INFO] UDA listo con {len(uda):,} ejemplos.")

    clean_texts: List[str] = []
    contrastive_triples: List[Tuple[str, str, str]] = []

    for i, ex in enumerate(uda["test"]): # TEMPORAL ******************************************************* CAMBIAR URGENTEMENTE
        q = ex.get("question", "").strip()
        pos = ex.get("long_answer", "").strip()
        if not q or not pos:
            continue

        neg = None
        for _ in range(10):
            j = random.randint(0, len(uda["test"]) - 1)
            if j == i:
                continue
            neg_cand = uda["test"][j].get("long_answer", "").strip()
            if not neg_cand:
                continue
            if _jaccard_sim(q, neg_cand) < 0.1:
                neg = neg_cand
                break

        if neg is None:
            continue

        clean_texts.extend((q, pos))             # query + positive answer
        contrastive_triples.append((q, pos, neg))


    print(f"[INFO] Tripletas contrastivas generadas: {len(contrastive_triples):,}")

    print(f"[INFO] Cargando SentenceTransformer '{base_model_name}' …")
    st_model = SentenceTransformer(base_model_name)

    print("[INFO] Generando embeddings VAE/DAE (positivos)…")
    target_emb = _compute_embeddings(clean_texts, st_model)

    if force or not os.path.exists(vae_path):
        torch.save({"input": target_emb, "target": target_emb.clone()}, vae_path)
        print(f"[OK]  VAE embeddings guardados → {vae_path}")

    if force or not os.path.exists(dae_path):
        input_emb = target_emb + torch.randn_like(target_emb) * noise_std
        torch.save({"input": input_emb, "target": target_emb}, dae_path)
        print(f"[OK]  DAE embeddings guardados → {dae_path}")

    if force or not os.path.exists(contrastive_path):
        print("[INFO] Generando embeddings de triples (query/pos/neg)…")
        qs, ps, ns = zip(*contrastive_triples)
        q_emb = _compute_embeddings(list(qs), st_model)
        p_emb = _compute_embeddings(list(ps), st_model)
        n_emb = _compute_embeddings(list(ns), st_model)
        torch.save({"query": q_emb, "positive": p_emb, "negative": n_emb}, contrastive_path)
        print(f"[OK]  Contrastive embeddings guardados → {contrastive_path}")

    print("[DONE] Preprocesado de UDA completo.")

def split_dataset(dataset: torch.utils.data.Dataset, val_split: float = 0.1, seed: int = 42) -> Tuple[Subset, Subset]:
    n_total = len(dataset)
    idx = list(range(n_total))
    random.Random(seed).shuffle(idx)
    n_val = int(n_total * val_split)
    val_idx = idx[:n_val]
    train_idx = idx[n_val:]
    return Subset(dataset, train_idx), Subset(dataset, val_idx)


def ensure_squad_data(
    *,
    output_dir: str = "./data",
    max_samples: Optional[int] = None,
    base_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
    noise_std: float = 0.05,
    include_unanswerable: bool = False,
    force: bool = False,
) -> None:
    """Generate VAE / DAE / CAE embedding tensors from **SQuAD v1/v2**.

    The tensors are stored on disk using the same structure as the UDA helper,
    so training scripts remain unchanged.  Filenames:

        squad_vae_embeddings.pt
        squad_dae_embeddings.pt
        squad_contrastive_embeddings.pt
    """
    os.makedirs(output_dir, exist_ok=True)

    vae_path         = os.path.join(output_dir, "squad_vae_embeddings.pt")
    dae_path         = os.path.join(output_dir, "squad_dae_embeddings.pt")
    contrastive_path = os.path.join(output_dir, "squad_contrastive_embeddings.pt")

    # --------------------------------------------------------------------- #
    #  Early exit if everything is already cached                           #
    # --------------------------------------------------------------------- #
    if (
        not force
        and all(os.path.exists(p) for p in (vae_path, dae_path, contrastive_path))
    ):
        print("[INFO] SQuAD embeddings already prepared nothing to do.")
        return

    # --------------------------------------------------------------------- #
    #  1. Load SQuAD                                                        #
    # --------------------------------------------------------------------- #
    ds_name = "squad_v2" if include_unanswerable else "squad"
    print(f"[INFO] Loading {ds_name} …")
    squad = load_dataset(ds_name, split="train")
    if max_samples is not None:
        squad = squad.select(range(min(max_samples, len(squad))))
    print(f"[INFO] SQuAD loaded with {len(squad):,} examples.")

    # --------------------------------------------------------------------- #
    #  2. Build positive contexts and contrastive triples                   #
    # --------------------------------------------------------------------- #
    clean_texts: List[str] = []
    contrastive_triples: List[Tuple[str, str, str]] = []

    for i, ex in enumerate(squad):
        q   = ex["question"].strip()
        ctx = ex["context"].strip()
        if not q or not ctx:
            continue

        # ----------------- simple negative mining ------------------------ #
        neg = None
        for _ in range(10):
            j = random.randint(0, len(squad) - 1)
            if j == i:
                continue
            neg_ctx = squad[j]["context"].strip()
            if neg_ctx and _jaccard_sim(q, neg_ctx) < 0.1:
                neg = neg_ctx
                break
        if neg is None:
            continue

        clean_texts.extend([q, ctx])              # query + positive context
        contrastive_triples.append((q, ctx, neg))

    print(f"[INFO] Contrastive triples generated: {len(contrastive_triples):,}")

    # --------------------------------------------------------------------- #
    #  3. Encode with SBERT                                                 #
    # --------------------------------------------------------------------- #
    print(f"[INFO] Loading SBERT '{base_model_name}' …")
    st_model = SentenceTransformer(base_model_name)

    print("[INFO] Encoding queries + positive contexts …")
    target_emb = _compute_embeddings(clean_texts, st_model)   # shape [N × D]

    # --------------------------------------------------------------------- #
    #  4. Save VAE and DAE variants                                         #
    # --------------------------------------------------------------------- #
    if force or not os.path.exists(vae_path):
        torch.save({"input": target_emb, "target": target_emb.clone()}, vae_path)
        print(f"[OK]  VAE embeddings   → {vae_path}")

    if force or not os.path.exists(dae_path):
        input_emb = target_emb + torch.randn_like(target_emb) * noise_std
        torch.save({"input": input_emb, "target": target_emb}, dae_path)
        print(f"[OK]  DAE embeddings   → {dae_path}")

    # --------------------------------------------------------------------- #
    #  5. Save contrastive triplets                                         #
    # --------------------------------------------------------------------- #
    if force or not os.path.exists(contrastive_path):
        print("[INFO] Encoding triplets …")
        qs, ps, ns = zip(*contrastive_triples)
        q_emb = _compute_embeddings(list(qs), st_model)
        p_emb = _compute_embeddings(list(ps), st_model)
        n_emb = _compute_embeddings(list(ns), st_model)
        torch.save(
            {"query": q_emb, "positive": p_emb, "negative": n_emb},
            contrastive_path,
        )
        print(f"[OK]  Contrastive embeddings → {contrastive_path}")

    print("[DONE] SQuAD preprocessing finished.")



def _prepare_uda(cfg: dict) -> Dict[str, str]:
    common = dict(
        output_dir="./data",
        max_samples=cfg["data"].get("max_samples"),
        base_model_name=cfg["embedding_model"]["name"],
        force=False,
    )
    ensure_uda_data(**common)
    return {
        "vae": "./data/uda_vae_embeddings.pt",
        "dae": "./data/uda_dae_embeddings.pt",
        "cae": "./data/uda_contrastive_embeddings.pt",
    }


def _prepare_squad(cfg: dict) -> Dict[str, str]:
    data_cfg = cfg["data"]
    common = dict(
        output_dir="./data",
        max_samples=data_cfg.get("max_samples"),
        base_model_name=cfg["embedding_model"]["name"],
        noise_std=0.05,
        include_unanswerable=data_cfg.get("include_unanswerable", False),
        force=False,
    )
    ensure_squad_data(**common)
    return {
        "vae": "./data/squad_vae_embeddings.pt",
        "dae": "./data/squad_dae_embeddings.pt",
        "cae": "./data/squad_contrastive_embeddings.pt",
    }


def prepare_datasets(
    cfg: dict,
    *,
    variant: str,
    dataset_override: Optional[str] = None,
) -> str:
    """Ensure dataset tensors exist and return path for requested variant.

    Args:
        cfg: Parsed YAML config dict (must include data and embedding_model).
        variant: One of `{"vae", "dae", "cae"}.
        dataset_override: If provided, forces `"uda" or "squad"

    Returns:
        The filesystem path to the tensor file corresponding to the *variant*.
    """
    variant = variant.lower()
    assert variant in {"vae", "dae", "cae"}, "variant must be vae, dae or cae"

    ds_name = (dataset_override or cfg.get("data", {}).get("dataset", "uda")).lower()
    if ds_name == "squad":
        paths = _prepare_squad(cfg)
    elif ds_name == "uda":
        paths = _prepare_uda(cfg)
    else:
        raise ValueError(f"Unknown dataset: {ds_name}")

    path = paths[variant]
    if not Path(path).exists():
        raise FileNotFoundError(f"Expected dataset file not found: {path}")
    return path
"""

utils/load_config.py
"""
import numpy as np
import yaml
import os, sys
from pathlib import Path
from types import SimpleNamespace   
import logging

def load_config(path: str) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def init_logger(cfg_logging: dict) -> SimpleNamespace:

    if cfg_logging.get("log_to_file", False):
        Path(cfg_logging["log_file"]).parent.mkdir(parents=True, exist_ok=True)

    handlers = [logging.StreamHandler(sys.stdout)]
    if cfg_logging.get("log_to_file", False):
        handlers.append(logging.FileHandler(cfg_logging["log_file"], encoding="utf-8"))

    logging.basicConfig(
        level=getattr(logging, cfg_logging.get("level", "INFO")),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        handlers=handlers,
        force=True,                        
    )

    return SimpleNamespace(
        main = logging.getLogger("main"),
        train = logging.getLogger("train"),
        utils = logging.getLogger("utils"),
    )
"""

utils/training_utils.py
"""
# utils/training_utils.py
import os, random, logging
import numpy as np
import torch

def set_seed( seed: int, deterministic: bool = False, logger: logging.Logger | None = None ) -> None:
    """
    Fija todas las semillas y el modo determinista de cuDNN.

    Args:
        seed (int): valor de la semilla.
        deterministic (bool): True → reproducibilidad completa
                              (más lento en GPU).
        logger (logging.Logger | None): instancia de logger principal;
                                        si es None se usa el del módulo.
    """
    logger = logger or logging.getLogger(__name__)

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    torch.backends.cudnn.deterministic = deterministic
    torch.backends.cudnn.benchmark     = not deterministic
    torch.use_deterministic_algorithms(deterministic)

    if deterministic:
        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
        logger.info("cuDNN deterministic mode  ACTIVE (desactivated benchmark mode)")
    else:
        logger.info("cuDNN benchmark mode ACTIVE (desactivated deterministic mode)")


def resolve_device(device_str: str | None = None) -> str:
    if device_str is not None:
        return device_str
    return "cuda" if torch.cuda.is_available() else "cpu"

"""

