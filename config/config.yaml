project:
  name: rag_autoencoder_tfm
  version: "0.1"

paths:
  data_dir: "./data"
  checkpoints_dir: "./models/checkpoints"
  logs_dir: "./logs"

embedding_model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  max_length: 256

models:
  vae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/uda_vae_embeddings.pt"          
    checkpoint: "./models/checkpoints/vae_text.pth"

  dae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/uda_dae_embeddings.pt"          
    checkpoint: "./models/checkpoints/dae_text.pth"

  contrastive:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/uda_contrastive_embeddings.pt"  
    checkpoint: "./models/checkpoints/contrastive_ae.pth"


training:
  batch_size: 128
  epochs: 50
  learning_rate: 1e-3
  seed: 42
  device: "cuda"  # "cpu"
  max_samples: null # None para usar todo el dataset

retrieval:
  similarity_metric: "cosine"   # opciones: cosine, mahalanobis
  top_k: 20
  compress_embeddings: true  # usar embeddings comprimidos

generation:
  provider: "openai"       # openai, anthropic, etc.
  model: "gpt-4o-mini"
  temperature: 0.3
  max_tokens: 256
  system_prompt_path: "./config/prompts/system_prompts.txt"


evaluation:
  retrieval_metrics: ["Recall@5", "MRR@10", "nDCG@10"]
  generation_metrics: ["ROUGE-L", "BLEU", "METEOR"]

logging:
  level: "INFO"
  log_to_file: true
  log_file: "./logs/run.log"
