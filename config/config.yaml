project:
  name: rag_autoencoder_tfm
  version: "0.1"

paths:
  data_dir: "./data"
  checkpoints_dir: "./models/checkpoints"
  logs_dir: "./logs"

embedding_model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  max_length: 256

models:
  vae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/SQUAD/squad_vae_embeddings.pt"          
    checkpoint: "./models/checkpoints/vae_text.pth"

  dae:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/SQUAD/squad_dae_embeddings.pt"
    checkpoint: "./models/checkpoints/dae_text.pth"

  contrastive:
    input_dim: 384
    latent_dim: 64
    hidden_dim: 512
    dataset_path: "./data/SQUAD/squad_contrastive_embeddings.pt"
    checkpoint: "./models/checkpoints/contrastive_ae.pth"

data:
  dataset: "squad"            #  "uda"  |  "squad"
  version: "v1"          #  v1, v2  ONly for squad
  max_samples:         #  optional, blankk to use all samples
  include_unanswerable: false   #  Only Squad

training:
  batch_size: 128
  epochs: 50
  learning_rate: 1e-3
  seed: 42
  device: "cuda"  # "cpu"
  deterministic: false      # (true = modo debug)

retrieval:
  similarity_metric: "cosine"   # cosine, mahalanobis
  top_k: 20
  compress_embeddings: true 

generation:
  provider: "openai"       # Just OpenAI by now
  model: "gpt-4o-mini"
  temperature: 0.3
  max_tokens: 256
  system_prompt_path: "./config/prompts/system_prompt.txt"


evaluation:
  retrieval_metrics: ["Recall@5", "MRR@10", "nDCG@10"] # You can change the k top  simply by changing the @k e.j Recall@10, "MRR@20"
  generation_metrics: ["ROUGE-L", "BLEU", "METEOR"]

logging:
  level: "INFO"
  log_to_file: true
  log_file: "./logs/run.log"
